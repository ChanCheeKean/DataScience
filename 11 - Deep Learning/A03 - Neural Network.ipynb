{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"neural-networks-deep-learning","graded_item_id":"c4HO0","launcher_item_id":"lSYZM"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"CLdmGCHoxcP5"},"source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","from lib.testCases_v4a import *\n","from lib.dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n","\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UUcSlDiRxcP7"},"source":["# 1. Initialization"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"jD7_ZImAxcP-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665337755649,"user_tz":-480,"elapsed":3,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"a77aaeba-e82c-424f-df77-a6a97bad7d04"},"source":["def initialize_parameters(n_x, n_h, n_y): \n","    np.random.seed(1)\n","    W1 = np.random.randn(n_h, n_x) * 0.01 \n","    b1 = np.zeros((n_h, 1)) \n","    W2 = np.random.randn(n_y, n_h) * 0.01 \n","    b2 = np.zeros((n_y, 1)) \n","    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n","    return parameters\n","\n","parameters = initialize_parameters(3,2,1)\n","parameters"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'W1': array([[ 0.01624345, -0.00611756, -0.00528172],\n","        [-0.01072969,  0.00865408, -0.02301539]]), 'b1': array([[0.],\n","        [0.]]), 'W2': array([[ 0.01744812, -0.00761207]]), 'b2': array([[0.]])}"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"yxY2S7RPxcP_"},"source":["# 2. L-layer Neural Network\n","\n","The initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the `initialize_parameters_deep`, you should make sure that your dimensions match between each layer. Recall that $n^{[l]}$ is the number of units in layer $l$. Thus for example if the size of our input $X$ is $(12288, 209)$ (with $m=209$ examples) then:\n","\n","<table style=\"width:100%\">\n","    <tr>\n","        <td><b>Layer Name</b></td> \n","        <td><b>Shape of W</b></td> \n","        <td><b>Shape of b</b></td> \n","        <td><b>Activation</b></td>\n","        <td><b>Shape of Activation</b></td> \n","    </tr>\n","    <tr>\n","        <td><b>Layer 1</b></td> \n","        <td>$(n^{[1]},12288)$</td>\n","        <td>$(n^{[1]},1)$</td>\n","        <td>$Z^{[1]} = W^{[1]}  X + b^{[1]} $</td>\n","        <td>$(n^{[1]},209)$</td>\n","    </tr>\n","    <tr>\n","        <td><b>Layer 2</b></td>\n","        <td>$(n^{[2]}, n^{[1]})$</td>\n","        <td>$(n^{[2]},1)$</td>\n","        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$</td>\n","        <td>$(n^{[2]}, 209)$</td>\n","    </tr>\n","    <tr>\n","        <td>$\\vdots$</td>\n","        <td>$\\vdots$</td>\n","        <td>$\\vdots$</td>\n","        <td>$\\vdots$</td> \n","        <td>$\\vdots$</td>\n","   </tr>\n","   <tr>\n","        <td><b>Layer L-1</b></td> \n","        <td>$(n^{[L-1]}, n^{[L-2]})$</td> \n","        <td>$(n^{[L-1]}, 1)$</td> \n","        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b$</td> \n","        <td>$(n^{[L-1]}, 209)$</td> \n","   </tr>\n","   <tr>\n","        <td><b>Layer L</b></td>\n","        <td>$(n^{[L]}, n^{[L-1]})$</td>\n","        <td>$(n^{[L]}, 1)$</td>\n","        <td>$Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n","        <td>$(n^{[L]}, 209)$</td>\n","    </tr>\n","\n","</table>\n","\n","When we compute $W X + b$ in python, it carries out broadcasting. For example, if: \n","\n","$$ W = \\begin{bmatrix}\n","    j  & k  & l\\\\\n","    m  & n & o \\\\\n","    p  & q & r \n","\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n","    a  & b  & c\\\\\n","    d  & e & f \\\\\n","    g  & h & i \n","\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n","    s  \\\\\n","    t  \\\\\n","    u\n","\\end{bmatrix}\\tag{2}$$\n","\n","Then $WX + b$ will be:\n","\n","$$ WX + b = \\begin{bmatrix}\n","    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n","    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n","    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n","\\end{bmatrix}\\tag{3}  $$"]},{"cell_type":"code","metadata":{"id":"sT_63TrlxcQB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665337757054,"user_tz":-480,"elapsed":5,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"ae919347-435c-4aa9-ba50-63ce8529b8fc"},"source":["def initialize_parameters_deep(layer_dims):\n","    parameters = {}\n","    L = len(layer_dims)\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01 # (layer_dims[1], layer_dims[-1])\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) # (layer_dims[1], 1)\n","        \n","    return parameters\n","    \n","parameters = initialize_parameters_deep([5, 4, 3])\n","parameters"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'W1': array([[ 0.00319039, -0.0024937 ,  0.01462108, -0.02060141, -0.00322417],\n","        [-0.00384054,  0.01133769, -0.01099891, -0.00172428, -0.00877858],\n","        [ 0.00042214,  0.00582815, -0.01100619,  0.01144724,  0.00901591],\n","        [ 0.00502494,  0.00900856, -0.00683728, -0.0012289 , -0.00935769]]),\n"," 'b1': array([[0.],\n","        [0.],\n","        [0.],\n","        [0.]]),\n"," 'W2': array([[-0.00267888,  0.00530355, -0.00691661, -0.00396754],\n","        [-0.00687173, -0.00845206, -0.00671246, -0.00012665],\n","        [-0.0111731 ,  0.00234416,  0.01659802,  0.00742044]]),\n"," 'b2': array([[0.],\n","        [0.],\n","        [0.]])}"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"9BhEl0_NxcQB"},"source":["# 3. Forward propagation module"]},{"cell_type":"code","metadata":{"id":"2meHcfO0xcQC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621061609910,"user_tz":-480,"elapsed":631,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"0c4c4509-753c-4cbf-f0ba-7973d3090cae"},"source":["def linear_forward(A, W, b):\n","    Z = np.dot(W, A) + b # (W.shape[0, A.shape[1]])\n","    cache = (A, W, b)\n","    return Z, cache\n","\n","A, W, b = linear_forward_test_case()\n","Z, linear_cache = linear_forward(A, W, b)\n","linear_cache"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 1.62434536, -0.61175641],\n","        [-0.52817175, -1.07296862],\n","        [ 0.86540763, -2.3015387 ]]),\n"," array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]),\n"," array([[-0.24937038]]))"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"DOGDYEptxcQC"},"source":["## 3.1 Linear-Activation Forward"]},{"cell_type":"code","metadata":{"id":"lw-BCrJ8xcQE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621061653006,"user_tz":-480,"elapsed":862,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"986e49a1-8cd9-47e7-91dc-55d8f8022a6d"},"source":["def sigmoid(Z):\n","    A = 1/(1+np.exp(-Z))\n","    cache = Z\n","    return A, cache\n","    \n","def relu(Z):    \n","    A = np.maximum(0,Z) # (Z.shape)\n","    cache = Z \n","    return A, cache\n","\n","def linear_activation_forward(A_prev, W, b, activation):    \n","    if activation == \"sigmoid\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z) \n","    \n","    elif activation == \"relu\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","    return A, cache\n","\n","A_prev, W, b = linear_activation_forward_test_case()\n","\n","# sigmoid\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n","print(A)\n","\n","# relu\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","print(A)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.96890023 0.11013289]]\n","[[3.43896131 0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mAhAgiRHxcQE"},"source":["## 3.2 L-Layer Forward \n","\n","\n","<img src=\"https://raw.githubusercontent.com/sebastianbirk/coursera-deep-learning-specialization/master/01_neural_networks_and_deep_learning/04_deep_neural_nets_with_numpy/images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n","<caption><center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center></caption><br>"]},{"cell_type":"code","metadata":{"id":"FtDU52NIxcQF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621061754453,"user_tz":-480,"elapsed":592,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"c878ea5d-a7c5-4202-e1e5-93556c5aa820"},"source":["def L_model_forward(X, parameters):\n","    caches = []\n","    A = X\n","    \n","    # number of layers in the neural network\n","    L = len(parameters) // 2                  \n","    \n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A \n","        A, cache = linear_activation_forward(A_prev, \n","                                             parameters['W' + str(l)], \n","                                             parameters['b' + str(l)], \n","                                             activation='relu')\n","        caches.append(cache)\n","    \n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","    AL, cache = linear_activation_forward(A, \n","                                          parameters['W' + str(L)], \n","                                          parameters['b' + str(L)], \n","                                          activation='sigmoid')\n","    caches.append(cache)    \n","    assert(AL.shape == (1, X.shape[1]))\n","    return AL, caches\n","\n","X, parameters = L_model_forward_test_case_2hidden()\n","AL, caches = L_model_forward(X, parameters)\n","print(X)\n","print(\"AL = \" + str(AL))\n","print(\"Length of caches list = \" + str(len(caches)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-0.31178367  0.72900392  0.21782079 -0.8990918 ]\n"," [-2.48678065  0.91325152  1.12706373 -1.51409323]\n"," [ 1.63929108 -0.4298936   2.63128056  0.60182225]\n"," [-0.33588161  1.23773784  0.11112817  0.12915125]\n"," [ 0.07612761 -0.15512816  0.63422534  0.810655  ]]\n","AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n","Length of caches list = 3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aZChvZ1excQF"},"source":["# 4. Cost function\n","\n","Compute the cross-entropy cost $J$, using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$\n"]},{"cell_type":"code","metadata":{"id":"RjEx32L3xcQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621061784746,"user_tz":-480,"elapsed":1219,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"ad8ee8e5-e6f2-41d8-f1ef-a2b215cac6ce"},"source":["def compute_cost(AL, Y):    \n","    m = Y.shape[1]\n","    cost = (-1/m)* np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1-Y, np.log(1-AL)))\n","    cost = np.squeeze(cost) \n","    assert(cost.shape == ())\n","    return cost\n","\n","Y, AL = compute_cost_test_case()\n","print(\"cost = \" + str(compute_cost(AL, Y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cost = 0.2797765635793422\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"46kFrK-1xcQG"},"source":["# 5. Backward propagation\n","\n","<img src=\"https://raw.githubusercontent.com/sebastianbirk/coursera-deep-learning-specialization/master/01_neural_networks_and_deep_learning/04_deep_neural_nets_with_numpy/images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n","<caption><center> **Figure 3** : Forward and Backward propagation for *LINEAR->RELU->LINEAR->SIGMOID* <br> *The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.*  </center></caption>\n","\n","<!-- \n","For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n","\n","$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n","\n","In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n","\n","Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n","\n","This is why we talk about **backpropagation**.\n","!-->"]},{"cell_type":"markdown","metadata":{"id":"gaGNXjTPxcQH"},"source":["## 5.1 Linear backward\n","\n","For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n","\n","Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n","\n","<img src=\"https://raw.githubusercontent.com/sebastianbirk/coursera-deep-learning-specialization/master/01_neural_networks_and_deep_learning/04_deep_neural_nets_with_numpy/images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n","<caption><center> **Figure 4** </center></caption>\n","\n","The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n","$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n","$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n","$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"]},{"cell_type":"code","metadata":{"id":"LuAorpU6xcQI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621061894618,"user_tz":-480,"elapsed":933,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"8c70cc71-8d11-4b12-e89f-b3e7aa9100ca"},"source":["def linear_backward(dZ, cache):\n","    # dZ2= A2 - Y\n","    # dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    dW = (1 / m) * np.dot(dZ, A_prev.T)\n","    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    return dA_prev, dW, db\n","\n","dZ, linear_cache = linear_backward_test_case()\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","print(dZ)\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862]\n"," [ 0.86540763 -2.3015387   1.74481176 -0.7612069 ]\n"," [ 0.3190391  -0.24937038  1.46210794 -2.06014071]]\n","dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n"," [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n"," [-0.4319552  -1.30987417  1.72354705  0.05070578]\n"," [-0.38981415  0.60811244 -1.25938424  1.47191593]\n"," [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n","dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n"," [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n"," [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n","db = [[-0.14713786]\n"," [-0.11313155]\n"," [-0.13209101]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jPN0eG98xcQI"},"source":["## 5.2 Linear-Activation backward"]},{"cell_type":"code","metadata":{"id":"0D3qptuSxcQJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621061908181,"user_tz":-480,"elapsed":786,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"25f80a2a-a33c-4a6b-ab8b-55435b08685b"},"source":["def relu_backward(dA, cache):\n","    Z = cache\n","    dZ = np.array(dA, copy=True)    \n","    dZ[Z <= 0] = 0\n","    assert (dZ.shape == Z.shape)\n","    return dZ\n","\n","def sigmoid_backward(dA, cache):\n","    Z = cache\n","    s = 1 / (1 + np.exp(-Z))\n","    dZ = dA * s * (1-s)\n","    assert (dZ.shape == Z.shape)\n","    return dZ\n","\n","def linear_activation_backward(dA, cache, activation):\n","    linear_cache, activation_cache = cache\n","    \n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, cache[1])\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        \n","    elif activation == \"sigmoid\":\n","        dZ = sigmoid_backward(dA, cache[1])\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db\n","\n","dAL, linear_activation_cache = linear_activation_backward_test_case()\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sigmoid:\n","dA_prev = [[ 0.11017994  0.01105339]\n"," [ 0.09466817  0.00949723]\n"," [-0.05743092 -0.00576154]]\n","dW = [[ 0.10266786  0.09778551 -0.01968084]]\n","db = [[-0.05729622]]\n","\n","relu:\n","dA_prev = [[ 0.44090989  0.        ]\n"," [ 0.37883606  0.        ]\n"," [-0.2298228   0.        ]]\n","dW = [[ 0.44513824  0.37371418 -0.10478989]]\n","db = [[-0.20837892]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RdCIB250xcQJ"},"source":["## 5.3 L-Model Backward \n","\n","<img src=\"https://raw.githubusercontent.com/sebastianbirk/coursera-deep-learning-specialization/master/01_neural_networks_and_deep_learning/04_deep_neural_nets_with_numpy/images/mn_backward.png\" style=\"width:450px;height:300px;\">\n","<caption><center>  **Figure 5** : Backward pass  </center></caption>\n","\n","** Initializing backpropagation**:\n","To backpropagate through this network, we know that the output is, \n","$A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n","To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n","```python\n","dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n","```\n","\n","You can then use this post-activation gradient `dAL` to keep going backward. As seen in Figure 5, you can now feed in `dAL` into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a `for` loop to iterate through all the other layers using the LINEAR->RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula : \n","\n","$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n","\n","For example, for $l=3$ this would store $dW^{[l]}$ in `grads[\"dW3\"]`."]},{"cell_type":"code","metadata":{"id":"ejSc7AguxcQJ"},"source":["def L_model_backward(AL, Y, caches):\n","    grads = {}\n","    \n","    # the number of layers\n","    L = len(caches) \n","    m = AL.shape[1]\n","    \n","     # after this line, Y is the same shape as AL\n","    Y = Y.reshape(AL.shape)\n","    \n","    # Initializing the backpropagation\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","    \n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","    current_cache = caches[L-1]\n","    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n","    \n","    # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n","        grads[\"dA\" + str(l)] = dA_prev_temp\n","        grads[\"dW\" + str(l + 1)] = dW_temp\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","    return grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpASoPLYxcQK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621061992580,"user_tz":-480,"elapsed":564,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"6f902c15-c045-4166-b28f-1f048361069f"},"source":["AL, Y_assess, caches = L_model_backward_test_case()\n","grads = L_model_backward(AL, Y_assess, caches)\n","print_grads(grads)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n"," [0.         0.         0.         0.        ]\n"," [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n","db1 = [[-0.22007063]\n"," [ 0.        ]\n"," [-0.02835349]]\n","dA1 = [[ 0.12913162 -0.44014127]\n"," [-0.14175655  0.48317296]\n"," [ 0.01663708 -0.05670698]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5NOLx7sKxcQK"},"source":["## 5.4 Update Parameters\n","\n","In this section you will update the parameters of the model, using gradient descent: \n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n","\n","where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "]},{"cell_type":"code","metadata":{"id":"W4fyZBZUxcQK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621062004391,"user_tz":-480,"elapsed":940,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"ffd463ac-8d39-4226-fd9c-12ccd0c4e4ad"},"source":["def update_parameters(parameters, grads, learning_rate):\n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    for l in range(L):\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n","    return parameters\n","\n","parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(parameters[\"W1\"]))\n","print (\"b1 = \"+ str(parameters[\"b1\"]))\n","print (\"W2 = \"+ str(parameters[\"W2\"]))\n","print (\"b2 = \"+ str(parameters[\"b2\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n"," [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n"," [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n","b1 = [[-0.04659241]\n"," [-1.28888275]\n"," [ 0.53405496]]\n","W2 = [[-0.55569196  0.0354055   1.32964895]]\n","b2 = [[-0.84610769]]\n"],"name":"stdout"}]}]}