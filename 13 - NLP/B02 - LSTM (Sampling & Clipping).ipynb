{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"nlp-sequence-models","graded_item_id":"1dYg0","launcher_item_id":"MLhxP"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[],"toc_visible":true}},"cells":[{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/dinos.txt -P datasets"],"metadata":{"id":"H4Q5vrT9uBsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ka6dQ-KJm9p"},"source":["import numpy as np\n","import random\n","import pprint\n","from lib.utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CR6g5prHJm9r"},"source":["# 1) Data Loading"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oRS8tXmtJm9r","executionInfo":{"status":"ok","timestamp":1665754849794,"user_tz":-480,"elapsed":5,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"540cadc5-d272-4f0f-af80-468663880efa"},"source":["data = open('./datasets/dinos.txt', 'r').read()\n","data = data.lower()\n","chars = list(set(data))\n","data_size, vocab_size = len(data), len(chars)\n","print('There are %d total data and %d unique characters in your data.' % (data_size, vocab_size))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 19909 total characters and 27 unique characters in your data.\n"]}]},{"cell_type":"markdown","metadata":{"id":"E6zEpuLcJm9s"},"source":["\n","* The characters are a-z (26 characters) plus the \"\\n\" (or newline character).\n","* In this assignment, the newline character \"\\n\" plays a role similar to the `<EOS>` (or \"End of sentence\") token we had discussed in lecture.  \n","    - Here, \"\\n\" indicates the end of the dinosaur name rather than the end of a sentence.\n","* `char_to_ix`: In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26.\n","* `ix_to_char`: We also create a second python dictionary that maps each index back to the corresponding character.\n","    -  This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DhoDvUXmJm9t","executionInfo":{"status":"ok","timestamp":1621067716144,"user_tz":-480,"elapsed":775,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"2454be5c-90b3-40f3-d5d4-3be48116d645"},"source":["chars = sorted(chars)\n","print(chars)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"A5KzpN8ZJm9t","executionInfo":{"status":"ok","timestamp":1621067718080,"user_tz":-480,"elapsed":836,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"f1fc4cf2-1b5a-473d-a6b3-280519a20c8c"},"source":["char_to_ix = {ch:i for i,ch in enumerate(chars) }\n","ix_to_char = {i:ch for i,ch in enumerate(chars) }\n","ix_to_char"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: '\\n',\n"," 1: 'a',\n"," 2: 'b',\n"," 3: 'c',\n"," 4: 'd',\n"," 5: 'e',\n"," 6: 'f',\n"," 7: 'g',\n"," 8: 'h',\n"," 9: 'i',\n"," 10: 'j',\n"," 11: 'k',\n"," 12: 'l',\n"," 13: 'm',\n"," 14: 'n',\n"," 15: 'o',\n"," 16: 'p',\n"," 17: 'q',\n"," 18: 'r',\n"," 19: 's',\n"," 20: 't',\n"," 21: 'u',\n"," 22: 'v',\n"," 23: 'w',\n"," 24: 'x',\n"," 25: 'y',\n"," 26: 'z'}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"uCpDja4MJm9v"},"source":["# 2) Building blocks of the model\n","\n","Your model will have the following structure:\n","\n","- Initialize parameters\n","- Run the optimization loop\n","    - Forward propagation to compute the loss function\n","    - Backward propagation to compute the gradients with respect to the loss function\n","    - Clip the gradients to avoid exploding gradients\n","    - Using the gradients, update your parameters with the gradient descent update rule.\n","- Return the learned parameters\n","    \n","<img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/blob/master/05_sequence_models/02_character_level_language_model_with_numpy_rnn/images/rnn.png?raw=true\" style=\"width:450;height:300px;\">\n","<caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook \"Building a Recurrent Neural Network - Step by Step\".  </center></caption>\n","\n","* At each time-step, the RNN tries to predict what is the next character given the previous characters.\n","* The dataset $\\mathbf{X} = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$ is a list of characters in the training set.\n","* $\\mathbf{Y} = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$ is the same list of characters but shifted one character forward.\n","* At every time-step $t$, $y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$.  The prediction at time $t$ is the same as the input at time $t + 1$.\n"]},{"cell_type":"markdown","metadata":{"id":"WniOe_7lJm9v"},"source":["## 2.1 Clipping the gradients in the optimization loop\n","\n","**Exploding gradients**\n","* When gradients are very large, they're called \"exploding gradients.\n","* Exploding gradients make the training process more difficult, because the updates may be so large that they \"overshoot\" the optimal values during back propagation.\n","\n","Recall that your overall loop structure usually consists of:\n","* forward pass,\n","* cost computation,\n","* backward pass,\n","* parameter update.\n","\n","Before updating the parameters, you will perform gradient clipping to make sure that your gradients are not \"exploding.\"\n","\n","**gradient clipping**\n","In the exercise below, you will implement a function `clip` that takes in a dictionary of gradients and returns a clipped version of gradients if needed.\n","* There are different ways to clip gradients.\n","* We will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to lie between some range [-N, N].\n","* For example, if the N=10\n","    - The range is [-10, 10]\n","    - If any component of the gradient vector is greater than 10, it is set to 10.\n","    - If any component of the gradient vector is less than -10, it is set to -10.\n","    - If any components are between -10 and 10, they keep their original values.\n","\n","<img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/blob/master/05_sequence_models/02_character_level_language_model_with_numpy_rnn/images/clip.png?raw=true\" style=\"width:400;height:150px;\">\n","<caption><center> **Figure 2**: Visualization of gradient descent with and without gradient clipping, in a case where the network is running into \"exploding gradient\" problems. </center></caption>"]},{"cell_type":"code","metadata":{"id":"nxoCaFW3Jm9w"},"source":["def clip(gradients, maxValue):\n","    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n","\n","    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]\n","    for gradient in [dWax, dWaa, dWya, db, dby]:\n","        np.clip(gradient, -maxValue, maxValue, out=gradient)\n","    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n","    return gradients"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rFSixiLEJm9y","executionInfo":{"status":"ok","timestamp":1621067858343,"user_tz":-480,"elapsed":648,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"b10a633a-7623-4b51-a275-2e2ce6a79312"},"source":["# Test with a maxValue of 5\n","maxValue = 5\n","dWax = np.random.randn(5,3)*10\n","dWaa = np.random.randn(5,5)*10\n","dWya = np.random.randn(2,5)*10\n","db = np.random.randn(5,1)*10\n","dby = np.random.randn(2,1)*10\n","gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n","gradients = clip(gradients, maxValue)\n","print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n","print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n","print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n","print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n","print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["gradients[\"dWaa\"][1][2] = 4.373479451817902\n","gradients[\"dWax\"][3][1] = 5.0\n","gradients[\"dWya\"][1][2] = 3.3458167583803884\n","gradients[\"db\"][4] = [-1.25954685]\n","gradients[\"dby\"][1] = [-3.3819235]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I4l1arK7Jm9z"},"source":["## 2.2 Sampling\n","\n","Now assume that your model is trained. You would like to generate new text (characters). The process of generation is explained in the picture below:\n","\n","<img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/blob/master/05_sequence_models/02_character_level_language_model_with_numpy_rnn/images/dinos3.png?raw=true\" style=\"width:500;height:300px;\">\n","<caption><center> **Figure 3**: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network sample one character at a time. </center></caption>"]},{"cell_type":"code","metadata":{"id":"adYfNohfJm94"},"source":["def sample(parameters, char_to_ix, seed=99):\n","\n","    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n","    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n","    vocab_size = by.shape[0]\n","    n_a = Waa.shape[1]\n","\n","    # Step 1: Create the a zero vector x that can be used as the one-hot vector\n","    x = np.zeros((vocab_size, 1))\n","    a_prev = np.zeros((n_a, 1))\n","\n","    # Create an empty list of indices\n","    indices = []\n","    idx = -1\n","\n","    # Loop over time-steps t. At each time-step:\n","    # sample a character from a probability distribution and append its index (`idx`) to the list \"indices\".\n","    # We'll stop if we reach 50 characters\n","    # Setting the maximum number of characters helps with debugging and prevents infinite loops.\n","    counter = 0\n","    newline_character = char_to_ix['\\n']\n","\n","    while (idx != newline_character and counter != 50):\n","        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n","        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)\n","        z = np.dot(Wya, a) + by\n","        y = softmax(z)\n","\n","        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n","        idx = np.random.choice(list(range(vocab_size)), p=y.ravel())\n","\n","        # Append the index to \"indices\"\n","        indices.append(idx)\n","\n","        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n","        x = np.zeros((vocab_size, 1))\n","        x[idx] = 1\n","\n","        # Update \"a_prev\" to be \"a\"\n","        a_prev = a\n","        counter +=1\n","\n","    if (counter == 50):\n","        indices.append(char_to_ix['\\n'])\n","    return indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPSbWhH-Jm95","executionInfo":{"status":"ok","timestamp":1621067973150,"user_tz":-480,"elapsed":619,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"0bac3633-e182-4548-8db6-087e303c6d63"},"source":["_, n_a = 20, 100\n","Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n","b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n","parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n","\n","indices = sample(parameters, char_to_ix, 0)\n","print(\"Sampling:\")\n","print(\"list of sampled indices:\\n\", indices)\n","print(\"list of sampled characters:\\n\", [ix_to_char[i] for i in indices])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sampling:\n","list of sampled indices:\n"," [16, 25, 17, 5, 7, 12, 15, 16, 1, 15, 26, 5, 15, 21, 9, 26, 23, 16, 15, 10, 1, 15, 8, 0]\n","list of sampled characters:\n"," ['p', 'y', 'q', 'e', 'g', 'l', 'o', 'p', 'a', 'o', 'z', 'e', 'o', 'u', 'i', 'z', 'w', 'p', 'o', 'j', 'a', 'o', 'h', '\\n']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JWYamEf5Jm95"},"source":["# 3) Model Building\n","\n","* Given the dataset of dinosaur names, we use each line of the dataset (one name) as one training example.\n","* Every 100 steps of stochastic gradient descent, you will sample 10 randomly chosen names to see how the algorithm is doing.\n","* Remember to shuffle the dataset, so that stochastic gradient descent visits the examples in random order."]},{"cell_type":"code","metadata":{"id":"g0I4wDw9Jm97"},"source":["def clip(gradients, maxValue):\n","    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n","\n","    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]\n","    for gradient in [dWax, dWaa, dWya, db, dby]:\n","        np.clip(gradient, -maxValue, maxValue, out=gradient)\n","\n","    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n","    return gradients\n","\n","def initialize_parameters(n_a, n_x, n_y):\n","    Wax = np.random.randn(n_a, n_x)*0.01 # input to hidden (50, 27)\n","    Waa = np.random.randn(n_a, n_a)*0.01 # hidden to hidden (50, 50)\n","    Wya = np.random.randn(n_y, n_a)*0.01 # hidden to output (27, 50)\n","    b = np.zeros((n_a, 1)) # hidden bias (50, 1)\n","    by = np.zeros((n_y, 1)) # output bias (27, 1)\n","\n","    parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b,\"by\": by}\n","    return parameters\n","\n","def get_initial_loss(vocab_size, seq_length):\n","    return -np.log(1.0 / vocab_size) * seq_length\n","\n","def smooth(loss, cur_loss):\n","    return loss * 0.999 + cur_loss * 0.001\n","\n","def softmax(x):\n","    e_x = np.exp(x - np.max(x))\n","    return e_x / e_x.sum(axis=0)\n","\n","def rnn_step_forward(parameters, a_prev, x):\n","    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n","    a_next = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b) # (50, 1)\n","    p_t = softmax(np.dot(Wya, a_next) + by) # (27, 1)\n","    return a_next, p_t\n","\n","def rnn_forward(X, Y, a0, parameters, vocab_size=27):\n","    # Initialize x, a and y_hat as empty dictionaries\n","    x, a, y_hat = {}, {}, {}\n","    a[-1] = np.copy(a0) # (50, 1)\n","\n","    # initialize your loss to 0\n","    loss = 0\n","    for t in range(len(X)):\n","        # Set x[t] to be the one-hot vector representation, if X[t] == None, we just have x[t]=0.\n","        x[t] = np.zeros((vocab_size, 1)) # (27, 1)\n","        if (X[t] != None):\n","            x[t][X[t]] = 1\n","\n","        # Run one step forward of the RNN\n","        a[t], y_hat[t] = rnn_step_forward(parameters, a[t-1], x[t])\n","\n","        # For Softmax, Update the loss by substracting the cross-entropy term of this time-step from it.\n","        loss -= np.log(y_hat[t][Y[t], 0])\n","    cache = (y_hat, a, x)\n","    return loss, cache\n","\n","def update_parameters(parameters, gradients, lr):\n","    parameters['Wax'] += -lr * gradients['dWax']\n","    parameters['Waa'] += -lr * gradients['dWaa']\n","    parameters['Wya'] += -lr * gradients['dWya']\n","    parameters['b'] += -lr * gradients['db']\n","    parameters['by'] += -lr * gradients['dby']\n","    return parameters\n","\n","def rnn_step_backward(dy, gradients, parameters, x, a, a_prev):\n","    gradients['dWya'] += np.dot(dy, a.T)\n","    gradients['dby'] += dy\n","    da = np.dot(parameters['Wya'].T, dy) + gradients['da_next'] # backprop into h\n","    daraw = (1 - a * a) * da # backprop through tanh nonlinearity\n","    gradients['db'] += daraw\n","    gradients['dWax'] += np.dot(daraw, x.T)\n","    gradients['dWaa'] += np.dot(daraw, a_prev.T)\n","    gradients['da_next'] = np.dot(parameters['Waa'].T, daraw)\n","    return gradients\n","\n","def rnn_backward(X, Y, parameters, cache):\n","    gradients = {}\n","    (y_hat, a, x) = cache\n","    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n","\n","    # each one should be initialized to zeros of the same dimension as its corresponding parameter\n","    gradients['dWax'], gradients['dWaa'], gradients['dWya'] = np.zeros_like(Wax), np.zeros_like(Waa), np.zeros_like(Wya)\n","    gradients['db'], gradients['dby'] = np.zeros_like(b), np.zeros_like(by)\n","    gradients['da_next'] = np.zeros_like(a[0])\n","\n","    # Backpropagate through time\n","    for t in reversed(range(len(X))):\n","        dy = np.copy(y_hat[t])\n","        dy[Y[t]] -= 1\n","        gradients = rnn_step_backward(dy, gradients, parameters, x[t], a[t], a[t-1])\n","    return gradients, a\n","\n","def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n","    # Forward propagate through time\n","    loss, cache = rnn_forward(X, Y, a_prev, parameters)\n","    # Backpropagate through time\n","    gradients, a = rnn_backward(X, Y, parameters, cache)\n","    # Clip your gradients between -5 (min) and 5 (max)\n","    gradients = clip(gradients, 5)\n","    # Update parameters\n","    parameters = update_parameters(parameters, gradients, learning_rate)\n","    return loss, gradients, a[len(X)-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcWck-JmJm97"},"source":["def model(data, ix_to_char, char_to_ix, num_iterations=2000, n_a=50, dino_names=7, vocab_size=27):\n","\n","    # Retrieve n_x and n_y from vocab_size\n","    n_x, n_y = vocab_size, vocab_size # 27\n","\n","    # Initialize parameters\n","    parameters = initialize_parameters(n_a, n_x, n_y)\n","\n","    # Initialize loss (this is required because we want to smooth our loss)\n","    loss = get_initial_loss(vocab_size, dino_names) # scalar\n","\n","    # Build list of all dinosaur names (training examples).\n","    with open(\"./datasets/dinos.txt\") as f:\n","        examples = f.readlines()\n","    examples = [x.lower().strip() for x in examples]\n","\n","    # Shuffle list of all dinosaur names\n","    np.random.shuffle(examples)\n","\n","    # Initialize the hidden state of your LSTM\n","    a_prev = np.zeros((n_a, 1)) #(50, 1)\n","\n","    # Optimization loop\n","    for j in range(num_iterations):\n","        # define one training example (X,Y)\n","        index = j % len(examples)\n","        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n","        Y = X[1:] + [char_to_ix[\"\\n\"]]\n","\n","        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n","        # Choose a learning rate of 0.01\n","        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)\n","\n","        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n","        loss = smooth(loss, curr_loss)\n","\n","        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n","        if j % 2000 == 0:\n","            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n","\n","            # The number of dinosaur names to print\n","            for name in range(dino_names):\n","                sampled_indices = sample(parameters, char_to_ix)\n","                print_sample(sampled_indices, ix_to_char)\n","            print('\\n')\n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"Np8-dC07Jm9-","executionInfo":{"status":"ok","timestamp":1621068077898,"user_tz":-480,"elapsed":3221,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"fefb8bb9-66cf-408a-9ac7-707a4b88b0fe"},"source":["parameters = model(data, ix_to_char, char_to_ix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Iteration: 0, Loss: 23.100519\n","\n","Emxlxjtwaqckolvpapftmkwblkrifncrequcgfxqrcpuqysthu\n","Laepgprgjarmtczzcobdtpzqwcgai\n","Ticwtwahdjirlbygznwdtovowmcbaguoh\n","Ywyughwhgionja\n","Tomgvgdanurktsptezrxtagxjutvmjeflgadtsdsmcxdznvgbd\n","Dhbb\n","Lukoeodeztwzzcggnarppaddjnfphluwjhypkopwbmifailf\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N8XyHL2ZJm9-"},"source":["```Python\n","Iteration: 34000, Loss: 22.447230\n","\n","Onyxipaledisons\n","Kiabaeropa\n","Lussiamang\n","Pacaeptabalsaurus\n","Xosalong\n","Eiacoteg\n","Troia\n","```"]},{"cell_type":"markdown","metadata":{"id":"YzXGBmx0Jm9_"},"source":["The RNN-Shakespeare model is very similar to the one you have built for dinosaur names. The only major differences are:\n","- LSTMs instead of the basic RNN to capture longer-range dependencies\n","- The model is a deeper, stacked LSTM model (2 layer)\n","- Using Keras instead of python to simplify the code"]},{"cell_type":"markdown","metadata":{"id":"pnobe7MEJm9_"},"source":["**References**:\n","- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n","- For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py"]}]}