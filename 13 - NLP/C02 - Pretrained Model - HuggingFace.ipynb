{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kYrE7gINKHDl"},"source":["!pip install transformers\n","!pip install tensorflow_addons\n","!curl -LO https://raw.githubusercontent.com/MohamadMerchant/SNLI/master/data.tar.gz\n","!tar -xvzf data.tar.gz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s50Fc-Bxe7VS"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import transformers\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.utils import plot_model\n","import logging\n","logging.getLogger('tensorflow').disabled = True\n","pd.set_option('max_colwidth', 400)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eh9IhQKeNZDu","executionInfo":{"status":"ok","timestamp":1620062580540,"user_tz":-480,"elapsed":61298,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"2896c54e-6131-4ff0-f5af-cf42daaf92de"},"source":["# Define the strategy to use and print the number of devices found\n","strategy = tf.distribute.MirroredStrategy()\n","print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of devices: 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xe_VxTVCNWf_"},"source":["# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n","# tf.config.experimental_connect_to_cluster(resolver)\n","# # This is the TPU initialization code that has to be at the beginning.\n","# tf.tpu.experimental.initialize_tpu_system(resolver)\n","# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n","# strategy = tf.distribute.TPUStrategy(resolver)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExQOqQRx-vm7"},"source":["# 1) HuggingFace - Text Classification"]},{"cell_type":"markdown","metadata":{"id":"9P9qEFOT-zLi"},"source":["## 1.1 Data Import"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"3wdjxyP9-lxA","executionInfo":{"status":"ok","timestamp":1619189573688,"user_tz":-480,"elapsed":4214,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"a526d3b4-6ce7-4942-eff0-557a4c42fad4"},"source":["train_df = pd.read_csv(\"./data/SNLI_Corpus/snli_1.0_train.csv\", nrows=100000)\n","valid_df = pd.read_csv(\"./data/SNLI_Corpus/snli_1.0_dev.csv\")\n","test_df = pd.read_csv(\"./data/SNLI_Corpus/snli_1.0_test.csv\")\n","\n","train_df = train_df[train_df.similarity != \"-\"].sample(frac=1.0, random_state=42).reset_index(drop=True)\n","valid_df = valid_df[valid_df.similarity != \"-\"].sample(frac=1.0, random_state=42).reset_index(drop=True)\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>similarity</th>\n","      <th>sentence1</th>\n","      <th>sentence2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>contradiction</td>\n","      <td>A woman is using toy which blows giant bubbles.</td>\n","      <td>A little girl is playing with chalk on a driveway.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neutral</td>\n","      <td>A young Asian girl holds a stuffed cat toy in a classroom.</td>\n","      <td>A young Asian girl sits in class with a stuffed cat toy, the only surviving possession remaining after the tsunami.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>entailment</td>\n","      <td>A young woman with an afro and an electronic device in her hands walks next to an orange bike.</td>\n","      <td>A young woman walks next to an orange bike.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>neutral</td>\n","      <td>A young asian girl is sliding down a pole on outdoor playground equipment.</td>\n","      <td>The girl has yellow skin</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>entailment</td>\n","      <td>a man is walking with a cane.</td>\n","      <td>The man is walking.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      similarity  ...                                                                                                            sentence2\n","0  contradiction  ...                                                                   A little girl is playing with chalk on a driveway.\n","1        neutral  ...  A young Asian girl sits in class with a stuffed cat toy, the only surviving possession remaining after the tsunami.\n","2     entailment  ...                                                                          A young woman walks next to an orange bike.\n","3        neutral  ...                                                                                             The girl has yellow skin\n","4     entailment  ...                                                                                                  The man is walking.\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"WtZfqUvxBnlx"},"source":["# label encoding\n","label_map = dict(enumerate(train_df['similarity'].astype('category').cat.categories))\n","y_train = train_df['similarity'].map({v:k for k, v in label_map.items()}).values\n","y_val = valid_df['similarity'].map({v:k for k, v in label_map.items()}).values\n","y_test = test_df['similarity'].map({v:k for k, v in label_map.items()}).values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AmKQOygICQ74"},"source":["## 1.2 Pre-processing"]},{"cell_type":"code","metadata":{"id":"LKtbploBnuWS"},"source":["max_length = 64\n","batch_size = 32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkPrtg5RDaLH"},"source":["tokenizer = transformers.BertTokenizer.from_pretrained( \"bert-base-uncased\", do_lower_case=True)\n","print(len(tokenizer.get_vocab()))\n","tokenizer.encode('Hello Tensorflow')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fiWf1kBnJp7q","executionInfo":{"status":"ok","timestamp":1619063830546,"user_tz":-480,"elapsed":12151,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"6d0620d3-ab1e-4ad7-fa0d-daa756ea8431"},"source":["sentence_pairs = train_df[[\"sentence1\", \"sentence2\"]].values[:5]\n","encoded = tokenizer.batch_encode_plus(\n","    sentence_pairs.tolist(),\n","    add_special_tokens=True,\n","    max_length=max_length,\n","    return_attention_mask=True,\n","    return_token_type_ids=True,\n","    padding='max_length',\n","    return_tensors=\"tf\")\n","\n","print(encoded.keys())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHm47JjILO6-","executionInfo":{"status":"ok","timestamp":1619063830547,"user_tz":-480,"elapsed":11899,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"0185e0c5-8c60-48a1-e8a4-9aa07b18eabc"},"source":["print(encoded['input_ids'][0][:32])\n","print(encoded['token_type_ids'][0][:32])\n","print(encoded['attention_mask'][0][:32])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[  101  1037  2450  2003  2478  9121  2029 13783  5016 17255  1012   102\n","  1037  2210  2611  2003  2652  2007 16833  2006  1037 11202  1012   102\n","     0     0     0     0     0     0     0     0], shape=(32,), dtype=int32)\n","tf.Tensor([0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0], shape=(32,), dtype=int32)\n","tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0], shape=(32,), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"KyI0-QBbJpBJ","executionInfo":{"status":"ok","timestamp":1619063830548,"user_tz":-480,"elapsed":11659,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"401529cf-3589-411b-b7c9-8cb1dcbc86d2"},"source":["tokenizer.decode(encoded['input_ids'][0][:32])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'[CLS] a woman is using toy which blows giant bubbles. [SEP] a little girl is playing with chalk on a driveway. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"iWVSSZ86_pmV"},"source":["class BertDataGenerator(tf.keras.utils.Sequence):\n","    \"\"\"Generates batches of data.\n","\n","    Args:\n","        sentence_pairs: Array of premise and hypothesis input sentences.\n","        labels: Array of labels.\n","        batch_size: Integer batch size.\n","        shuffle: boolean, whether to shuffle the data.\n","        include_targets: boolean, whether to incude the labels.\n","\n","    Returns:\n","        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n","        (or just `[input_ids, attention_mask, `token_type_ids]`\n","         if `include_targets=False`)\n","    \"\"\"\n","\n","    def __init__(self, sentence_pairs, labels, batch_size=batch_size, shuffle=True, include_targets=True):\n","\n","        self.sentence_pairs = sentence_pairs\n","        self.labels = labels\n","        self.shuffle = shuffle\n","        self.batch_size = batch_size\n","        self.include_targets = include_targets\n","        self.tokenizer = transformers.BertTokenizer.from_pretrained( \"bert-base-uncased\", do_lower_case=True)\n","        self.indexes = np.arange(len(self.sentence_pairs))\n","        self.on_epoch_end()\n","\n","    def __len__(self):\n","        # Denotes the number of batches per epoch.\n","        return len(self.sentence_pairs) // self.batch_size\n","\n","    def __getitem__(self, idx):\n","        # Retrieves the batch of index.\n","        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n","        sentence_pairs = self.sentence_pairs[indexes]\n","\n","        # With BERT tokenizer's batch_encode_plus batch of both the sentences are, encoded together and separated by [SEP] token.\n","        encoded = self.tokenizer.batch_encode_plus(\n","            sentence_pairs.tolist(),\n","            add_special_tokens=True,\n","            max_length=max_length,\n","            return_attention_mask=True,\n","            return_token_type_ids=True,\n","            padding='max_length',\n","            return_tensors=\"tf\",\n","        )\n","\n","        # Convert batch of encoded features to numpy array.\n","        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n","        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n","        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n","\n","        # Set to true if data generator is used for training/validation.\n","        if self.include_targets:\n","            labels = np.array(self.labels[indexes], dtype=\"int32\")\n","            return [input_ids, attention_masks, token_type_ids], labels\n","        else:\n","            return [input_ids, attention_masks, token_type_ids]\n","\n","    def on_epoch_end(self):\n","        # Shuffle indexes after each epoch if shuffle is set to True.\n","        if self.shuffle:\n","            np.random.RandomState(42).shuffle(self.indexes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VVeARI6mHDta"},"source":["## 1.3 Model Building"]},{"cell_type":"markdown","metadata":{"id":"_pgfi34deBZt"},"source":["Outputs of Bert Model comprised of:\n","\n","- ***last_hidden_state*** with shape=(m, seq_len, embed_dim). The first token of every last_hidden_state is the special classification token â€“ [CLS]. This token is used in classification tasks as an aggregate of the entire sequence representation. It is ignored in non-classification tasks.\n","\n","- ***pooler_output*** represent each input sequence as a whole with shape=(m, emb_dim). Last layer hidden-state of the first token of the sequence [CLS] further processed by a Linear layer and a Tanh activation function.\n","\n","- ***hidden_states*** which generate the hidden state for all transformer layers. Only when set *output_hidden_states=True*, shape=(num_layers, m, seq_len, emb_dim).  In a 4-layers BERT model a token will have 4 intermediate representations. The last value of the list is equal to **last_hidden_state**.\n","\n","- ***attentions*** attention weights from each layer.  Only when set *output_attentions=True*, shape=(num_layers, m, seq_len, emb_dim)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ic7tTYRbT4Uf"},"source":["def build_model():\n","\n","  # Encoded token ids from BERT tokenizer\n","  input_ids = tf.keras.layers.Input(shape=(max_length, ), dtype=tf.int32, name=\"input_ids\")\n","  # Attention masks indicates to the model which tokens should be attended to\n","  attention_masks = tf.keras.layers.Input(shape=(max_length, ), dtype=tf.int32, name=\"attention_masks\")\n","  # Token type ids are binary masks identifying different sequences in the model\n","  token_type_ids = tf.keras.layers.Input(shape=(max_length, ), dtype=tf.int32, name=\"token_type_ids\")\n","\n","  # Loading pretrained BERT model, freeze the weight, check bert_model.config to configure\n","  bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n","  bert_model.trainable = False\n","\n","  bert_output = bert_model(input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids, output_attentions=False, output_hidden_states=False)\n","  sequence_output = bert_output[\"last_hidden_state\"] # (m, seq_len, emb_dim)\n","  pooled_output = bert_output[\"pooler_output\"] # (m, emb_dim)\n","\n","  # Add trainable layers on top of Bert to adapt the pretrained features on the new data.\n","  bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(2, return_sequences=True))(sequence_output) # (m, emb_dim, hidden_unit * 2)\n","\n","  # Applying hybrid pooling approach to bi_lstm sequence output.\n","  avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm) # (m, hidden_unit)\n","  max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm) # (m, hidden_unit)\n","  concat = tf.keras.layers.concatenate([avg_pool, max_pool]) # (m, hidden_unit)\n","  dropout = tf.keras.layers.Dropout(0.3)(concat)\n","  output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout) #(m, 3)\n","\n","  model = tf.keras.models.Model(inputs=[input_ids, attention_masks, token_type_ids], outputs=output)\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6-a6ZpahyEy"},"source":["bert_encoder = build_model()\n","bert_encoder.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"sparse_categorical_crossentropy\", metrics=[\"acc\"])\n","bert_encoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D_BilrsSlp2d"},"source":["## 1.4 Model Training (Frozen Pre-trained Bert)"]},{"cell_type":"code","metadata":{"id":"zfpkxlWKlxAh"},"source":["# generate batch data\n","train_data = BertDataGenerator(train_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"), y_train, batch_size=batch_size, shuffle=True)\n","valid_data = BertDataGenerator(valid_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"), y_val, batch_size=batch_size, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90kThjWYl4mk"},"source":["epochs = 2\n","history = bert_encoder.fit(train_data, validation_data=valid_data, epochs=epochs, use_multiprocessing=True, workers=-1, steps_per_epoch=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hedVSC45nKuv"},"source":["## 1.5 Fine Tuning Bert\n","\n","This step must only be performed after the feature extraction model has been trained to convergence on the new data.\n","\n","This is an optional last step where bert_model is unfreezed and retrained with a very low learning rate. This can deliver meaningful improvement by incrementally adapting the pretrained features to the new data."]},{"cell_type":"code","metadata":{"id":"DTOFmOgpmj1r"},"source":["bert_encoder.layers[3].trainable = True\n","# Recompile the model to make the change effective.\n","bert_encoder.compile(optimizer=tf.keras.optimizers.Adam(1e-5), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","bert_encoder.summary()\n","\n","# train entire model\n","history = bert_encoder.fit(train_data, validation_data=valid_data, epochs=epochs, use_multiprocessing=True, workers=-1,)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YlJSwbZ9pgrR"},"source":["## 1.6 Evaluation"]},{"cell_type":"code","metadata":{"id":"NQL6zvYLph6t"},"source":["test_data = BertDataGenerator(test_df[[\"sentence1\", \"sentence2\"]].values.astype(\"str\"), y_test, batch_size=batch_size, shuffle=False)\n","model.evaluate(test_data, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-34EWG73qfS_"},"source":["def check_similarity(sentence1, sentence2):\n","    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n","    test_data = BertDataGenerator(sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False)\n","\n","    proba = model.predict(test_data)[0]\n","    idx = np.argmax(proba)\n","    proba = f\"{proba[idx]: .2f}%\"\n","    pred = label_map.get(idx)\n","    return pred, proba"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFq7v5J6rG_7"},"source":["sentence1 = \"Two women are observing something together.\"\n","sentence2 = \"Two women are standing with their eyes closed.\"\n","check_similarity(sentence1, sentence2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2) Pytorch Lightning"],"metadata":{"id":"2rDM8ZE03enq"}},{"cell_type":"code","source":["!pip install iterative_train_test_split tqdm mlflow"],"metadata":{"id":"Seg4zKOy3gQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import sys, os\n","import yaml, logging\n","import mlflow\n","from mlflow.tracking import MlflowClient\n","import mlflow.pytorch\n","from skmultilearn.model_selection import iterative_train_test_split\n","import transformers\n","import torch\n","import pytorch_lightning as pl\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from sklearn.metrics import classification_report, f1_score\n","from tqdm import tqdm\n","from typing import Optional, Union"],"metadata":{"id":"9pMHbLji3h0U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.1 Data Processing"],"metadata":{"id":"OofEyfb23lJG"}},{"cell_type":"code","source":["def split_data(\n","    df:pd.DataFrame,\n","    aspect_classes:list,\n","    x_col:list=['text', 'label'],\n","    test_size:float=0.2,\n","    seed:int=0):\n","    '''Split data into test train set'''\n","\n","    np.random.seed(seed)\n","    x = df[x_col].values\n","    y = df[aspect_classes].values\n","    X_train, y_train, X_test, y_test = iterative_train_test_split(x, y, test_size=test_size)\n","    return pd.DataFrame(X_train, columns=x_col), pd.DataFrame(X_test, columns=x_col)\n","\n","\n","class DataModule(pl.LightningDataModule):\n","    def __init__(\n","        self, df_train:pd.DataFrame, df_test:pd.DataFrame, max_len:int, batch_size:int,\n","        tokenizer:str=\"distilbert-base-uncased\", text_col:str='text'\n","        ):\n","        '''Picking Up Raw Data and Processing'''\n","\n","        super().__init__()\n","        self.train_df = df_train\n","        self.test_df = df_test\n","        self.max_len = max_len\n","        self.batch_size = batch_size\n","        self.text_col = text_col\n","\n","        if tokenizer == \"distilbert-base-uncased\":\n","            logger.info(\"Applying Distillbert Tokenizer\")\n","            self.tokenizer = transformers.DistilBertTokenizer.from_pretrained(\n","                \"distilbert-base-uncased\")\n","        else:\n","            logger.info(\"Applying Bertweet Tokenizer\")\n","            self.tokenizer = transformers.BertweetTokenizer.from_pretrained(\n","                \"vinai/bertweet-base\", normalization=True)\n","\n","    class Dataset(Dataset):\n","        def __init__(self, encodings, labels):\n","            self.encodings = encodings\n","            self.labels = labels\n","\n","        def __getitem__(self, idx):\n","            item = {\n","                key: torch.tensor(val[idx]).clone().detach()\n","                for key, val in self.encodings.items()\n","                }\n","            item['labels'] = torch.tensor(self.labels[idx])\n","            return item\n","\n","        def __len__(self):\n","            return len(self.labels)\n","\n","    def train_dataloader(self):\n","        '''Return DataLoader for train tokens and labels'''\n","\n","        features = self.tokenizer(\n","            self.train_df[self.text_col].tolist(),\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding='max_length',\n","            return_tensors='pt')\n","\n","        labels = self.train_df['label'].tolist()\n","        dataset = self.Dataset(features, labels)\n","        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=10)\n","\n","    def val_dataloader(self):\n","        '''Return DataLoader for test tokens and labels'''\n","\n","        # only pad to longest length of the current batch\n","        features = self.tokenizer(\n","            self.test_df[self.text_col].tolist(),\n","            max_length=self.max_len,\n","            truncation=True,\n","            padding='longest',\n","            return_tensors='pt')\n","\n","        labels = self.test_df['label'].tolist()\n","        dataset = self.Dataset(features, labels)\n","        return DataLoader(dataset, batch_size=self.batch_size, shuffle=False, num_workers=10)\n","\n","    def calculate_pos_weights(self, class_counts, len_data):\n","        '''calculate weight for imbalance data'''\n","        pos_weights = np.ones_like(class_counts)\n","        neg_counts = [len_data - pos_count for pos_count in class_counts]\n","        for cdx, (pos_count, neg_count) in enumerate(zip(class_counts,  neg_counts)):\n","            pos_weights[cdx] = neg_count / (pos_count + 1e-5)\n","            # pos_weights[cdx] = 1. if pos_weights[cdx] == 0 else pos_weights[cdx]\n","        return torch.as_tensor(pos_weights, dtype=torch.float)\n","\n","    def get_weight(self, df, aspect_classes):\n","        return self.calculate_pos_weights(\n","             df[aspect_classes].sum().values,\n","             len(df)\n","         )"],"metadata":{"id":"dbbXewxt3jIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT = 'sample.csv'\n","X_COL = ['text']\n","Y_COL = ['ability', 'dependability', 'purpose', 'integrity']\n","\n","BATCH_SIZE = 32\n","INPUT_MAX_LEN = 128\n","STANDARD_LR = 5e-5\n","FINE_LR = 5e-7\n","EPOCHS = 20\n","LIMIT_STEP = 500\n","MODEL = \"vinai/bertweet-base\"\n","TEXT_COL = 'text'\n","\n","df = pd.read_parquet(INPUT)\n","df_train, df_val = split_data(df, Y_COL, X_COL, test_size=0.2, seed=0)\n","\n","data_module = DataModule(\n","    df_train,\n","    df_val,\n","    max_len=INPUT_MAX_LEN,\n","    batch_size=BATCH_SIZE,\n","    tokenizer=MODEL,\n","    text_col=TEXT_COL\n",")"],"metadata":{"id":"a5fNr4uj3oWP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.2 Model Training"],"metadata":{"id":"wtBCCizP3sD2"}},{"cell_type":"code","source":["class LightningArticleClassifier(pl.LightningModule):\n","    def __init__(\n","        self, output_class_len, learning_rate,\n","        max_len=64, hidden_dim=64, pos_weight=None, bert_model=\"distilbert-base-uncased\"\n","        ):\n","\n","        super().__init__()\n","        self.max_len = max_len\n","        self.lr = learning_rate\n","        self.emb_dim = 768\n","        self.hidden_dim = hidden_dim\n","        self.drop_out = torch.nn.Dropout(0.1)\n","        self.fc1 = torch.nn.Linear(self.emb_dim, self.emb_dim // 2)\n","        self.fc2 = torch.nn.Linear(self.emb_dim // 2, self.hidden_dim * 4)\n","        self.fc3 = torch.nn.Linear(self.hidden_dim * 4, self.hidden_dim)\n","        self.fc4 = torch.nn.Linear(self.hidden_dim, output_class_len)\n","        self.tanh = torch.nn.Tanh()\n","        self.gelu = torch.nn.GELU()\n","        self.softmax = torch.nn.LogSoftmax(dim=1)\n","\n","        if pos_weight is not None:\n","            self.pos_weight = torch.tensor(pos_weight, dtype=torch.float)\n","        else:\n","            self.pos_weight = None\n","\n","        if bert_model == \"distilbert-base-uncased\":\n","            logger.info(\"Importing Distillbert Model\")\n","            self.bert_model = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n","        else:\n","            logger.info(\"Importing Bertweet Model\")\n","            self.bert_model = transformers.AutoModel.from_pretrained(\"vinai/bertweet-base\")\n","\n","        # metrics\n","        self.val_loss, self.val_corrects, self.val_len = 0., 0., 0.\n","        self.train_loss, self.train_corrects, self.train_len = 0., 0., 0.\n","        # self.train_f1, self.val_f1 = 0., 0.\n","        self.train_step, self.val_step = 0, 0\n","        self.epoch_loss_train, self.epoch_acc_train,  self.epoch_f1_train = [], [], []\n","        self.epoch_loss_val, self.epoch_acc_val, self.epoch_f1_val = [], [], []\n","\n","    def forward(self, input_ids, attention_mask):\n","        bert_output = self.bert_model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            output_attentions=False,\n","            output_hidden_states=False)\n","\n","        # feed forward layer\n","        # output = bert_output['pooled_output'][:, :]\n","        output = bert_output['last_hidden_state'][:, 0, :]\n","        output = self.fc1(output)\n","        output = self.tanh(output)\n","        output = self.drop_out(output)\n","        output = self.fc2(output)\n","        output = self.gelu(output)\n","        output = self.drop_out(output)\n","        output = self.fc3(output)\n","        output = self.gelu(output)\n","        output = self.drop_out(output)\n","        output = self.fc4(output)\n","        # output = torch.sigmoid(output)\n","        # output = self.softmax(output)\n","        return output\n","\n","    def criterion(self, y_pred, y_true):\n","\n","        if self.pos_weight != None:\n","            # criterion = torch.nn.CrossEntropyLoss(weight=self.pos_weight.cuda())\n","            criterion = torch.nn.BCEWithLogitsLoss(pos_weight=self.pos_weight.cuda())\n","        else:\n","            # criterion = torch.nn.CrossEntropyLoss()\n","            criterion = torch.nn.BCEWithLogitsLoss()\n","        return criterion(y_pred, y_true.float())\n","\n","    def training_step(self, train_batch, batch_idx):\n","        input_ids = train_batch['input_ids']\n","        attention_mask = train_batch['attention_mask']\n","        labels = train_batch['labels']\n","        y_pred = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(y_pred, labels)\n","        # _, preds = torch.max(y_pred, 1)\n","        preds = (y_pred >= 0.5).int()\n","\n","        self.train_loss += loss\n","        # self.train_corrects += torch.sum(torch.sum(preds == labels.data))\n","        self.train_corrects += torch.sum(torch.all(torch.eq(preds, labels), dim=1).int())\n","        # self.train_f1 += f1_score(preds.cpu(), labels.data.cpu(), average='macro')\n","\n","        self.train_len += len(labels)\n","        self.train_step += 1\n","        self.log('train_loss', loss)\n","\n","        return loss\n","\n","    def training_epoch_end(self, out):\n","        self.epoch_acc_train.append(self.train_corrects / (self.train_len + 1))\n","        self.epoch_loss_train.append(self.train_loss / (self.train_len + 1))\n","        # self.epoch_f1_train.append(self.train_f1 / self.train_step)\n","\n","        if self.current_epoch % 2 == 0:\n","            logger.info(f'\\nEpoch: {self.current_epoch}')\n","            logger.info(f'Training: loss: {self.epoch_loss_train[-1]}')\n","            logger.info(f'Training: Accuracy: {self.epoch_acc_train[-1]}')\n","            # print(f'Training: Macro F1: {self.epoch_f1_train[-1]}')\n","\n","        self.train_loss, self.train_corrects = 0., 0.\n","        self.train_step, self.train_len = 0., 0.\n","\n","    def validation_step(self, val_batch, batch_idx):\n","        input_ids = val_batch['input_ids']\n","        attention_mask = val_batch['attention_mask']\n","        labels = val_batch['labels']\n","        y_pred = self.forward(input_ids, attention_mask)\n","        loss = self.criterion(y_pred, labels)\n","        preds = (y_pred >= 0.5).int()\n","        # _, preds = torch.max(y_pred, 1)\n","\n","        self.val_loss += loss\n","        # self.val_corrects += torch.sum(torch.sum(preds == labels.data))\n","        self.val_corrects += torch.sum(torch.all(torch.eq(preds, labels), dim=1).int())\n","\n","        # self.val_f1 += f1_score(preds.cpu(), labels.data.cpu(), average='macro')\n","\n","        self.val_len += len(labels)\n","        self.val_step += 1\n","        self.log('val_loss', loss)\n","        return loss\n","\n","    def validation_epoch_end(self, out):\n","        self.epoch_acc_val.append(self.val_corrects / (self.val_len + 1))\n","        self.epoch_loss_val.append(self.val_loss / (self.val_len + 1))\n","        # self.epoch_f1_val.append(self.val_f1 / self.val_step)\n","\n","        if self.current_epoch % 2 == 0:\n","            logger.info(f'Validation: loss: {self.epoch_loss_val[-1]}')\n","            logger.info(f'Validation: Accuracy: {self.epoch_acc_val[-1]}')\n","            # print(f'Validation: Macro F1: {self.epoch_f1_val[-1]}')\n","\n","        self.val_loss, self.val_corrects = 0., 0.\n","        self.val_step, self.val_len = 0., 0.\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        '''\n","        lr_scheduler = {'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min'),\n","                        \"monitor\": \"train_loss\",\n","                       }\n","\n","        return [optimizer], [lr_scheduler]\n","        '''\n","        return optimizer"],"metadata":{"id":"g7M-cVWY3qK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = LightningArticleClassifier(\n","    output_class_len=len(aspect_classes),\n","    learning_rate=STANDARD_LR,\n","    max_len=INPUT_MAX_LEN,\n","    # pos_weight=[1, 5, 1, 1, 1]\n","    bert_model=MODEL\n",")\n","\n","trainer = pl.Trainer(\n","    max_epochs=EPOCHS,\n","    limit_train_batches=LIMIT_STEP,\n","    accelerator=\"gpu\",\n","    strategy=\"dp\",\n","    gpus=-1,\n","    # accelerator='ddp',\n","    # default_root_dir='/dbfs/FileStore/temp/kean_temp/logs'\n",")\n","\n","for param in model.bert_model.parameters():\n","    param.requires_grad = False\n","trainer.fit(model, data_module)\n","\n","### fine tuning\n","if FINE_LR is not None:\n","    trainer = pl.Trainer(\n","        max_epochs=5,\n","        limit_train_batches=LIMIT_STEP,\n","        accelerator=\"gpu\",\n","        strategy=\"dp\",\n","        gpus=-1,\n","        # accelerator='ddp',\n","        # default_root_dir='/dbfs/FileStore/temp/kean_temp/logs'\n","    )\n","\n","    model.lr = FINE_LR\n","    for param in model.bert_model.parameters():\n","        param.requires_grad = True\n","    trainer.fit(model, data_module)"],"metadata":{"id":"DBpTSchw3whm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def log_model(model_name, model, params, metrics, artifacts=None, experiment_uri=''):\n","    '''Log Model in MLFlow'''\n","\n","    ### define experiment uri for mlflow\n","    if experiment_uri != '':\n","        if not mlflow.get_experiment_by_name(experiment_uri):\n","            mlflow.create_experiment(experiment_uri)\n","        mlflow.set_experiment(experiment_uri)\n","\n","    with mlflow.start_run(run_name=model_name) as run:\n","        experimentID = run.info.experiment_id\n","        print(\"Experiment ID\", experimentID)\n","        mlflow.pytorch.log_model(model, model_name)\n","        mlflow.pytorch.log_state_dict(model.state_dict(), model_name)\n","\n","        for k,v in params.items():\n","            mlflow.log_param(k, v)\n","        for k,v in metrics.items():\n","            mlflow.log_metric(k, v)\n","        if artifacts is not None:\n","            for artifact in artifacts:\n","                mlflow.log_artifact(artifact)\n","        mlflow.end_run()"],"metadata":{"id":"X56HOszl3xES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["experiment_uri = f\"/Users/{cfg['mlflow_email']}/{cfg['model_name']}\"\n","logger.info(f\"Logging to MlFlow in : {experiment_uri}\")\n","model_name = cfg['model_name']\n","\n","params = {\n","    'description': cfg['model_desc'],\n","    'epochs': EPOCHS,\n","    'max_sequence_length': INPUT_MAX_LEN,\n","    'batch_size': BATCH_SIZE,\n","    'max_step_per_epoch': LIMIT_STEP,\n","    'lr': STANDARD_LR,\n","}\n","\n","model_metrics = {\n","  \"train_loss\" : round(model.epoch_loss_train[-1].item(), 3),\n","  \"train_acc\" : round(model.epoch_acc_train[-1].item(), 3),\n","  \"eval_loss\" : round(model.epoch_loss_val[-1].item(), 3),\n","  \"eval_acc\" : round(model.epoch_acc_val[-1].item(), 3),\n","}\n","\n","log_model(model_name, model, params, model_metrics, experiment_uri=experiment_uri)"],"metadata":{"id":"RrcLlIQs33rW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.3 Model Validation"],"metadata":{"id":"ZoVAyTQc38_L"}},{"cell_type":"code","source":["### model validation\n","data_module = DataModule(\n","    df_train,\n","    df_val,\n","    max_len=INPUT_MAX_LEN,\n","    batch_size=BATCH_SIZE,\n","    tokenizer=MODEL,\n","    text_col=text_col\n",")\n","\n","df_test = data_module.test_df\n","test_set = data_module.val_dataloader()\n","batch = test_set.dataset.encodings\n","total_len = len(batch['input_ids'])\n","step = 512\n","\n","probas = []\n","model.eval()\n","with torch.no_grad():\n","    for i in tqdm(range(0, total_len, step)):\n","        outputs = model.forward(\n","            input_ids=batch['input_ids'][i : i + step],\n","            attention_mask=batch['attention_mask'][i : i + step]\n","        )\n","        probas.append(torch.sigmoid(outputs).cpu().detach().numpy())\n","\n","thres = 0.5\n","result = np.vstack(probas)\n","result = (result >= thres).astype(int)\n","print(\n","    classification_report(np.array(test_set.dataset.labels), result, target_names=aspect_classes)\n",")"],"metadata":{"id":"qbYXeShV34_C"},"execution_count":null,"outputs":[]}]}