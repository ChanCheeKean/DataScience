{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install sentencepiece\n","!wget https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar\n","!tar -xf mistral-7B-v0.1.tar"],"metadata":{"id":"VApPkpoH3ZR-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","import json\n","import math\n","from dataclasses import dataclass\n","import torch\n","from torch import nn\n","from sentencepiece import SentencePieceProcessor"],"metadata":{"id":"jrrJDv7d3X2T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1) Mistral LLM\n","[Mistral Github](https://github.com/mistralai/mistral-src/blob/main/one_file_ref.py)\n","\n","TODO: Explore original cache, prefill chunking (curently using one file ref)"],"metadata":{"id":"PzmZpcHIjnEQ"}},{"cell_type":"markdown","source":["## 1.1 Config"],"metadata":{"id":"p2izTu753nZ5"}},{"cell_type":"code","source":["@dataclass\n","class ModelArgs:\n","    dim: int\n","    n_layers: int\n","    head_dim: int\n","    hidden_dim: int\n","    n_heads: int\n","    n_kv_heads: int\n","    sliding_window: int\n","    norm_eps: float\n","    vocab_size: int\n","    max_batch_size: int = 0\n","\n","model_path = './mistral-7B-v0.1'\n","data_set = [\n","    \"Lucky is a dog from my neighbour.\",\n","    \"She likes to play ball with Lucy.\",\n","    \"Lucy is her best friend.\",\n","]\n","num_pipeline_ranks = 2"],"metadata":{"id":"VpB4Q0_G3nx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# if num_pipeline_ranks > 1:\n","#     torch.distributed.init_process_group()\n","#     torch.cuda.set_device(torch.distributed.get_rank())\n","#     should_print = torch.distributed.get_rank() == 0"],"metadata":{"id":"2NV1d7Yi8m1u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2 Tokenization"],"metadata":{"id":"yz9JhV5x3Pug"}},{"cell_type":"code","source":["class Tokenizer:\n","    def __init__(self, model_path: str):\n","        assert Path(model_path).exists(), model_path\n","        self._model = SentencePieceProcessor(model_file=model_path)\n","        assert self._model.vocab_size() == self._model.get_piece_size()\n","\n","    @property\n","    def n_words(self):\n","        return self._model.vocab_size()\n","\n","    @property\n","    def bos_id(self) -> int:\n","        return self._model.bos_id()\n","\n","    @property\n","    def eos_id(self):\n","        return self._model.eos_id()\n","\n","    @property\n","    def pad_id(self):\n","        return self._model.pad_id()\n","\n","    def encode(self, s: str, bos: bool=True):\n","        t = self._model.encode(s)\n","        if bos:\n","            t = [self.bos_id, *t]\n","        return t\n","\n","    def decode(self, t):\n","        return self._model.decode(t)"],"metadata":{"id":"vw2prn1Jjnbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# encode each sentence\n","tokenizer = Tokenizer(f'{model_path}/tokenizer.model')\n","encoded_prompts = [tokenizer.encode(prompt) for prompt in data_set]\n","print(encoded_prompts)\n","\n","# creating an empty tensor\n","prompt_lens = [len(x) for x in encoded_prompts]\n","min_prompt_len = min(prompt_lens)\n","max_prompt_len = max(prompt_lens)\n","print('Length: ', min_prompt_len, max_prompt_len)\n","input_tokens = torch.full(\n","    (len(data_set), max_prompt_len),\n","    tokenizer.pad_id,\n","    dtype=torch.long,\n","    device=\"cuda\"\n",")\n","print(f\"Empty Tensor: {input_tokens}\")\n","\n","# inserting tokens into the tensor\n","for i, encoded in enumerate(encoded_prompts):\n","    input_tokens[i, :len(encoded)] = torch.tensor(encoded).to(input_tokens)\n","print(f\"Tokenized Tensor: {input_tokens}\")\n","input_mask = input_tokens != tokenizer.pad_id\n","\n","# position for rotary embedding\n","positions = torch.arange(0, min_prompt_len).to(\"cuda\")\n","print(f\"Position: {positions}\")"],"metadata":{"id":"2HBFdbRdFeLC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703952338701,"user_tz":-480,"elapsed":427,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"944c0e53-fa0f-45c8-e9d2-073d7d46323d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1, 393, 11791, 349, 264, 3914, 477, 586, 18583, 28723], [1, 985, 12672, 298, 1156, 4374, 395, 18010, 28723], [1, 18010, 349, 559, 1489, 1832, 28723]]\n","Length:  7 10\n","Empty Tensor: tensor([[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n","        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n","        [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]], device='cuda:0')\n","Tokenized Tensor: tensor([[    1,   393, 11791,   349,   264,  3914,   477,   586, 18583, 28723],\n","        [    1,   985, 12672,   298,  1156,  4374,   395, 18010, 28723,    -1],\n","        [    1, 18010,   349,   559,  1489,  1832, 28723,    -1,    -1,    -1]],\n","       device='cuda:0')\n","Position: tensor([0, 1, 2, 3, 4, 5, 6], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["## 1.3 Model"],"metadata":{"id":"OKRXEuRE6FOn"}},{"cell_type":"code","source":["args = ModelArgs(\n","    dim=512,\n","    n_layers=1,\n","    head_dim=128,\n","    hidden_dim=2048,\n","    n_heads=4,\n","    n_kv_heads=2,\n","    sliding_window=3,\n","    norm_eps=1e-5,\n","    vocab_size=32_000,\n","    max_batch_size=3,\n",")\n","\n","# official parameter\n","# {\n","#     \"dim\": 4096,\n","#     \"n_layers\": 32,\n","#     \"head_dim\": 128,\n","#     \"hidden_dim\": 14336,\n","#     \"n_heads\": 32,\n","#     \"n_kv_heads\": 8,\n","#     \"norm_eps\": 1e-05,\n","#     \"sliding_window\": 4096,\n","#     \"vocab_size\": 32000\n","# }"],"metadata":{"id":"3R0JuOiKCgi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n","\n","    # 4 dimension\n","    ndim = x.ndim\n","\n","    # (m, seq_len, n_head, head_dim // 2) --> (1, seq_len, 1, head_dim // 2)\n","    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n","\n","    # (seq_len, head_dim // 2) --> (1, seq_len, 1, head_dim // 2)\n","    return freqs_cis.view(*shape)\n","\n","def apply_rotary_emb(xq, xk, freqs_cis):\n","\n","    # input: (m, seq_len, n_head, head_dim)\n","    # (m, seq_len, n_head, head_dim // 2, 2) --> (m, seq_len, n_head, head_dim // 2)\n","    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n","    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n","\n","    # (1, seq_len, 1, head_dim // 2)\n","    freqs_cis = _reshape_for_broadcast(freqs_cis, xq_)\n","\n","    # (m, seq_len, n_heads, head_dim // 2) --> (m, seq_len, n_heads, head_dim // 2, 2)\n","    xq_out = torch.view_as_real(xq_ * freqs_cis)\n","    xk_out = torch.view_as_real(xk_ * freqs_cis)\n","\n","    # (m, seq_len, n_heads, head_dim)\n","    xq_out = xq_out.reshape(*xq.shape)\n","    xk_out = xk_out.reshape(*xk.shape)\n","\n","    return xq_out.type_as(xq), xk_out.type_as(xk)\n","\n","def repeat_kv(keys: torch.Tensor, values: torch.Tensor, repeats: int):\n","    \"\"\"\n","    Part of Grouped Query Attention\n","    Repeat heads of key and values to match the dimension of query\n","    \"\"\"\n","\n","    # (m, seq_len, n_kv_heads, head_dim) --> (m, seq_len, n_kv_heads * repeat, head_dim)\n","    keys = torch.repeat_interleave(keys, repeats=repeats, dim=2)\n","    values = torch.repeat_interleave(values, repeats=repeats, dim=2)\n","    return keys, values"],"metadata":{"id":"IQODPO2h9X3j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Sliding window attention**\n","\n","![](https://miro.medium.com/v2/resize:fit:1400/0*uJ9qfE3Ik92XnEdz)\n","\n","Information Flow from Input to Upper Layer.\n","Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens at most: after two attention layers, information can move forward by 2W tokens, etc.\n","\n","For instance in a sequence of length 16K and a sliding window of 4K, after 4 layers, information has propagated to the full sequence length."],"metadata":{"id":"FDTlDlsPkNGI"}},{"cell_type":"markdown","source":["**Rolling buffer cache**\n","\n","![](https://github.com/mistralai/mistral-src/raw/main/assets/rolling_cache.png)\n","\n","The cache has a fixed size of W, and we store the (key, value) for position i in cache position i % W. When the position i is larger than W, past values in the cache are overwritten."],"metadata":{"id":"B6QQK-rBqKkb"}},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        super().__init__()\n","        self.args = args\n","\n","        self.n_heads = args.n_heads\n","        self.n_kv_heads = args.n_kv_heads\n","        self.repeats = self.n_heads // self.n_kv_heads\n","        self.sliding_window = self.args.sliding_window\n","        self.scale = self.args.head_dim**-0.5\n","\n","        self.wq = nn.Linear(args.dim, args.n_heads * args.head_dim, bias=False)\n","        self.wk = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)\n","        self.wv = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)\n","        self.wo = nn.Linear(args.n_heads * args.head_dim, args.dim, bias=False)\n","\n","        # only cache the sliding window instead of max seq_len\n","        self.cache_k = torch.empty(\n","            (\n","                args.max_batch_size,\n","                args.sliding_window,\n","                self.n_kv_heads,\n","                self.args.head_dim,\n","            ), dtype=torch.float32).cuda()\n","\n","        self.cache_v = torch.empty(\n","            (\n","                args.max_batch_size,\n","                args.sliding_window,\n","                self.n_kv_heads,\n","                self.args.head_dim,\n","            ), dtype=torch.float32).cuda()\n","\n","    def forward(self, x, freqs_cis, positions, mask):\n","        \"\"\"\n","        x: torch Tensor(m, seq_len, dim)\n","        freqs_cis: torch Tensor(seq_len, head_dim // 2)\n","        positions: torch Tensor(seq_len)\n","        mask: torch Tensor(seq_len, seq_len)\n","        \"\"\"\n","\n","        # (m, seq_len, dim)\n","        bsz, seqlen, _ = x.shape\n","        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n","\n","        ### grouped query attention ###\n","        # (m, seq_len, n_heads, head_dim)\n","        xq = xq.view(bsz, seqlen, self.n_heads, self.args.head_dim)\n","        # (m, seq_len, n_kv_heads, head_dim)\n","        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.args.head_dim)\n","        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.args.head_dim)\n","\n","        # (m, seq_len, n_heads, head_dim), \"n_kv_heads\" for xk\n","        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n","\n","        # The cache is a rotating buffer, the last element will become first after exceeding length\n","        # [0, 1, 2, 3, 4, 5, 6] --> [0, 1, 2, 0, 1, 2, 0] --> [1, 2, 0] --> (1, sliding_window, 1, 1)\n","        scatter_pos = (positions[-self.sliding_window:] % self.sliding_window)[None, :, None, None]\n","        # (m, sliding_window, n_kv_heads, head_dim)\n","        scatter_pos = scatter_pos.repeat(bsz, 1, self.n_kv_heads, self.args.head_dim)\n","\n","        # (m, sliding_window, n_kv_heads, head_dim)\n","        # it rotate according to the index, ['on', 'the', 'cat', 'is'] -> ['the', 'cat', 'is', 'on']\n","        self.cache_k[:bsz].scatter_(dim=1, index=scatter_pos, src=xk[:, -self.sliding_window:])\n","        self.cache_v[:bsz].scatter_(dim=1, index=scatter_pos, src=xv[:, -self.sliding_window:])\n","\n","        if positions.shape[0] > 1:\n","            key, value = repeat_kv(xk, xv, self.repeats)\n","        # use cache if seq_len is 1\n","        else:\n","            cur_pos = positions[-1].item() + 1\n","            key, value = repeat_kv(\n","                self.cache_k[:bsz, :cur_pos, ...], self.cache_v[:bsz, :cur_pos, ...], self.repeats\n","            )\n","\n","        ### original ###\n","        # xformers requires (B=1, S, H, D)\n","        # xq, key, val = xq[None, ...], key[None, ...], val[None, ...]\n","        # output = memory_efficient_attention(\n","        #     xq, key, val, None if cache is None else cache.mask\n","        # )\n","        # return self.wo(output.view(seqlen_sum, self.n_heads * self.head_dim))\n","\n","        # (m, n_heads, seq_len, head_dim)\n","        query = xq.transpose(1, 2)\n","        key = key.transpose(1, 2)\n","        value = value.transpose(1, 2)\n","\n","        # (m, n_heads, seqlen | 1, seqlen)\n","        scores = torch.matmul(query, key.transpose(2, 3)) * self.scale\n","\n","        if mask is not None:\n","            # mask (1, 1, seq_len, seq_len)\n","            scores += mask[None, None, ...]\n","        scores = scores.float()\n","        scores = nn.functional.softmax(scores, dim=-1).type_as(query)\n","\n","        # (m, n_heads, seqlen, head_dim)\n","        output = torch.matmul(scores, value)\n","        # (m, seqlen, dim)\n","        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n","        return self.wo(output)"],"metadata":{"id":"k2LgRBYe9LX8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### kv cache in the original model ###\n","'''\n","from xformers.ops.fmha.attn_bias import (\n","    AttentionBias,\n","    BlockDiagonalCausalMask,\n","    BlockDiagonalCausalWithOffsetPaddedKeysMask,\n","    BlockDiagonalMask,\n",")\n","\n","\n","@dataclass\n","class RotatingCacheInputMetadata:\n","    # rope absolute positions\n","    positions: torch.Tensor\n","    # which elements in the sequences need to be cached\n","    to_cache_mask: torch.Tensor\n","    # how many elements are cached per sequence\n","    cached_elements: torch.Tensor\n","    # where tokens should go in the cache\n","    cache_positions: torch.Tensor\n","\n","    # if prefill, use block diagonal causal mask\n","    # else use causal with padded key mask\n","    prefill: bool\n","    mask: AttentionBias\n","    seqlens: list[int]\n","\n","\n","def interleave_list(l1, l2):\n","    assert len(l1) == len(l2)\n","    return [v for pair in zip(l1, l2) for v in pair]\n","\n","def unrotate(cache: torch.Tensor, seqlen: int):\n","    assert cache.ndim == 3  # (W, H, D)\n","    position = seqlen % cache.shape[0]\n","    if seqlen < cache.shape[0]:\n","        return cache[:seqlen]\n","    elif position == 0:\n","        return cache\n","    else:\n","        return torch.cat([cache[position:], cache[:position]], dim=0)\n","\n","class CacheView:\n","    def __init__(self, cache_k, cache_v, metadata, kv_seqlens):\n","        self.cache_k = cache_k\n","        self.cache_v = cache_v\n","        self.kv_seqlens = kv_seqlens\n","        self.metadata = metadata\n","\n","    def update(self, xk, xv):\n","        \"\"\"\n","        to_cache_mask masks the last [sliding_window] tokens in each sequence\n","        \"\"\"\n","        n_kv_heads, head_dim = self.cache_k.shape[-2:]\n","        flat_cache_k = self.cache_k.view(-1, n_kv_heads, head_dim)\n","        flat_cache_v = self.cache_v.view(-1, n_kv_heads, head_dim)\n","\n","        flat_cache_k.index_copy_(0, self.metadata.cache_positions, xk[self.metadata.to_cache_mask])\n","        flat_cache_v.index_copy_(0, self.metadata.cache_positions, xv[self.metadata.to_cache_mask])\n","\n","    def interleave_kv(self, xk, xv):\n","        \"\"\"\n","        This is a naive implementation and not optimized for speed.\n","        \"\"\"\n","        assert xk.ndim == xv.ndim == 3 # (B * T, H, D)\n","        assert xk.shape == xv.shape\n","\n","        if all([s == 0 for s in self.metadata.seqlens]):\n","            # No cache to interleave\n","            return xk, xv\n","\n","        # Make it a list of [(T, H, D)]\n","        xk = torch.split(xk, self.metadata.seqlens)\n","        xv = torch.split(xv, self.metadata.seqlens)\n","\n","        # Order elements in cache by position by unrotating\n","        cache_k = [unrotate(t, s) for t, s in zip(self.cache_k, self.kv_seqlens)]\n","        cache_v = [unrotate(t, s) for t, s in zip(self.cache_v, self.kv_seqlens)]\n","\n","        interleaved_k = interleave_list(cache_k, xk)\n","        interleaved_v = interleave_list(cache_v, xv)\n","\n","        return torch.cat(interleaved_k, dim=0), torch.cat(interleaved_v, dim=0)\n","\n","    @property\n","    def sliding_window(self):\n","        return self.cache_k.shape[1]\n","\n","    @property\n","    def key(self):\n","        return self.cache_k[:len(self.kv_seqlens)]\n","\n","    @property\n","    def value(self):\n","        return self.cache_v[:len(self.kv_seqlens)]\n","\n","    @property\n","    def prefill(self):\n","        return self.metadata.prefill\n","\n","    @property\n","    def mask(self):\n","        return self.metadata.mask\n","\n","\n","class RotatingBufferCache:\n","    \"\"\"\n","    This is an example that implements a less naive rotating buffer cache, allowing for variable length sequences.\n","    Allocated cache is rectangular which is wasteful (see PagedAttention for better mechanisms)\n","    \"\"\"\n","    def __init__(self, n_layers, max_batch_size, sliding_window, n_kv_heads, head_dim):\n","\n","        self.sliding_window = sliding_window\n","        self.n_kv_heads = n_kv_heads\n","        self.head_dim = head_dim\n","\n","        self.cache_k = torch.empty((\n","            n_layers,\n","            max_batch_size,\n","            sliding_window,\n","            n_kv_heads,\n","            head_dim\n","        ))\n","        self.cache_v = torch.empty((\n","            n_layers,\n","            max_batch_size,\n","            sliding_window,\n","            n_kv_heads,\n","            head_dim\n","        ))\n","        # holds the valid length for each batch element in the cache\n","        self.kv_seqlens = None\n","\n","    def get_view(self, layer_id: int, metadata: RotatingCacheInputMetadata) -> CacheView:\n","        return CacheView(self.cache_k[layer_id], self.cache_v[layer_id], metadata, self.kv_seqlens)\n","\n","    def reset(self):\n","        self.kv_seqlens = None\n","\n","    def init_kvseqlens(self, batch_size: int):\n","        self.kv_seqlens = torch.zeros((batch_size,), device=self.device, dtype=torch.long)\n","\n","    @property\n","    def device(self):\n","        return self.cache_k.device\n","\n","    def to(self, device: torch.device, dtype: torch.dtype):\n","        self.cache_k = self.cache_k.to(device=device, dtype=dtype)\n","        self.cache_v = self.cache_v.to(device=device, dtype=dtype)\n","\n","        return self\n","\n","    def update_seqlens(self, seqlens: List[int]):\n","        self.kv_seqlens += torch.tensor(seqlens, device=self.device, dtype=torch.long)\n","\n","    def get_input_metadata(self, seqlens: List[int]):\n","\n","        if self.kv_seqlens is None:\n","            self.init_kvseqlens(len(seqlens))\n","        seqpos = self.kv_seqlens.tolist()\n","        masks = [\n","            [x >= seqlen - self.sliding_window for x in range(seqlen)]\n","                for seqlen in seqlens\n","        ]\n","        to_cache_mask = torch.tensor(sum(masks, []), device=self.device, dtype=torch.bool)\n","        cached_elements = torch.tensor([sum(mask) for mask in masks], device=self.device, dtype=torch.long)\n","        positions = [torch.arange(pos, pos + seqlen) for pos, seqlen in zip(seqpos, seqlens)]\n","        positions = torch.cat(positions).to(device=self.device, dtype=torch.long)\n","        batch_idx = torch.tensor(\n","            sum([[i]*seqlen for i, seqlen in enumerate(seqlens)], []),\n","            device=self.device,\n","            dtype=torch.long,\n","        )\n","        cache_positions = positions % self.sliding_window + batch_idx * self.sliding_window\n","\n","        first_prefill = seqpos[0] == 0\n","        subsequent_prefill = any(seqlen > 1 for seqlen in seqlens)\n","        if first_prefill:\n","            mask = BlockDiagonalCausalMask.from_seqlens(seqlens).make_local_attention(self.sliding_window)\n","        elif subsequent_prefill:\n","            mask = BlockDiagonalMask.from_seqlens(\n","                q_seqlen=seqlens,\n","                kv_seqlen=[\n","                    s + cached_s.clamp(max=self.sliding_window).item()\n","                        for (s, cached_s) in zip(seqlens, self.kv_seqlens)\n","                ]\n","            ).make_local_attention_from_bottomright(self.sliding_window)\n","        else:\n","            mask = BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens(\n","                q_seqlen=seqlens,\n","                kv_padding=self.sliding_window,\n","                kv_seqlen=(self.kv_seqlens + cached_elements).clamp(max=self.sliding_window).tolist()\n","            )\n","\n","        return RotatingCacheInputMetadata(\n","            positions=positions,\n","            to_cache_mask=to_cache_mask,\n","            cached_elements=cached_elements,\n","            cache_positions=cache_positions[to_cache_mask],\n","            prefill=first_prefill or subsequent_prefill,\n","            mask=mask,\n","            seqlens=seqlens,\n","        )\n","\n","class Attention(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        super().__init__()\n","        self.args = args\n","\n","        self.n_heads: int = args.n_heads\n","        self.head_dim: int = args.head_dim\n","        self.n_kv_heads: int = args.n_kv_heads\n","        self.repeats = self.n_heads // self.n_kv_heads\n","        self.scale = self.args.head_dim**-0.5\n","\n","        self.wq = nn.Linear(args.dim, args.n_heads * args.head_dim, bias=False)\n","        self.wk = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)\n","        self.wv = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)\n","        self.wo = nn.Linear(args.n_heads * args.head_dim, args.dim, bias=False)\n","\n","    def forward(self, x, freqs_cis, cache):\n","\n","        seqlen_sum, _ = x.shape\n","        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n","        xq = xq.view(seqlen_sum, self.n_heads, self.head_dim)\n","        xk = xk.view(seqlen_sum, self.n_kv_heads, self.head_dim)\n","        xv = xv.view(seqlen_sum, self.n_kv_heads, self.head_dim)\n","        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n","\n","        if cache is None:\n","            key, val = xk, xv\n","        elif cache.prefill:\n","            key, val = cache.interleave_kv(xk, xv)\n","            cache.update(xk, xv)\n","        else:\n","            cache.update(xk, xv)\n","            key, val = cache.key, cache.value\n","            key = key.view(seqlen_sum * cache.sliding_window, self.n_kv_heads, self.head_dim)\n","            val = val.view(seqlen_sum * cache.sliding_window, self.n_kv_heads, self.head_dim)\n","\n","        # Repeat keys and values to match number of query heads\n","        key, val = repeat_kv(key, val, self.repeats, dim=1)\n","\n","        # xformers requires (B=1, S, H, D)\n","        xq, key, val = xq[None, ...], key[None, ...], val[None, ...]\n","        output = memory_efficient_attention(xq, key, val, None if cache is None else cache.mask)\n","        return self.wo(output.view(seqlen_sum, self.n_heads * self.head_dim))\n","\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"ixf0OBHF6m4Y","executionInfo":{"status":"ok","timestamp":1703952338703,"user_tz":-480,"elapsed":27,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"4e072749-6097-43d5-81a4-51a2e870ec5b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom xformers.ops.fmha.attn_bias import (\\n    AttentionBias,\\n    BlockDiagonalCausalMask,\\n    BlockDiagonalCausalWithOffsetPaddedKeysMask,\\n    BlockDiagonalMask,\\n)\\n\\n\\n@dataclass\\nclass RotatingCacheInputMetadata:\\n    # rope absolute positions\\n    positions: torch.Tensor\\n    # which elements in the sequences need to be cached\\n    to_cache_mask: torch.Tensor\\n    # how many elements are cached per sequence\\n    cached_elements: torch.Tensor\\n    # where tokens should go in the cache\\n    cache_positions: torch.Tensor\\n\\n    # if prefill, use block diagonal causal mask\\n    # else use causal with padded key mask\\n    prefill: bool\\n    mask: AttentionBias\\n    seqlens: list[int]\\n\\n\\ndef interleave_list(l1, l2):\\n    assert len(l1) == len(l2)\\n    return [v for pair in zip(l1, l2) for v in pair]\\n\\ndef unrotate(cache: torch.Tensor, seqlen: int):\\n    assert cache.ndim == 3  # (W, H, D)\\n    position = seqlen % cache.shape[0]\\n    if seqlen < cache.shape[0]:\\n        return cache[:seqlen]\\n    elif position == 0:\\n        return cache\\n    else:\\n        return torch.cat([cache[position:], cache[:position]], dim=0)\\n\\nclass CacheView:\\n    def __init__(self, cache_k, cache_v, metadata, kv_seqlens):\\n        self.cache_k = cache_k\\n        self.cache_v = cache_v\\n        self.kv_seqlens = kv_seqlens\\n        self.metadata = metadata\\n\\n    def update(self, xk, xv):\\n        \"\"\"\\n        to_cache_mask masks the last [sliding_window] tokens in each sequence\\n        \"\"\"\\n        n_kv_heads, head_dim = self.cache_k.shape[-2:]\\n        flat_cache_k = self.cache_k.view(-1, n_kv_heads, head_dim)\\n        flat_cache_v = self.cache_v.view(-1, n_kv_heads, head_dim)\\n        \\n        flat_cache_k.index_copy_(0, self.metadata.cache_positions, xk[self.metadata.to_cache_mask])\\n        flat_cache_v.index_copy_(0, self.metadata.cache_positions, xv[self.metadata.to_cache_mask])\\n\\n    def interleave_kv(self, xk, xv):\\n        \"\"\"\\n        This is a naive implementation and not optimized for speed.\\n        \"\"\"\\n        assert xk.ndim == xv.ndim == 3 # (B * T, H, D)\\n        assert xk.shape == xv.shape\\n\\n        if all([s == 0 for s in self.metadata.seqlens]):\\n            # No cache to interleave\\n            return xk, xv\\n\\n        # Make it a list of [(T, H, D)]\\n        xk = torch.split(xk, self.metadata.seqlens)\\n        xv = torch.split(xv, self.metadata.seqlens)\\n\\n        # Order elements in cache by position by unrotating\\n        cache_k = [unrotate(t, s) for t, s in zip(self.cache_k, self.kv_seqlens)]\\n        cache_v = [unrotate(t, s) for t, s in zip(self.cache_v, self.kv_seqlens)]\\n\\n        interleaved_k = interleave_list(cache_k, xk)\\n        interleaved_v = interleave_list(cache_v, xv)\\n\\n        return torch.cat(interleaved_k, dim=0), torch.cat(interleaved_v, dim=0)\\n\\n    @property\\n    def sliding_window(self):\\n        return self.cache_k.shape[1]\\n\\n    @property\\n    def key(self):\\n        return self.cache_k[:len(self.kv_seqlens)]\\n\\n    @property\\n    def value(self):\\n        return self.cache_v[:len(self.kv_seqlens)]\\n\\n    @property\\n    def prefill(self):\\n        return self.metadata.prefill\\n\\n    @property\\n    def mask(self):\\n        return self.metadata.mask\\n\\n\\nclass RotatingBufferCache:\\n    \"\"\"\\n    This is an example that implements a less naive rotating buffer cache, allowing for variable length sequences.\\n    Allocated cache is rectangular which is wasteful (see PagedAttention for better mechanisms)\\n    \"\"\"\\n    def __init__(self, n_layers, max_batch_size, sliding_window, n_kv_heads, head_dim):\\n\\n        self.sliding_window = sliding_window\\n        self.n_kv_heads = n_kv_heads\\n        self.head_dim = head_dim\\n\\n        self.cache_k = torch.empty((\\n            n_layers,\\n            max_batch_size,\\n            sliding_window,\\n            n_kv_heads,\\n            head_dim\\n        ))\\n        self.cache_v = torch.empty((\\n            n_layers,\\n            max_batch_size,\\n            sliding_window,\\n            n_kv_heads,\\n            head_dim\\n        ))\\n        # holds the valid length for each batch element in the cache\\n        self.kv_seqlens = None\\n\\n    def get_view(self, layer_id: int, metadata: RotatingCacheInputMetadata) -> CacheView:\\n        return CacheView(self.cache_k[layer_id], self.cache_v[layer_id], metadata, self.kv_seqlens)\\n\\n    def reset(self):\\n        self.kv_seqlens = None\\n\\n    def init_kvseqlens(self, batch_size: int):\\n        self.kv_seqlens = torch.zeros((batch_size,), device=self.device, dtype=torch.long)\\n\\n    @property\\n    def device(self):\\n        return self.cache_k.device\\n\\n    def to(self, device: torch.device, dtype: torch.dtype):\\n        self.cache_k = self.cache_k.to(device=device, dtype=dtype)\\n        self.cache_v = self.cache_v.to(device=device, dtype=dtype)\\n\\n        return self\\n\\n    def update_seqlens(self, seqlens: List[int]):\\n        self.kv_seqlens += torch.tensor(seqlens, device=self.device, dtype=torch.long)\\n\\n    def get_input_metadata(self, seqlens: List[int]):\\n\\n        if self.kv_seqlens is None:\\n            self.init_kvseqlens(len(seqlens))\\n        seqpos = self.kv_seqlens.tolist()\\n        masks = [\\n            [x >= seqlen - self.sliding_window for x in range(seqlen)]\\n                for seqlen in seqlens\\n        ]\\n        to_cache_mask = torch.tensor(sum(masks, []), device=self.device, dtype=torch.bool)\\n        cached_elements = torch.tensor([sum(mask) for mask in masks], device=self.device, dtype=torch.long)\\n        positions = [torch.arange(pos, pos + seqlen) for pos, seqlen in zip(seqpos, seqlens)]\\n        positions = torch.cat(positions).to(device=self.device, dtype=torch.long)\\n        batch_idx = torch.tensor(\\n            sum([[i]*seqlen for i, seqlen in enumerate(seqlens)], []), \\n            device=self.device, \\n            dtype=torch.long,\\n        )\\n        cache_positions = positions % self.sliding_window + batch_idx * self.sliding_window\\n\\n        first_prefill = seqpos[0] == 0\\n        subsequent_prefill = any(seqlen > 1 for seqlen in seqlens)\\n        if first_prefill:\\n            mask = BlockDiagonalCausalMask.from_seqlens(seqlens).make_local_attention(self.sliding_window)\\n        elif subsequent_prefill:\\n            mask = BlockDiagonalMask.from_seqlens(\\n                q_seqlen=seqlens,\\n                kv_seqlen=[\\n                    s + cached_s.clamp(max=self.sliding_window).item() \\n                        for (s, cached_s) in zip(seqlens, self.kv_seqlens)\\n                ]\\n            ).make_local_attention_from_bottomright(self.sliding_window)\\n        else:\\n            mask = BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens(\\n                q_seqlen=seqlens,\\n                kv_padding=self.sliding_window,\\n                kv_seqlen=(self.kv_seqlens + cached_elements).clamp(max=self.sliding_window).tolist()\\n            )\\n\\n        return RotatingCacheInputMetadata(\\n            positions=positions,\\n            to_cache_mask=to_cache_mask,\\n            cached_elements=cached_elements,\\n            cache_positions=cache_positions[to_cache_mask],\\n            prefill=first_prefill or subsequent_prefill,\\n            mask=mask,\\n            seqlens=seqlens,\\n        )\\n\\nclass Attention(nn.Module):\\n    def __init__(self, args: ModelArgs):\\n        super().__init__()\\n        self.args = args\\n\\n        self.n_heads: int = args.n_heads\\n        self.head_dim: int = args.head_dim\\n        self.n_kv_heads: int = args.n_kv_heads\\n        self.repeats = self.n_heads // self.n_kv_heads\\n        self.scale = self.args.head_dim**-0.5\\n\\n        self.wq = nn.Linear(args.dim, args.n_heads * args.head_dim, bias=False)\\n        self.wk = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)\\n        self.wv = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)\\n        self.wo = nn.Linear(args.n_heads * args.head_dim, args.dim, bias=False)\\n\\n    def forward(self, x, freqs_cis, cache):\\n\\n        seqlen_sum, _ = x.shape\\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\\n        xq = xq.view(seqlen_sum, self.n_heads, self.head_dim)\\n        xk = xk.view(seqlen_sum, self.n_kv_heads, self.head_dim)\\n        xv = xv.view(seqlen_sum, self.n_kv_heads, self.head_dim)\\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\\n\\n        if cache is None:\\n            key, val = xk, xv\\n        elif cache.prefill:\\n            key, val = cache.interleave_kv(xk, xv)\\n            cache.update(xk, xv)\\n        else:\\n            cache.update(xk, xv)\\n            key, val = cache.key, cache.value\\n            key = key.view(seqlen_sum * cache.sliding_window, self.n_kv_heads, self.head_dim)\\n            val = val.view(seqlen_sum * cache.sliding_window, self.n_kv_heads, self.head_dim)\\n\\n        # Repeat keys and values to match number of query heads\\n        key, val = repeat_kv(key, val, self.repeats, dim=1)\\n\\n        # xformers requires (B=1, S, H, D)\\n        xq, key, val = xq[None, ...], key[None, ...], val[None, ...]\\n        output = memory_efficient_attention(xq, key, val, None if cache is None else cache.mask)\\n        return self.wo(output.view(seqlen_sum, self.n_heads * self.head_dim))\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        super().__init__()\n","        self.w1 = nn.Linear(args.dim, args.hidden_dim, bias=False)\n","        self.w2 = nn.Linear(args.hidden_dim, args.dim, bias=False)\n","        self.w3 = nn.Linear(args.dim, args.hidden_dim, bias=False)\n","\n","    def forward(self, x) -> torch.Tensor:\n","        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))"],"metadata":{"id":"Y3vw1UiV9M4y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class RMSNorm(torch.nn.Module):\n","    def __init__(self, dim, eps=1e-6):\n","        super().__init__()\n","        self.eps = eps\n","        self.weight = nn.Parameter(torch.ones(dim))\n","\n","    def _norm(self, x):\n","        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n","\n","    def forward(self, x):\n","        output = self._norm(x.float()).type_as(x)\n","        return output * self.weight\n","\n","class TransformerBlock(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        super().__init__()\n","        self.n_heads = args.n_heads\n","        self.dim = args.dim\n","        self.attention = Attention(args)\n","        self.feed_forward = FeedForward(args=args)\n","        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n","        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n","        self.args = args\n","\n","    def forward(self, x, freqs_cis, positions, mask):\n","\n","        # (m, seq_len, dim)\n","        r = self.attention.forward(self.attention_norm(x), freqs_cis, positions, mask)\n","\n","        # skip connection and feedforward\n","        h = x + r\n","        r = self.feed_forward.forward(self.ffn_norm(h))\n","        out = h + r\n","        return out"],"metadata":{"id":"sJkZIkI58wDZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def precompute_freqs_cis(head_dim, end=128_000, theta=10000.0):\n","    # (head_dim // 2)\n","    freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n","    # 12800\n","    t = torch.arange(end, device=freqs.device)\n","    # (12800, head_dim // 2)\n","    freqs = torch.outer(t, freqs).float()\n","    return torch.polar(torch.ones_like(freqs), freqs)\n","\n","class Transformer(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        super().__init__()\n","        self.args = args\n","        self.vocab_size = args.vocab_size\n","        self.n_layers = args.n_layers\n","        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n","        self.layers = torch.nn.ModuleList(\n","            [TransformerBlock(args=args) for _ in range(self.n_layers)]\n","        )\n","        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n","        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n","        self.freqs_cis = precompute_freqs_cis(self.args.head_dim, 128_000).to(\"cuda\")\n","\n","    def forward(self, input_ids, positions):\n","\n","        # [m, seq_len] --> [m, seq_len, dim]\n","        h = self.tok_embeddings(input_ids)\n","\n","        # position (0, 1, 2 ... seq_len)\n","        # (seq_len, head_dim // 2)\n","        freqs_cis = self.freqs_cis[positions]\n","\n","        mask = None\n","        if input_ids.shape[1] > 1:\n","            seqlen = input_ids.shape[1]\n","            # (seq_len, seq_len)\n","            tensor = torch.full(\n","                (seqlen, seqlen),\n","                dtype=h.dtype,\n","                fill_value=1,\n","                device=h.device,\n","            )\n","\n","            # triangular mask\n","            # make the mask banded to account for sliding window\n","            mask = torch.tril(tensor, diagonal=0).to(h.dtype)\n","            mask = torch.triu(mask, diagonal=-self.args.sliding_window)\n","            # 1 become 0, 0 become -inf\n","            mask = torch.log(mask)\n","\n","        # (m, seq_len, dim)\n","        for layer in self.layers:\n","            h = layer(h, freqs_cis, positions, mask)\n","        return self.output(self.norm(h)).float()\n","\n","    ### to load it straight from pretrained ###\n","    @staticmethod\n","    def from_folder(folder, max_batch_size=1, device=\"cuda\", dtype=torch.float16):\n","        with open(f'{folder}/params.json', 'r') as f:\n","            model_args = ModelArgs(**json.loads(f.read()))\n","        model_args.max_batch_size = max_batch_size\n","        model = Transformer(model_args).to(device=device, dtype=dtype)\n","        loaded = torch.load(f'{folder}/consolidated.00.pth')\n","        model.load_state_dict(loaded)\n","        return model"],"metadata":{"id":"_ONB2n5vCKMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Transformer(args).to(\"cuda\", dtype=torch.float32)\n","logits = model.forward(input_tokens[:, :min_prompt_len], positions)\n","logprobs = nn.functional.log_softmax(logits, dim=-1)\n","print(logits.size(), logprobs.size())"],"metadata":{"id":"JPLcbQ5MAvqz","executionInfo":{"status":"ok","timestamp":1703954205681,"user_tz":-480,"elapsed":1421,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"20e4edc5-95e2-46a5-b8c7-bad7ba8d796c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 7, 32000]) torch.Size([3, 7, 32000])\n"]}]},{"cell_type":"markdown","source":["## 1.4 Generation"],"metadata":{"id":"7HR6nyrFiKb5"}},{"cell_type":"code","source":["# model = model.from_folder(\"./mistral-7B-v0.1\")\n","# model.eval()"],"metadata":{"id":"-dMLRogHOj7I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = './mistral-7B-v0.1'\n","prompts = [\n","    \"Lucky is a dog from my neighbour.\",\n","    \"She likes to play ball with Lucy.\",\n","    \"Lucy is her best friend.\",\n","]\n","\n","# tokenize every word\n","tokenizer = Tokenizer(f'{model_path}/tokenizer.model')\n","encoded_prompts = [tokenizer.encode(prompt) for prompt in prompts]"],"metadata":{"id":"1dP1Cm5Jj1uX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["min_prompt_len = min(prompt_lens)\n","max_prompt_len = max(prompt_lens)\n","\n","# create temporary tensor filled with pad_id\n","input_tokens = torch.full(\n","    (len(prompts), max_prompt_len),\n","    tokenizer.pad_id,\n","    dtype=torch.long,\n","    device=\"cuda\"\n",")\n","\n","# replace the token\n","for i, encoded in enumerate(encoded_prompts):\n","    input_tokens[i, :len(encoded)] = torch.tensor(encoded).to(input_tokens)\n","input_mask = input_tokens != tokenizer.pad_id\n","print(input_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mbDKGR8eiUAZ","executionInfo":{"status":"ok","timestamp":1703952340651,"user_tz":-480,"elapsed":11,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"04c73f52-d1c3-4feb-8e41-af0cbe773f44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    1,   393, 11791,   349,   264,  3914,   477,   586, 18583, 28723,\n","            -1],\n","        [    1,   985, 12672,   298,  1156,  4374,   395, 18010, 28723,    -1,\n","            -1],\n","        [    1, 18010,   349,   559,  1489,  1832, 28723,    -1,    -1,    -1,\n","            -1]], device='cuda:0')\n"]}]},{"cell_type":"code","source":["positions = torch.arange(0, min_prompt_len).to(\"cuda\")\n","logits = model.forward(input_tokens[:, :min_prompt_len], positions)\n","logprobs = nn.functional.log_softmax(logits, dim=-1)\n","print(logprobs.size())\n","\n","# get probability for all tokens in the prompt\n","all_logprobs = [\n","    logprobs[:, :-1, :].gather(2, input_tokens[:, 1:min_prompt_len, None]).squeeze(-1),\n","]\n","print(all_logprobs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpCdak31kNFg","executionInfo":{"status":"ok","timestamp":1703952341092,"user_tz":-480,"elapsed":449,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"0f3c2711-8a12-424c-fef8-cfaf640b2a43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 7, 32000])\n","[tensor([[-11.2580, -10.4556, -10.4530, -11.7142,  -9.8308, -10.6482],\n","        [-11.0530, -10.4912, -10.6849, -10.0249, -10.4921, -11.3246],\n","        [-10.3404, -11.1509, -10.4690, -11.3796,  -9.9112, -11.8490]],\n","       device='cuda:0', grad_fn=<SqueezeBackward1>)]\n"]}]},{"cell_type":"code","source":["def sample_top_p(probs: torch.Tensor, p: float):\n","    assert 0 <= p <= 1\n","\n","    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n","    probs_sum = torch.cumsum(probs_sort, dim=-1)\n","    mask = probs_sum - probs_sort > p\n","    probs_sort[mask] = 0.0\n","    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n","\n","    # sampled from the multinomial probability distribution\n","    next_token = torch.multinomial(probs_sort, num_samples=1)\n","\n","    # torch gather is indexing\n","    return torch.gather(probs_idx, -1, next_token)\n","\n","def sample(logits: torch.Tensor, temperature: float, top_p: float):\n","\n","    # logits size: (m, vocab_size)\n","    if temperature > 0:\n","        # logits increase, probs more extreme, making the output stable\n","        probs = torch.softmax(logits / temperature, dim=-1)\n","        next_token = sample_top_p(probs, top_p)\n","    else:\n","        next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n","\n","    return next_token.reshape(-1)"],"metadata":{"id":"UnFYCEB8s6aE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get probability for each token from input_tokens\n","max_tokens = 10\n","temperature = 0.8\n","generated = []\n","\n","# start from the min_prompt_len (for batch >1)\n","# loop until getting the max tokens\n","\n","for cur_pos in range(min_prompt_len, max_tokens):\n","\n","    # token id for max prob (m,)\n","    # next_token = torch.argmax(logprobs[:, -1, :], dim=-1)\n","    next_token = sample(logprobs[:, -1, :], temperature=temperature, top_p=0.8)\n","\n","    if cur_pos < input_mask.shape[1]:\n","        # if not pad, return original token else predicted token\n","        next_token = torch.where(\n","            input_mask[:, cur_pos],\n","            input_tokens[:, cur_pos],\n","            next_token\n","        )\n","    all_logprobs.append(logprobs[:, -1, :].gather(1, next_token[:, None]))\n","\n","    # (max_tokens, m, 1)\n","    generated.append(next_token[:, None])\n","\n","    # feed the last token and position and get prediction\n","    logits = model.forward(next_token[:, None], torch.LongTensor([cur_pos]).to(next_token))\n","    logprobs = nn.functional.log_softmax(logits, dim=-1)\n","\n","all_logprobs = torch.cat(all_logprobs, 1)\n","print('All Prob Size: ', all_logprobs.size())\n","generated = torch.cat(generated, 1)\n","print('Generated Tokens: ', generated)\n","\n","res = []\n","for i, x in enumerate(encoded_prompts):\n","    res.append(tokenizer.decode(x[:min_prompt_len] + generated[i].tolist()))\n","\n","for x in res:\n","    print(x)\n","    print(\"=====================\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1Rz07JekTmA","executionInfo":{"status":"ok","timestamp":1703952341092,"user_tz":-480,"elapsed":9,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"c6f953e6-09ba-4703-e431-3959c0a4f571"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All Prob Size:  torch.Size([3, 16])\n","Generated Tokens:  tensor([[  586, 18583, 28723, 29919, 31342,  6654,  4228,   399, 31841, 15854],\n","        [18010, 28723,   948, 16018, 28367,   689, 15099, 20251,  3445,  6302],\n","        [30021, 30327, 17565, 17159,  1603,  3185, 29532, 24422,  4040, 10668]],\n","       device='cuda:0')\n","Lucky is a dog from my neighbour.费ইrior School R❒ gri\n","=====================\n","She likes to play ball with Lucy. end dés (+ Chistes fucked command college\n","=====================\n","Lucy is her best friend.技颜 ALL cultiv////CON效itungbig Wild\n","=====================\n"]}]},{"cell_type":"markdown","source":["# 2) Mixtral Mix of Expert (8x7b MoE)\n","\n","1. Ensemble technique with multiple expert (e.g. some experts specialized for different languages or tasks).  \n","2. The output of experts are combined (weighted sum or averaging)\n","3. Only 2 out of 8 experts are used for each token\n","4. Before going into experts network, the gate produces logits to select topK experts.\n","\n","[MoE One File Ref](https://github.com/mistralai/mistral-src/blob/main/moe_one_file_ref.py)\n"],"metadata":{"id":"j8OhzFJtskJM"}},{"cell_type":"code","source":["@dataclass\n","class MoeArgs():\n","    num_experts: int\n","    num_experts_per_tok: int\n","\n","@dataclass\n","class ModelArgs():\n","    dim: int\n","    n_layers: int\n","    head_dim: int\n","    hidden_dim: int\n","    n_heads: int\n","    n_kv_heads: int\n","    norm_eps: float\n","    vocab_size: int\n","    moe: MoeArgs\n","    max_batch_size: int = 0\n","    max_seq_len: int = 0\n","\n","moe_args = MoeArgs(num_experts=4, num_experts_per_tok=2)\n","\n","args = ModelArgs(\n","    dim=512,\n","    n_layers=1,\n","    head_dim=128,\n","    hidden_dim=2048,\n","    n_heads=4,\n","    n_kv_heads=2,\n","    norm_eps=1e-5,\n","    vocab_size=32_000,\n","    max_batch_size=3,\n","    max_seq_len=20,\n","    moe=moe_args\n",")"],"metadata":{"id":"7iSiQtl-q-0T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# no sliding window in MoE, the rest remains\n","class Attention(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        super().__init__()\n","        self.args = args\n","\n","        self.n_heads: int = args.n_heads\n","        self.n_kv_heads: int = args.n_kv_heads\n","\n","        self.repeats = self.n_heads // self.n_kv_heads\n","        self.scale = self.args.head_dim**-0.5\n","\n","        self.wq = nn.Linear(args.dim, args.n_heads * args.head_dim, bias=False)\n","        self.wk = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)\n","        self.wv = nn.Linear(args.dim, args.n_kv_heads * args.head_dim, bias=False)\n","        self.wo = nn.Linear(args.n_heads * args.head_dim, args.dim, bias=False)\n","        self._cache_k = None\n","        self._cache_v = None\n","\n","    def get_caches(self, x: torch.Tensor):\n","        dtype, device = x.dtype, x.device\n","        if self._cache_k is None:\n","            self._cache_k = torch.empty(\n","                (\n","                    self.args.max_batch_size,\n","                    self.args.max_seq_len,\n","                    self.n_kv_heads,\n","                    self.args.head_dim,\n","                ),\n","                dtype=dtype,\n","                device=device,\n","            )\n","        if self._cache_v is None:\n","            self._cache_v = torch.empty(\n","                (\n","                    self.args.max_batch_size,\n","                    self.args.max_seq_len,\n","                    self.n_kv_heads,\n","                    self.args.head_dim,\n","                ),\n","                dtype=dtype,\n","                device=device,\n","            )\n","        return self._cache_k, self._cache_v\n","\n","    def forward(self, x, freqs_cis, positions, mask) :\n","        bsz, seqlen, _ = x.shape\n","        cache_k, cache_v = self.get_caches(x)\n","\n","        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n","        xq = xq.view(bsz, seqlen, self.n_heads, self.args.head_dim)\n","        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.args.head_dim)\n","        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.args.head_dim)\n","        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n","\n","        # The cache is a rotating buffer\n","        scatter_pos = (positions % self.args.max_seq_len)[None, :, None, None]\n","        scatter_pos = scatter_pos.repeat(bsz, 1, self.n_kv_heads, self.args.head_dim)\n","        cache_k[:bsz].scatter_(dim=1, index=scatter_pos, src=xk)\n","        cache_v[:bsz].scatter_(dim=1, index=scatter_pos, src=xv)\n","\n","        if positions.shape[0] > 1:\n","            key, value = repeat_kv(xk, xv, self.repeats)\n","        else:\n","            assert mask is None\n","            cur_pos = int(positions[-1].item() + 1)\n","            key, value = repeat_kv(\n","                cache_k[:bsz, :cur_pos, ...],\n","                cache_v[:bsz, :cur_pos, ...],\n","                self.repeats,\n","            )\n","\n","        query = xq.transpose(1, 2)\n","        key = key.transpose(1, 2)\n","        value = value.transpose(1, 2)\n","        scores = torch.matmul(query, key.transpose(2, 3)) * self.scale\n","\n","        if mask is not None:\n","            scores += mask[None, None, ...]\n","\n","        scores = scores.float()\n","        scores = nn.functional.softmax(scores, dim=-1).type_as(query)\n","        output = torch.matmul(scores, value)\n","        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n","        return self.wo(output)"],"metadata":{"id":"KAV7LxA4Av_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# MoE are just list of feedforward layers\n","class MoeLayer(nn.Module):\n","    def __init__(self, experts, gate, moe_args):\n","        super().__init__()\n","        assert len(experts) > 0\n","\n","        # list of feedforward layers\n","        self.experts = nn.ModuleList(experts)\n","        # list of linear layers\n","        self.gate = gate\n","        self.args = moe_args\n","\n","    def forward(self, inputs: torch.Tensor):\n","\n","        # (m, seq_len, dim) --> (m * seq_len, dim)\n","        inputs_squashed = inputs.view(-1, inputs.shape[-1])\n","\n","        # (m * seq_len, num_experts)\n","        gate_logits = self.gate(inputs_squashed)\n","\n","        # (m * seq_len, num_experts_per_tok),\n","        weights, selected_experts = torch.topk(\n","            gate_logits, self.args.num_experts_per_tok)\n","        weights = nn.functional.softmax(\n","            weights, dim=1, dtype=torch.float).type_as(inputs)\n","\n","        # (m * seq_len, dim)\n","        results = torch.zeros_like(inputs_squashed)\n","        for i, expert in enumerate(self.experts):\n","            # index of batch and expert\n","            batch_idx, nth_expert = torch.where(selected_experts == i)\n","\n","            # weightage * output of expert layers (selected_m, num_expert)\n","            results[batch_idx] += ( weights[batch_idx, nth_expert, None] *\n","                expert(inputs_squashed[batch_idx]) )\n","\n","        # (m * seq_len, dim) --> (m, seq_len, dim)\n","        return results.view_as(inputs)"],"metadata":{"id":"IMAxYp7NAwJV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, args: ModelArgs):\n","        super().__init__()\n","        self.n_heads = args.n_heads\n","        self.dim = args.dim\n","        self.attention = Attention(args)\n","        self.feed_forward = MoeLayer(\n","            experts=[FeedForward(args=args) for _ in range(args.moe.num_experts)],\n","            gate=nn.Linear(args.dim, args.moe.num_experts, bias=False),\n","            moe_args=args.moe,\n","        )\n","        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n","        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n","        self.args = args\n","\n","    def forward(self, x, freqs_cis, positions, mask):\n","        # (m, seq_len, dim)\n","        r = self.attention.forward(self.attention_norm(x), freqs_cis, positions, mask)\n","        h = x + r\n","        r = self.feed_forward.forward(self.ffn_norm(h))\n","        out = h + r\n","        return out"],"metadata":{"id":"g5Q1P0EIAwQl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self, args, pipeline_rank=0, num_pipeline_ranks=1):\n","        super().__init__()\n","        self.args = args\n","        self.vocab_size = args.vocab_size\n","        self.pipeline_rank = pipeline_rank\n","        self.num_pipeline_ranks = num_pipeline_ranks\n","        self._precomputed_freqs_cis = None\n","\n","        # Modules specific to some ranks:\n","        self.tok_embeddings = None\n","        self.norm = None\n","        self.output = None\n","        if pipeline_rank == 0:\n","            self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n","        if pipeline_rank == num_pipeline_ranks - 1:\n","            self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n","            self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n","\n","        # Initialize all layers but slice off those not of this rank.\n","        layers = [TransformerBlock(args=args) for _ in range(args.n_layers)]\n","        num_layers_per_rank = math.ceil(args.n_layers / self.num_pipeline_ranks)\n","        offset = self.pipeline_rank * num_layers_per_rank\n","        end = min(args.n_layers, offset + num_layers_per_rank)\n","        self.layers = nn.ModuleDict({str(i): layers[i] for i in range(offset, end)})\n","        self.n_local_layers = len(self.layers)\n","\n","    @property\n","    def dtype(self) -> torch.dtype:\n","        return next(self.parameters()).dtype\n","\n","    @property\n","    def device(self) -> torch.device:\n","        return next(self.parameters()).device\n","\n","    @property\n","    def freqs_cis(self) -> torch.Tensor:\n","        # cache freqs_cis but need to take care that it is on the right device\n","        if self._precomputed_freqs_cis is None:\n","            self._precomputed_freqs_cis = precompute_freqs_cis(\n","                head_dim=self.args.head_dim, end=128_000, theta=1000000.0)\n","        if self._precomputed_freqs_cis.device != self.device:\n","            self._precomputed_freqs_cis = self._precomputed_freqs_cis.to(device=self.device)\n","        return self._precomputed_freqs_cis\n","\n","    def forward(self, input_ids, positions):\n","\n","        # position (0, 1, 2 ... seq_len)\n","        # (seq_len, head_dim // 2)\n","        freqs_cis = self.freqs_cis[positions]\n","\n","        (bsz, seqlen) = input_ids.shape\n","        num_toks = bsz * seqlen\n","\n","        if self.pipeline_rank == 0:\n","            h = self.tok_embeddings(input_ids)\n","        else:\n","            h = torch.empty(bsz, seqlen, self.args.dim, device=self.device, dtype=self.dtype)\n","            torch.distributed.recv(h, src=self.pipeline_rank - 1)\n","\n","        mask = None\n","        if input_ids.shape[1] > 1:\n","            tensor = torch.full(\n","                (seqlen, seqlen),\n","                dtype=h.dtype,\n","                fill_value=1,\n","                device=h.device,\n","            )\n","            mask = torch.log(torch.tril(tensor, diagonal=0)).to(h.dtype)\n","\n","        # (m, seq_len, dim)\n","        for layer in self.layers.values():\n","            h = layer(h, freqs_cis, positions, mask)\n","\n","        if self.pipeline_rank < self.num_pipeline_ranks - 1:\n","            torch.distributed.send(h, dst=self.pipeline_rank + 1)\n","            outs = torch.empty(*h.shape[:-1], self.vocab_size, device=h.device, dtype=h.dtype)\n","        else:\n","            outs = self.output(self.norm(h))\n","        if self.num_pipeline_ranks > 1:\n","            torch.distributed.broadcast(outs, src=self.num_pipeline_ranks - 1)\n","        return outs.float()\n","\n","        def load_state_dict(self, state_dict, *args, **kwargs):\n","            state_to_load = {}\n","            skipped = set([])\n","            for k, v in state_dict.items():\n","                if k.startswith(\"tok_embeddings\"):\n","                    if self.pipeline_rank == 0:\n","                        state_to_load[k] = v\n","                    else:\n","                        logging.debug(\n","                            \"Skipping parameter %s at pipeline rank %d\",\n","                            k,\n","                            self.pipeline_rank,\n","                        )\n","                        skipped.add(k)\n","                elif k.startswith(\"norm\") or k.startswith(\"output\"):\n","                    if self.pipeline_rank == self.num_pipeline_ranks - 1:\n","                        state_to_load[k] = v\n","                    else:\n","                        logging.debug(\n","                            \"Skipping parameter %s at pipeline rank %d\",\n","                            k,\n","                            self.pipeline_rank,\n","                        )\n","                        skipped.add(k)\n","                elif k.startswith(\"layers\"):\n","                    layer_id = k.split(\".\")[1]\n","                    if layer_id in self.layers:\n","                        state_to_load[k] = v\n","                    else:\n","                        logging.debug(\n","                            \"Skipping parameter %s at pipeline rank %d\",\n","                            k,\n","                            self.pipeline_rank,\n","                        )\n","                        skipped.add(k)\n","                else:\n","                    raise ValueError(f\"Unexpected key {k}\")\n","            assert set(state_dict.keys()) == skipped.union(set(state_to_load.keys()))\n","            super().load_state_dict(state_to_load, *args, **kwargs)\n","\n","    @staticmethod\n","    def from_folder(\n","            folder, max_batch_size, max_seq_len, num_pipeline_ranks=1,\n","            device=\"cuda\", dtype=torch.float16\n","        ):\n","        with open(folder / \"params.json\", \"r\") as f:\n","            model_args = ModelArgs.from_dict(json.load(f))\n","        model_args.max_batch_size = max_batch_size\n","        model_args.max_seq_len = max_seq_len\n","        if num_pipeline_ranks > 1:\n","            pipeline_rank = torch.distributed.get_rank()\n","        else:\n","            pipeline_rank = 0\n","        with torch.device(\"meta\"):\n","            model = Transformer(\n","                model_args,\n","                pipeline_rank=pipeline_rank,\n","                num_pipeline_ranks=num_pipeline_ranks\n","            )\n","        loaded = torch.load(str(folder / \"consolidated.00.pth\"), mmap=True)\n","        model.load_state_dict(loaded, assign=True)\n","        return model.to(device=device, dtype=dtype)"],"metadata":{"id":"826ING2Sfion"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Transformer(args).to(\"cuda\", dtype=torch.float32)\n","logits = model.forward(input_tokens[:, :min_prompt_len], positions)\n","logprobs = nn.functional.log_softmax(logits, dim=-1)\n","# print(logits.size(), logprobs.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2SCT9KShle5F","executionInfo":{"status":"ok","timestamp":1703957021975,"user_tz":-480,"elapsed":418,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"f9fa61df-f925-4a15-e6f2-d9754cf9a9df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([12]) torch.Size([12])\n","torch.Size([12]) torch.Size([12])\n","torch.Size([11]) torch.Size([11])\n","torch.Size([7]) torch.Size([7])\n"]}]},{"cell_type":"markdown","source":["# 3) Phi-2"],"metadata":{"id":"CXfiqYvOsnt8"}},{"cell_type":"code","source":["import math\n","from typing import Optional\n","\n","from transformers import PretrainedConfig\n","\n","class PhiConfig(PretrainedConfig):\n","    \"\"\"Phi configuration.\"\"\"\n","\n","    model_type = \"phi-msft\"\n","    attribute_map = {\n","        \"max_position_embeddings\": \"n_positions\",\n","        \"hidden_size\": \"n_embd\",\n","        \"num_attention_heads\": \"n_head\",\n","        \"num_hidden_layers\": \"n_layer\",\n","    }\n","\n","    def __init__(\n","        self,\n","        vocab_size: int = 50304,\n","        n_positions: int = 2048,\n","        n_embd: int = 1024,\n","        n_layer: int = 20,\n","        n_inner: Optional[int] = None,\n","        n_head: int = 16,\n","        n_head_kv: Optional[int] = None,\n","        rotary_dim: Optional[int] = 32,\n","        activation_function: Optional[str] = \"gelu_new\",\n","        flash_attn: bool = False,\n","        flash_rotary: bool = False,\n","        fused_dense: bool = False,\n","        attn_pdrop: float = 0.0,\n","        embd_pdrop: float = 0.0,\n","        resid_pdrop: float = 0.0,\n","        layer_norm_epsilon: float = 1e-5,\n","        initializer_range: float = 0.02,\n","        tie_word_embeddings: bool = False,\n","        pad_vocab_size_multiple: int = 64,\n","        **kwargs):\n","\n","        self.vocab_size = int(\n","            math.ceil(vocab_size / pad_vocab_size_multiple) *\n","            pad_vocab_size_multiple\n","        )\n","        self.n_positions = n_positions\n","        self.n_embd = n_embd\n","        self.n_layer = n_layer\n","        self.n_inner = n_inner\n","        self.n_head = n_head\n","        self.n_head_kv = n_head_kv\n","        self.rotary_dim = min(rotary_dim, n_embd // n_head)\n","        self.activation_function = activation_function\n","        self.flash_attn = flash_attn\n","        self.flash_rotary = flash_rotary\n","        self.fused_dense = fused_dense\n","        self.attn_pdrop = attn_pdrop\n","        self.embd_pdrop = embd_pdrop\n","        self.resid_pdrop = resid_pdrop\n","        self.layer_norm_epsilon = layer_norm_epsilon\n","        self.initializer_range = initializer_range\n","\n","        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)"],"metadata":{"id":"KvOxswc2ipBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["arg = {\n","    \"_name_or_path\": \"microsoft/phi-2\",\n","    \"activation_function\": \"gelu_new\",\n","    \"architectures\": [\"PhiForCausalLM\"],\n","    \"attn_pdrop\": 0.0,\n","    \"auto_map\": {\n","        \"AutoConfig\": \"configuration_phi.PhiConfig\",\n","        \"AutoModelForCausalLM\": \"modeling_phi.PhiForCausalLM\"\n","    },\n","    \"embd_pdrop\": 0.0,\n","    \"flash_attn\": false,\n","    \"flash_rotary\": false,\n","    \"fused_dense\": false,\n","    \"img_processor\": null,\n","    \"initializer_range\": 0.02,\n","    \"layer_norm_epsilon\": 1e-05,\n","    \"model_type\": \"phi-msft\",\n","    \"n_embd\": 2560,\n","    \"n_head\": 32,\n","    \"n_head_kv\": null,\n","    \"n_inner\": null,\n","    \"n_layer\": 32,\n","    \"n_positions\": 2048,\n","    \"resid_pdrop\": 0.1,\n","    \"rotary_dim\": 32,\n","    \"tie_word_embeddings\": false,\n","    \"torch_dtype\": \"float16\",\n","    \"transformers_version\": \"4.35.2\",\n","    \"vocab_size\": 51200\n","}"],"metadata":{"id":"Won-cm475onq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dmgsJdBU64in"},"execution_count":null,"outputs":[]}]}