{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"nlp-sequence-models","graded_item_id":"n16CQ","launcher_item_id":"npjGi"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"YqDN1ftTmHkX","executionInfo":{"status":"ok","timestamp":1665908081332,"user_tz":-480,"elapsed":9544,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"05997025-79cf-4dd8-fd77-eeba90b5ec2e"},"source":["!wget https://raw.githubusercontent.com/PacktPublishing/The-Deep-Learning-Challenge/master/Section%205/source/spa-eng/spa.txt -P datasets\n","!pip install faker"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-16 08:14:30--  https://raw.githubusercontent.com/PacktPublishing/The-Deep-Learning-Challenge/master/Section%205/source/spa-eng/spa.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 8152302 (7.8M) [text/plain]\n","Saving to: ‘datasets/spa.txt’\n","\n","spa.txt             100%[===================>]   7.77M  --.-KB/s    in 0.1s    \n","\n","2022-10-16 08:14:31 (74.9 MB/s) - ‘datasets/spa.txt’ saved [8152302/8152302]\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting faker\n","  Downloading Faker-15.1.1-py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 4.4 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.7/dist-packages (from faker) (2.8.2)\n","Requirement already satisfied: typing-extensions>=3.10.0.1 in /usr/local/lib/python3.7/dist-packages (from faker) (4.1.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.4->faker) (1.15.0)\n","Installing collected packages: faker\n","Successfully installed faker-15.1.1\n"]}]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T09:12:31.826146Z","start_time":"2021-04-10T09:12:31.819440Z"},"id":"z6d4WuhdSED_"},"source":["from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model, Model\n","import tensorflow.keras.backend as K\n","import numpy as np\n","import tensorflow as tf\n","from faker import Faker\n","import random\n","from tqdm import tqdm\n","from babel.dates import format_date\n","import matplotlib.pyplot as plt\n","from lib.nmt_utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SoC1kZ6oSEEH"},"source":["# 1) Luong Attention"]},{"cell_type":"markdown","metadata":{"id":"amgPSJ_cSEEI"},"source":["## 1.1 Data Loading"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUhqW7ZISEEI","executionInfo":{"status":"ok","timestamp":1657297287708,"user_tz":-480,"elapsed":1060,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"3380f9be-c723-4b71-b8b7-4d0b6c39b501"},"source":["m = 10000\n","dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n","dataset[:10]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:00<00:00, 18467.01it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["[('9 may 1998', '1998-05-09'),\n"," ('10.11.19', '2019-11-10'),\n"," ('9/10/70', '1970-09-10'),\n"," ('saturday april 28 1990', '1990-04-28'),\n"," ('thursday january 26 1995', '1995-01-26'),\n"," ('monday march 7 1983', '1983-03-07'),\n"," ('sunday may 22 1988', '1988-05-22'),\n"," ('08 jul 2008', '2008-07-08'),\n"," ('8 sep 1999', '1999-09-08'),\n"," ('thursday january 1 1981', '1981-01-01')]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["machine_vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0DuJjnPndl5","executionInfo":{"status":"ok","timestamp":1657297299397,"user_tz":-480,"elapsed":794,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"28c958c1-1a95-4f4f-8981-0181478a2357"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'-': 0,\n"," '0': 1,\n"," '1': 2,\n"," '2': 3,\n"," '3': 4,\n"," '4': 5,\n"," '5': 6,\n"," '6': 7,\n"," '7': 8,\n"," '8': 9,\n"," '9': 10}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["human_vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lj2Pn96_ncCT","executionInfo":{"status":"ok","timestamp":1657297292837,"user_tz":-480,"elapsed":502,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9a773484-a01f-4d2b-a5e8-21a9c3a2d769"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{' ': 0,\n"," '.': 1,\n"," '/': 2,\n"," '0': 3,\n"," '1': 4,\n"," '2': 5,\n"," '3': 6,\n"," '4': 7,\n"," '5': 8,\n"," '6': 9,\n"," '7': 10,\n"," '8': 11,\n"," '9': 12,\n"," '<pad>': 36,\n"," '<unk>': 35,\n"," 'a': 13,\n"," 'b': 14,\n"," 'c': 15,\n"," 'd': 16,\n"," 'e': 17,\n"," 'f': 18,\n"," 'g': 19,\n"," 'h': 20,\n"," 'i': 21,\n"," 'j': 22,\n"," 'l': 23,\n"," 'm': 24,\n"," 'n': 25,\n"," 'o': 26,\n"," 'p': 27,\n"," 'r': 28,\n"," 's': 29,\n"," 't': 30,\n"," 'u': 31,\n"," 'v': 32,\n"," 'w': 33,\n"," 'y': 34}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"po6HXVsQSEEL"},"source":["def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n","    X, Y = zip(*dataset)\n","    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n","    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n","    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n","    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n","    return X, np.array(Y), Xoh, Yoh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"RWWRbGeJSEEL","executionInfo":{"status":"ok","timestamp":1657298453381,"user_tz":-480,"elapsed":534,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"14c4d56f-872f-4cf2-de48-6574d842d693"},"source":["Tx = 30\n","Ty = 10\n","X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n","\n","print(\"X.shape:\", X.shape)\n","print(\"Y.shape:\", Y.shape)\n","print(\"Xoh.shape:\", Xoh.shape)\n","print(\"Yoh.shape:\", Yoh.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X.shape: (10000, 30)\n","Y.shape: (10000, 10)\n","Xoh.shape: (10000, 30, 37)\n","Yoh.shape: (10000, 10, 11)\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"UclQlg-BSEEM","executionInfo":{"status":"ok","timestamp":1621093702343,"user_tz":-480,"elapsed":7202,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"7f367009-13c2-4604-f78e-05d7eed6db1a"},"source":["index = 0\n","print(\"Source date:\", dataset[index][0])\n","print(\"Target date:\", dataset[index][1])\n","print()\n","print(\"Source after preprocessing (indices):\", X[index])\n","print(\"Target after preprocessing (indices):\", Y[index])\n","print()\n","print(\"Source after preprocessing (one-hot):\", Xoh[index])\n","print(\"Target after preprocessing (one-hot):\", Yoh[index])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Source date: 9 may 1998\n","Target date: 1998-05-09\n","\n","Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n"," 36 36 36 36 36 36]\n","Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n","\n","Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]]\n","Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xc8w3F6sSEEN"},"source":["## 1.2 Attention Mechanism\n","\n","* If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. \n","* Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n","* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n","\n","<table>\n","<td> \n","<img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/blob/master/05_sequence_models/06_neural_machine_translation_with_keras_lstm_attention/images/attn_model.png?raw=true\" style=\"width:500;height:500px;\"> <br>\n","</td> \n","<td> \n","<img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/blob/master/05_sequence_models/06_neural_machine_translation_with_keras_lstm_attention/images/attn_mechanism.png?raw=true\" style=\"width:500;height:500px;\"> <br>\n","</td> \n","</table>\n","<caption><center> Figure 1: Neural machine translation with attention</center></caption>\n"]},{"cell_type":"markdown","metadata":{"id":"DbW8PYaVSEEP"},"source":["**Pre-attention and Post-attention LSTMs on both sides of the attention mechanism**\n","\n","- There are two separate LSTMs in this model (see diagram on the left): pre-attention and post-attention LSTMs.\n","- *Pre-attention* Bi-LSTM is the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism.\n","    - The attention mechanism is shown in the middle of the left-hand diagram.\n","    - The pre-attention Bi-LSTM goes through $T_x$ time steps\n","- *Post-attention* LSTM: at the top of the diagram comes *after* the attention mechanism. \n","    - The post-attention LSTM goes through $T_y$ time steps. \n","\n","- The post-attention LSTM passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next. "]},{"cell_type":"markdown","metadata":{"id":"AAmQD12FSEEQ"},"source":["**Each time step does not use predictions from the previous time step**\n","\n","* The post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input.\n","* The post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input. \n","* We have designed the model this way because unlike language generation (where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date."]},{"cell_type":"markdown","metadata":{"id":"YTj-Ah_bSEEQ"},"source":["Concatenation of hidden states from the forward and backward pre-attention LSTMs\n","- $\\overrightarrow{a}^{\\langle t \\rangle}$: hidden state of the forward-direction, pre-attention LSTM.\n","- $\\overleftarrow{a}^{\\langle t \\rangle}$: hidden state of the backward-direction, pre-attention LSTM.\n","- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: the concatenation of the activations of both the forward-direction $\\overrightarrow{a}^{\\langle t \\rangle}$ and backward-directions $\\overleftarrow{a}^{\\langle t \\rangle}$ of the pre-attention Bi-LSTM. "]},{"cell_type":"markdown","metadata":{"id":"-nDkHqz3SEEQ"},"source":["**Computing \"energies\" $e^{\\langle t, t' \\rangle}$ as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$**\n","- Recall in the lesson videos \"Attention Model\", at time 6:45 to 8:16, the definition of \"e\" as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n","    - \"e\" is called the \"energies\" variable.\n","    - $s^{\\langle t-1 \\rangle}$ is the hidden state of the post-attention LSTM\n","    - $a^{\\langle t' \\rangle}$ is the hidden state of the pre-attention LSTM.\n","    - $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network, which learns the function to output $e^{\\langle t, t' \\rangle}$.\n","    - $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $a^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$."]},{"cell_type":"markdown","metadata":{"id":"osFM8LgRSEER"},"source":["## 1.3 One Step Attention\n","\n","**one_step_attention**\n","    - $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$: the attention weights\n","    - $context^{ \\langle t \\rangle }$: the context vector:\n","    \n","$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n","\n","* The function `model()` will call the layers in `one_step_attention()` $T_y$ using a for-loop.\n","* It is important that all $T_y$ copies have the same weights. \n","    * It should not reinitialize the weights every time. \n","    * In other words, all $T_y$ steps should have shared weights. \n"]},{"cell_type":"code","metadata":{"id":"MWxYcJk_SEET"},"source":["class Attention(tf.keras.layers.Layer):\n","    def __init__(self, x_len=Tx, name='attention'):\n","        super(Attention, self).__init__(name=name)\n","        self.repeator = RepeatVector(Tx)\n","        self.densor1 = Dense(10, activation = \"tanh\")\n","        self.densor2 = Dense(1, activation = \"relu\")\n","        self.activator = Activation(softmax)\n","        self.dotor = Dot(axes=1)\n","        \n","    def call(self, a, s_prev):\n","        # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) to concatenate it with a\n","        s_prev = self.repeator(s_prev) # (m, 30, 64)\n","\n","        # Use concatenator to concatenate a and s_prev on the last axis\n","        concat = tf.concat([a, s_prev], axis=-1) # (m, 30, 128)\n","\n","        # compute the \"intermediate energies\" variable e.\n","        score = self.densor1(concat) # (m, 30, 10)  \n","\n","        # compute the \"energies\" variable energies.\n","        score = self.densor2(score) # (m, 30, 1)  \n","\n","        # Use \"activator\" on \"score\" to compute the attention weights\n","        attention_w = self.activator(score) # (m, 30, 1)  \n","\n","        # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell\n","        context = self.dotor([attention_w, a])\n","        return context"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2GsjXV0SEET"},"source":["## 1.4 Model Building"]},{"cell_type":"code","metadata":{"id":"g9DCODzbSEEU"},"source":["def lstm_attention_model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n","    \n","    # Define the inputs of your model with a shape (Tx,)\n","    # Define s0(initial hidden state) and c0(initial cell state) for the decoder LSTM with shape (n_s,)\n","    X = Input(shape=(Tx, human_vocab_size)) # (m, 30, 37) \n","    s1 = Input(shape=(n_s,), name='s0') # (m, 64)\n","    c1 = Input(shape=(n_s,), name='c0') # (m, 64)\n","    s, c = s1, c1\n","    m = tf.cast(tf.shape(s1)[0], dtype='int32')\n","    \n","    # Initialize empty list of outputs\n","    outputs = []\n","    \n","    # layers, must be constant throughout the loop\n","    attention = Attention()\n","    post_activation_LSTM_cell = LSTM(n_s, return_state=True)\n","    dense_1 = Dense(len(machine_vocab), activation=softmax)\n","    \n","    # Define pre-attention Bi-LSTM\n","    a = Bidirectional(LSTM(n_a, return_sequences=True), input_shape=(m, Tx, n_a * 2))(X) # (m, 30, 64)\n","    \n","    # Iterate for Ty steps, without teacher forcing, last prediction is next input\n","    for t in range(Ty):\n","    \n","        # Perform one step of the attention mechanism to get back the context vector at step t\n","        context = attention.call(a, s) # (m, 1, 64)\n","        \n","        # Apply the post-attention LSTM cell to the \"context\" vector.\n","        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c]) # (m, 64)\n","        \n","        # Apply Dense layer to the hidden state output of the post-attention LSTM\n","        out = dense_1(s) # (m, 11)\n","                \n","        # Append \"out\" to the \"outputs\" list\n","        outputs.append(out) # (10, m, 11)\n","        \n","    outputs = tf.transpose(outputs, perm=[1, 0, 2])\n","    \n","    # Create model instance taking three inputs and returning the list of outputs.\n","    model = Model(inputs=[X, s1, c1], outputs=outputs)    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLEw6_iHSEEV"},"source":["# number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n","n_a = 32\n","# number of units for the post-attention LSTM's hidden state \"s\"\n","n_s = 64\n","model = lstm_attention_model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W70POFsCSEEV"},"source":["## 1.5 Model Compile"]},{"cell_type":"code","metadata":{"id":"XBiogVPxSEEV"},"source":["opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01) \n","model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZT5mS0qLSEEV"},"source":["## 1.6 Initialization\n","The last step is to define all your inputs and outputs to fit the model:\n","- You have input X of shape $(m = 10000, T_x = 30)$ containing the training examples.\n","- You need to create `s0` and `c0` to initialize your `post_attention_LSTM_cell` with zeros.\n","- Given the `model()` you coded, you need the \"outputs\" to be a list of 10 elements of shape (m, T_y). \n","    - The list `outputs[i][0], ..., outputs[i][Ty]` represents the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`). \n","    - `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."]},{"cell_type":"code","metadata":{"id":"oeLG3eR6SEEV"},"source":["s0 = np.zeros((m, n_s))\n","c0 = np.zeros((m, n_s))\n","# outputs = list(Yoh.swapaxes(0, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"k6Sc_bLlSEEW","executionInfo":{"status":"ok","timestamp":1621093913616,"user_tz":-480,"elapsed":214094,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"aa50c953-4794-4e59-b063-639d953f0f18"},"source":["model.fit([Xoh, s0, c0], Yoh, epochs=20, batch_size=64)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","157/157 [==============================] - 29s 59ms/step - loss: 1.8280 - accuracy: 0.3550\n","Epoch 2/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.8313 - accuracy: 0.6973\n","Epoch 3/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.6671 - accuracy: 0.7607\n","Epoch 4/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.5040 - accuracy: 0.8295\n","Epoch 5/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.3641 - accuracy: 0.8889\n","Epoch 6/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.2677 - accuracy: 0.9223\n","Epoch 7/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.2091 - accuracy: 0.9373\n","Epoch 8/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.1719 - accuracy: 0.9478\n","Epoch 9/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.1455 - accuracy: 0.9555\n","Epoch 10/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.1292 - accuracy: 0.9621\n","Epoch 11/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.1117 - accuracy: 0.9689\n","Epoch 12/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0975 - accuracy: 0.9747\n","Epoch 13/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0878 - accuracy: 0.9780\n","Epoch 14/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0781 - accuracy: 0.9819\n","Epoch 15/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.0679 - accuracy: 0.9855\n","Epoch 16/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0627 - accuracy: 0.9867\n","Epoch 17/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.0546 - accuracy: 0.9891\n","Epoch 18/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0504 - accuracy: 0.9897\n","Epoch 19/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0434 - accuracy: 0.9917\n","Epoch 20/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0398 - accuracy: 0.9927\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f0df334d590>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"08uhfa4LSEEW"},"source":[" <img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/raw/8a14375d1c66f21d01a46f508bbf670f8c6cf637/05_sequence_models/06_neural_machine_translation_with_keras_lstm_attention/images/table.png\" style=\"width:700;height:200px;\"> <br>\n","<caption><center>Thus, `dense_2_acc_8: 0.89` means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. </center></caption>"]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"8ei9Sl3kSEEW","executionInfo":{"status":"ok","timestamp":1621093918048,"user_tz":-480,"elapsed":217027,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"eefe9920-4f1a-481a-bf80-ea06c00b5282"},"source":["EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n","\n","source = np.array([string_to_int(example, Tx, human_vocab) for example in EXAMPLES])\n","source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n","\n","s_init = np.zeros((source.shape[0], n_s))\n","s_init = np.zeros((source.shape[0], n_s))\n","preds = model.predict([source, s_init, s_init])\n","\n","predictions = []\n","for pred, ex in zip(preds, EXAMPLES):\n","    out = np.argmax(pred, axis=-1)\n","    out = [inv_machine_vocab[int(i)] for i in out]\n","    out = ''.join(out)\n","    predictions.append(out)\n","    print(\"source:\", ex)\n","    print(\"output:\", out,\"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["source: 3 May 1979\n","output: 1979-05-03 \n","\n","source: 5 April 09\n","output: 2009-04-05 \n","\n","source: 21th of August 2016\n","output: 2016-08-21 \n","\n","source: Tue 10 Jul 2007\n","output: 2007-07-10 \n","\n","source: Saturday May 9 2018\n","output: 2018-05-09 \n","\n","source: March 3 2001\n","output: 2001-03-03 \n","\n","source: March 3rd 2001\n","output: 2001-03-03 \n","\n","source: 1 March 2001\n","output: 2001-03-01 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lXyEZJGFSEEY"},"source":["**Here's what you should remember**\n","\n","- Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French->English) but also for tasks like date format translation. \n","- An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output. \n","- A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different. \n","- You can visualize attention weights $\\alpha^{\\langle t,t' \\rangle}$ to see what the network is paying attention to while generating each output."]},{"cell_type":"markdown","metadata":{"id":"b3JEGGvuSEEY"},"source":["# 2) BahdanauAttention\n","\n","<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" style=\"width:600;height:300px;\"> <br>\n","<caption><center>\n","\n","The input is put through an encoder model which gives us the encoder output of shape (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size).\n","\n","Here are the equations that are implemented:\n","\n","<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" style=\"width:600;height:300px;\"> <br>\n","<caption><center>"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T09:13:45.584119Z","start_time":"2021-04-10T09:13:44.498729Z"},"id":"SiMT1uypSEEY"},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUUQ6xlSSEEY"},"source":["## 2.1 Data Loading"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T09:24:46.562361Z","start_time":"2021-04-10T09:24:46.551894Z"},"id":"eOcSYpcQSEEY"},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n","\n","def create_dataset(path, num_examples):\n","    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","    word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')] for line in lines[:num_examples]]\n","    return zip(*word_pairs)\n","\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    w = w.strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","def tokenize(lang):\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","    lang_tokenizer.fit_on_texts(lang)\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n","    # tensor = tf.one_hot(tensor, len(lang_tokenizer.index_word))\n","    return tensor, lang_tokenizer\n","\n","def load_dataset(path, num_examples=None):\n","    # creating cleaned input, output pairs\n","    targ_lang, inp_lang = create_dataset(path, num_examples)\n","    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T09:28:58.815856Z","start_time":"2021-04-10T09:28:58.488873Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"DUJo8mrxSEEZ","executionInfo":{"status":"ok","timestamp":1621097184389,"user_tz":-480,"elapsed":819,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"aa0872d2-4c5c-4cc1-b1f3-e1e6d49cef18"},"source":["num_examples = 64 * 15\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('datasets/spa.txt', num_examples)\n","print(input_tensor.shape, target_tensor.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(960, 9) (960, 7)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aiu2mYMlSEEZ"},"source":["## 2.2 Preparing Data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T16:18:52.835806Z","start_time":"2021-04-10T16:18:52.826635Z"},"id":"8IVkecc5SEEZ"},"source":["BUFFER_SIZE = len(input_tensor)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor) // BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index) + 1\n","vocab_tar_size = len(targ_lang.word_index) + 1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((\n","    {'inputs': input_tensor, 'dec_inputs': target_tensor},\n","    {'outputs': target_tensor}\n","))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T16:18:54.280043Z","start_time":"2021-04-10T16:18:54.249015Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"ZoiLWD61SEEZ","executionInfo":{"status":"ok","timestamp":1621097189589,"user_tz":-480,"elapsed":1099,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"b7f85962-90e2-4842-f0f8-ba08227b7691"},"source":["full_set = next(iter(dataset))\n","example_input_batch, example_target_batch = full_set[0]['inputs'], full_set[1]['outputs']\n","example_input_batch.shape, example_target_batch.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 9]), TensorShape([64, 7]))"]},"metadata":{"tags":[]},"execution_count":212}]},{"cell_type":"markdown","metadata":{"id":"ZFT0C8MqSEEa"},"source":["## 2.3 Model Building"]},{"cell_type":"code","metadata":{"id":"kH5lJgnYSEEa"},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, unit, name='BahdanauAttention'):\n","        super(BahdanauAttention, self).__init__(name=name)\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, query, values):\n","        # to broadcast with values (m, seq_len, hidden size)\n","        query_with_time_axis = tf.expand_dims(query, 1) # (m, n_h) --> # (m, 1, n_h)\n","        w1 = self.W1(query_with_time_axis) # (m, 1, n_h)\n","        w2 = self.W2(values) # (m, seq_len, n_h)\n","        \n","        # attention weights\n","        score = tf.nn.tanh(w1 + w2) # (m, seq_len, n_h)\n","        score = self.V(score) # (m, seq_len, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1) # (m, seq_len, 1)\n","        \n","        # context vector\n","        context_vector = attention_weights * values # (m, seq_len, n_h)\n","        context_vector = tf.reduce_sum(context_vector, axis=1) # (m, n_h)\n","        context_vector = tf.expand_dims(context_vector, 1) # (m, 1, n_h)\n","        return context_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-ppZJudSEEb"},"source":["def encoder_decoder(vocab_size_enc, vocab_size_dec, embedding_dim, n_h, batch_sz, Ty, name='Attention'):\n","    \n","    # inputs\n","    enc_inputs = tf.keras.Input(shape=(None, ), name=\"enc_inputs\") # (m, seq_len_en)\n","    dec_inputs = tf.keras.Input(shape=(None, ), name=\"dec_inputs\")\n","    \n","    # layers for encoder and decoder, should only initialized once\n","    embedding_enc = tf.keras.layers.Embedding(vocab_size_enc, embedding_dim)\n","    gru_enc = tf.keras.layers.GRU(n_h, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    attention = BahdanauAttention(n_h)\n","    embedding_dec = tf.keras.layers.Embedding(vocab_size_dec, embedding_dim)\n","    gru_dec = tf.keras.layers.GRU(n_h, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    fc_dec = tf.keras.layers.Dense(vocab_size_dec)\n","    \n","    # encoder\n","    initial_enc_hidden = tf.zeros((batch_sz, n_h)) # (m, n_h)\n","    enc_x = embedding_enc(enc_inputs) # (m, seq_len_en, embedding_dim)\n","    enc_output, enc_hidden = gru_enc(enc_x, initial_state=initial_enc_hidden) # (m, seq_len, n_h), (m, n_h)    \n","    \n","    # decoder, one input at a step, only initialized once\n","    # first decoder input is <start>\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1) # (m, 1)\n","    dec_hidden = enc_hidden\n","    outputs = []\n","    \n","    for t in range(1, Ty + 1):\n","        dec_x = embedding_dec(dec_inputs) # (m, 1, emb_dim)\n","        # first, enc attention + last enc hidden, then enc attention + last dec hidden\n","        context_vector = attention(dec_hidden, enc_output) # (m, n_h)\n","#         dec_x = tf.concat([tf.expand_dims(context_vector, 1), dec_x], axis=-1) # (m, 1, emb_dim + n_h)\n","        \n","        # gru, depend on both context and previous input\n","        dec_output, dec_hidden = gru_dec(context_vector) # (m, 1, n_h) (m, n_h) \n","        dec_output = tf.reshape(dec_output, (-1, dec_output.shape[2])) # (m * 1, n_h)\n","        pred = fc_dec(dec_output) # (m, vocab_size) \n","        \n","        # Teacher forcing - feeding the target as the next input\n","        dec_input = tf.expand_dims(dec_inputs[:, t], 1)\n","        outputs.append(pred)\n","\n","    outputs = tf.transpose(outputs, perm=[1, 0, 2]) # (m, seq_len, vocab_size)\n","    outputs = tf.nn.softmax(outputs, axis=-1)\n","    # outputs = tf.argmax(outputs, axis=-1) # (m, seq_len)\n","    # outputs = tf.cast(outputs, tf.float32)\n","    return tf.keras.Model(inputs=[enc_inputs, dec_inputs], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T16:13:49.641468Z","start_time":"2021-04-10T16:13:49.637501Z"},"id":"5UVXjZDnSEEb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621097781021,"user_tz":-480,"elapsed":3536,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"b5eba1f9-bf3f-4a2b-84be-7ece722fa931"},"source":["# unit test\n","model_test = encoder_decoder(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, target_tensor.shape[1])\n","output = model_test([example_input_batch, example_target_batch])\n","output.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 7, 362])"]},"metadata":{"tags":[]},"execution_count":221}]},{"cell_type":"code","metadata":{"id":"GurLt3XeSEEb"},"source":["tf.keras.backend.clear_session()\n","inp = [input_tensor, target_tensor]\n","out = target_tensor\n","model = encoder_decoder(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, target_tensor.shape[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kBPWBvc6SEEb"},"source":["MAX_LEN = target_tensor.shape[1] + 1\n","def loss_function(y_true, y_pred):\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LEN - 1))\n","    # SparseCategoricalCrossentropy as y_pred.shape = (m, seq_len, voab_size), while y_true.shape = (m, seq_len)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n","    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","    loss = tf.multiply(loss, mask)\n","    return tf.reduce_mean(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e719pKIMSEEc"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","        \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKrJfgLYSEEc"},"source":["learning_rate = CustomSchedule(embedding_dim)\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","    # ensure labels have shape (batch_size, MAX_LEN - 1)\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LEN - 1))\n","    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"FjSIpks_SEEc","executionInfo":{"status":"ok","timestamp":1621098037877,"user_tz":-480,"elapsed":257526,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"62f86f43-cdf0-4cc9-8832-5e8ba467d151"},"source":["model.fit([input_tensor, target_tensor], target_tensor, batch_size=BATCH_SIZE, epochs=10, steps_per_epoch=steps_per_epoch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","15/15 [==============================] - 37s 2s/step - loss: 4.4667 - accuracy: 0.0019\n","Epoch 2/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4657 - accuracy: 0.0612\n","Epoch 3/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4753 - accuracy: 0.1407\n","Epoch 4/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4716 - accuracy: 0.1429\n","Epoch 5/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4826 - accuracy: 0.1429\n","Epoch 6/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4621 - accuracy: 0.1429\n","Epoch 7/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4660 - accuracy: 0.1429\n","Epoch 8/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4454 - accuracy: 0.1429\n","Epoch 9/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4587 - accuracy: 0.1429\n","Epoch 10/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.3360 - accuracy: 0.1429\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f0dd8fce0d0>"]},"metadata":{"tags":[]},"execution_count":226}]}]}