{"nbformat":4,"nbformat_minor":0,"metadata":{"coursera":{"course_slug":"nlp-sequence-models","graded_item_id":"n16CQ","launcher_item_id":"npjGi"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"YqDN1ftTmHkX"},"source":["!wget https://raw.githubusercontent.com/PacktPublishing/The-Deep-Learning-Challenge/master/Section%205/source/spa-eng/spa.txt -P datasets\n","!pip install faker"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T09:12:31.826146Z","start_time":"2021-04-10T09:12:31.819440Z"},"id":"z6d4WuhdSED_"},"source":["from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model, Model\n","import tensorflow.keras.backend as K\n","import numpy as np\n","import tensorflow as tf\n","from faker import Faker\n","import random\n","from tqdm import tqdm\n","from babel.dates import format_date\n","import matplotlib.pyplot as plt\n","from lib.nmt_utils import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SoC1kZ6oSEEH"},"source":["# 1) Luong Attention"]},{"cell_type":"markdown","metadata":{"id":"amgPSJ_cSEEI"},"source":["## 1.1 Data Loading"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUhqW7ZISEEI","executionInfo":{"status":"ok","timestamp":1657297287708,"user_tz":-480,"elapsed":1060,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"3380f9be-c723-4b71-b8b7-4d0b6c39b501"},"source":["m = 10000\n","dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n","dataset[:10]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:00<00:00, 18467.01it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["[('9 may 1998', '1998-05-09'),\n"," ('10.11.19', '2019-11-10'),\n"," ('9/10/70', '1970-09-10'),\n"," ('saturday april 28 1990', '1990-04-28'),\n"," ('thursday january 26 1995', '1995-01-26'),\n"," ('monday march 7 1983', '1983-03-07'),\n"," ('sunday may 22 1988', '1988-05-22'),\n"," ('08 jul 2008', '2008-07-08'),\n"," ('8 sep 1999', '1999-09-08'),\n"," ('thursday january 1 1981', '1981-01-01')]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["machine_vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0DuJjnPndl5","executionInfo":{"status":"ok","timestamp":1657297299397,"user_tz":-480,"elapsed":794,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"28c958c1-1a95-4f4f-8981-0181478a2357"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'-': 0,\n"," '0': 1,\n"," '1': 2,\n"," '2': 3,\n"," '3': 4,\n"," '4': 5,\n"," '5': 6,\n"," '6': 7,\n"," '7': 8,\n"," '8': 9,\n"," '9': 10}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["human_vocab"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lj2Pn96_ncCT","executionInfo":{"status":"ok","timestamp":1657297292837,"user_tz":-480,"elapsed":502,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9a773484-a01f-4d2b-a5e8-21a9c3a2d769"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{' ': 0,\n"," '.': 1,\n"," '/': 2,\n"," '0': 3,\n"," '1': 4,\n"," '2': 5,\n"," '3': 6,\n"," '4': 7,\n"," '5': 8,\n"," '6': 9,\n"," '7': 10,\n"," '8': 11,\n"," '9': 12,\n"," '<pad>': 36,\n"," '<unk>': 35,\n"," 'a': 13,\n"," 'b': 14,\n"," 'c': 15,\n"," 'd': 16,\n"," 'e': 17,\n"," 'f': 18,\n"," 'g': 19,\n"," 'h': 20,\n"," 'i': 21,\n"," 'j': 22,\n"," 'l': 23,\n"," 'm': 24,\n"," 'n': 25,\n"," 'o': 26,\n"," 'p': 27,\n"," 'r': 28,\n"," 's': 29,\n"," 't': 30,\n"," 'u': 31,\n"," 'v': 32,\n"," 'w': 33,\n"," 'y': 34}"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"po6HXVsQSEEL"},"source":["def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n","    X, Y = zip(*dataset)\n","    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n","    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n","    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n","    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n","    return X, np.array(Y), Xoh, Yoh"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"RWWRbGeJSEEL","executionInfo":{"status":"ok","timestamp":1657298453381,"user_tz":-480,"elapsed":534,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"14c4d56f-872f-4cf2-de48-6574d842d693"},"source":["Tx = 30\n","Ty = 10\n","X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n","\n","print(\"X.shape:\", X.shape)\n","print(\"Y.shape:\", Y.shape)\n","print(\"Xoh.shape:\", Xoh.shape)\n","print(\"Yoh.shape:\", Yoh.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X.shape: (10000, 30)\n","Y.shape: (10000, 10)\n","Xoh.shape: (10000, 30, 37)\n","Yoh.shape: (10000, 10, 11)\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"UclQlg-BSEEM","executionInfo":{"status":"ok","timestamp":1621093702343,"user_tz":-480,"elapsed":7202,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"7f367009-13c2-4604-f78e-05d7eed6db1a"},"source":["index = 0\n","print(\"Source date:\", dataset[index][0])\n","print(\"Target date:\", dataset[index][1])\n","print()\n","print(\"Source after preprocessing (indices):\", X[index])\n","print(\"Target after preprocessing (indices):\", Y[index])\n","print()\n","print(\"Source after preprocessing (one-hot):\", Xoh[index])\n","print(\"Target after preprocessing (one-hot):\", Yoh[index])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Source date: 9 may 1998\n","Target date: 1998-05-09\n","\n","Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n"," 36 36 36 36 36 36]\n","Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n","\n","Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]]\n","Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Xc8w3F6sSEEN"},"source":["## 1.2 Attention Mechanism\n","\n","* If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate.\n","* Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down.\n","* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step.\n","\n","<table>\n","<td>\n","<img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/blob/master/05_sequence_models/06_neural_machine_translation_with_keras_lstm_attention/images/attn_model.png?raw=true\" style=\"width:500;height:500px;\"> <br>\n","</td>\n","<td>\n","<img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/blob/master/05_sequence_models/06_neural_machine_translation_with_keras_lstm_attention/images/attn_mechanism.png?raw=true\" style=\"width:500;height:500px;\"> <br>\n","</td>\n","</table>\n","<caption><center> Figure 1: Neural machine translation with attention</center></caption>\n"]},{"cell_type":"markdown","metadata":{"id":"DbW8PYaVSEEP"},"source":["**Pre-attention and Post-attention LSTMs on both sides of the attention mechanism**\n","\n","- There are two separate LSTMs in this model (see diagram on the left): pre-attention and post-attention LSTMs.\n","- *Pre-attention* Bi-LSTM is the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism.\n","    - The attention mechanism is shown in the middle of the left-hand diagram.\n","    - The pre-attention Bi-LSTM goes through $T_x$ time steps\n","- *Post-attention* LSTM: at the top of the diagram comes *after* the attention mechanism.\n","    - The post-attention LSTM goes through $T_y$ time steps.\n","\n","- The post-attention LSTM passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next."]},{"cell_type":"markdown","metadata":{"id":"AAmQD12FSEEQ"},"source":["**Each time step does not use predictions from the previous time step**\n","\n","* The post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input.\n","* The post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input.\n","* We have designed the model this way because unlike language generation (where adjacent characters are highly correlated) there isn't as strong a dependency between the previous character and the next character in a YYYY-MM-DD date."]},{"cell_type":"markdown","metadata":{"id":"YTj-Ah_bSEEQ"},"source":["Concatenation of hidden states from the forward and backward pre-attention LSTMs\n","- $\\overrightarrow{a}^{\\langle t \\rangle}$: hidden state of the forward-direction, pre-attention LSTM.\n","- $\\overleftarrow{a}^{\\langle t \\rangle}$: hidden state of the backward-direction, pre-attention LSTM.\n","- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: the concatenation of the activations of both the forward-direction $\\overrightarrow{a}^{\\langle t \\rangle}$ and backward-directions $\\overleftarrow{a}^{\\langle t \\rangle}$ of the pre-attention Bi-LSTM."]},{"cell_type":"markdown","metadata":{"id":"-nDkHqz3SEEQ"},"source":["**Computing \"energies\" $e^{\\langle t, t' \\rangle}$ as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$**\n","- Recall in the lesson videos \"Attention Model\", at time 6:45 to 8:16, the definition of \"e\" as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n","    - \"e\" is called the \"energies\" variable.\n","    - $s^{\\langle t-1 \\rangle}$ is the hidden state of the post-attention LSTM\n","    - $a^{\\langle t' \\rangle}$ is the hidden state of the pre-attention LSTM.\n","    - $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network, which learns the function to output $e^{\\langle t, t' \\rangle}$.\n","    - $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $a^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$."]},{"cell_type":"markdown","metadata":{"id":"osFM8LgRSEER"},"source":["## 1.3 One Step Attention\n","\n","**one_step_attention**\n","    - $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$: the attention weights\n","    - $context^{ \\langle t \\rangle }$: the context vector:\n","    \n","$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$\n","\n","* The function `model()` will call the layers in `one_step_attention()` $T_y$ using a for-loop.\n","* It is important that all $T_y$ copies have the same weights.\n","    * It should not reinitialize the weights every time.\n","    * In other words, all $T_y$ steps should have shared weights.\n"]},{"cell_type":"code","metadata":{"id":"MWxYcJk_SEET"},"source":["class Attention(tf.keras.layers.Layer):\n","    def __init__(self, x_len=Tx, name='attention'):\n","        super(Attention, self).__init__(name=name)\n","        self.repeator = RepeatVector(Tx)\n","        self.densor1 = Dense(10, activation = \"tanh\")\n","        self.densor2 = Dense(1, activation = \"relu\")\n","        self.activator = Activation(softmax)\n","        self.dotor = Dot(axes=1)\n","\n","    def call(self, a, s_prev):\n","        # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) to concatenate it with a\n","        s_prev = self.repeator(s_prev) # (m, 30, 64)\n","\n","        # Use concatenator to concatenate a and s_prev on the last axis\n","        concat = tf.concat([a, s_prev], axis=-1) # (m, 30, 128)\n","\n","        # compute the \"intermediate energies\" variable e.\n","        score = self.densor1(concat) # (m, 30, 10)\n","\n","        # compute the \"energies\" variable energies.\n","        score = self.densor2(score) # (m, 30, 1)\n","\n","        # Use \"activator\" on \"score\" to compute the attention weights\n","        attention_w = self.activator(score) # (m, 30, 1)\n","\n","        # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell\n","        context = self.dotor([attention_w, a])\n","        return context"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2GsjXV0SEET"},"source":["## 1.4 Model Building"]},{"cell_type":"code","metadata":{"id":"g9DCODzbSEEU"},"source":["def lstm_attention_model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n","\n","    # Define the inputs of your model with a shape (Tx,)\n","    # Define s0(initial hidden state) and c0(initial cell state) for the decoder LSTM with shape (n_s,)\n","    X = Input(shape=(Tx, human_vocab_size)) # (m, 30, 37)\n","    s1 = Input(shape=(n_s,), name='s0') # (m, 64)\n","    c1 = Input(shape=(n_s,), name='c0') # (m, 64)\n","    s, c = s1, c1\n","    m = tf.cast(tf.shape(s1)[0], dtype='int32')\n","\n","    # Initialize empty list of outputs\n","    outputs = []\n","\n","    # layers, must be constant throughout the loop\n","    attention = Attention()\n","    post_activation_LSTM_cell = LSTM(n_s, return_state=True)\n","    dense_1 = Dense(len(machine_vocab), activation=softmax)\n","\n","    # Define pre-attention Bi-LSTM\n","    a = Bidirectional(LSTM(n_a, return_sequences=True), input_shape=(m, Tx, n_a * 2))(X) # (m, 30, 64)\n","\n","    # Iterate for Ty steps, without teacher forcing, last prediction is next input\n","    for t in range(Ty):\n","\n","        # Perform one step of the attention mechanism to get back the context vector at step t\n","        context = attention.call(a, s) # (m, 1, 64)\n","\n","        # Apply the post-attention LSTM cell to the \"context\" vector.\n","        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c]) # (m, 64)\n","\n","        # Apply Dense layer to the hidden state output of the post-attention LSTM\n","        out = dense_1(s) # (m, 11)\n","\n","        # Append \"out\" to the \"outputs\" list\n","        outputs.append(out) # (10, m, 11)\n","\n","    outputs = tf.transpose(outputs, perm=[1, 0, 2])\n","\n","    # Create model instance taking three inputs and returning the list of outputs.\n","    model = Model(inputs=[X, s1, c1], outputs=outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kLEw6_iHSEEV"},"source":["# number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n","n_a = 32\n","# number of units for the post-attention LSTM's hidden state \"s\"\n","n_s = 64\n","model = lstm_attention_model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W70POFsCSEEV"},"source":["## 1.5 Model Compile"]},{"cell_type":"code","metadata":{"id":"XBiogVPxSEEV"},"source":["opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n","model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZT5mS0qLSEEV"},"source":["## 1.6 Initialization\n","The last step is to define all your inputs and outputs to fit the model:\n","- You have input X of shape $(m = 10000, T_x = 30)$ containing the training examples.\n","- You need to create `s0` and `c0` to initialize your `post_attention_LSTM_cell` with zeros.\n","- Given the `model()` you coded, you need the \"outputs\" to be a list of 10 elements of shape (m, T_y).\n","    - The list `outputs[i][0], ..., outputs[i][Ty]` represents the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`).\n","    - `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."]},{"cell_type":"code","metadata":{"id":"oeLG3eR6SEEV"},"source":["s0 = np.zeros((m, n_s))\n","c0 = np.zeros((m, n_s))\n","# outputs = list(Yoh.swapaxes(0, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"k6Sc_bLlSEEW","executionInfo":{"status":"ok","timestamp":1621093913616,"user_tz":-480,"elapsed":214094,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"aa50c953-4794-4e59-b063-639d953f0f18"},"source":["model.fit([Xoh, s0, c0], Yoh, epochs=20, batch_size=64)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","157/157 [==============================] - 29s 59ms/step - loss: 1.8280 - accuracy: 0.3550\n","Epoch 2/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.8313 - accuracy: 0.6973\n","Epoch 3/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.6671 - accuracy: 0.7607\n","Epoch 4/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.5040 - accuracy: 0.8295\n","Epoch 5/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.3641 - accuracy: 0.8889\n","Epoch 6/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.2677 - accuracy: 0.9223\n","Epoch 7/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.2091 - accuracy: 0.9373\n","Epoch 8/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.1719 - accuracy: 0.9478\n","Epoch 9/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.1455 - accuracy: 0.9555\n","Epoch 10/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.1292 - accuracy: 0.9621\n","Epoch 11/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.1117 - accuracy: 0.9689\n","Epoch 12/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0975 - accuracy: 0.9747\n","Epoch 13/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0878 - accuracy: 0.9780\n","Epoch 14/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0781 - accuracy: 0.9819\n","Epoch 15/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.0679 - accuracy: 0.9855\n","Epoch 16/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0627 - accuracy: 0.9867\n","Epoch 17/20\n","157/157 [==============================] - 9s 59ms/step - loss: 0.0546 - accuracy: 0.9891\n","Epoch 18/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0504 - accuracy: 0.9897\n","Epoch 19/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0434 - accuracy: 0.9917\n","Epoch 20/20\n","157/157 [==============================] - 9s 60ms/step - loss: 0.0398 - accuracy: 0.9927\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f0df334d590>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"08uhfa4LSEEW"},"source":[" <img src=\"https://github.com/sebastianbirk/coursera-deep-learning-specialization/raw/8a14375d1c66f21d01a46f508bbf670f8c6cf637/05_sequence_models/06_neural_machine_translation_with_keras_lstm_attention/images/table.png\" style=\"width:700;height:200px;\"> <br>\n","<caption><center>Thus, `dense_2_acc_8: 0.89` means that you are predicting the 7th character of the output correctly 89% of the time in the current batch of data. </center></caption>"]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"8ei9Sl3kSEEW","executionInfo":{"status":"ok","timestamp":1621093918048,"user_tz":-480,"elapsed":217027,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"eefe9920-4f1a-481a-bf80-ea06c00b5282"},"source":["EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n","\n","source = np.array([string_to_int(example, Tx, human_vocab) for example in EXAMPLES])\n","source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n","\n","s_init = np.zeros((source.shape[0], n_s))\n","s_init = np.zeros((source.shape[0], n_s))\n","preds = model.predict([source, s_init, s_init])\n","\n","predictions = []\n","for pred, ex in zip(preds, EXAMPLES):\n","    out = np.argmax(pred, axis=-1)\n","    out = [inv_machine_vocab[int(i)] for i in out]\n","    out = ''.join(out)\n","    predictions.append(out)\n","    print(\"source:\", ex)\n","    print(\"output:\", out,\"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["source: 3 May 1979\n","output: 1979-05-03 \n","\n","source: 5 April 09\n","output: 2009-04-05 \n","\n","source: 21th of August 2016\n","output: 2016-08-21 \n","\n","source: Tue 10 Jul 2007\n","output: 2007-07-10 \n","\n","source: Saturday May 9 2018\n","output: 2018-05-09 \n","\n","source: March 3 2001\n","output: 2001-03-03 \n","\n","source: March 3rd 2001\n","output: 2001-03-03 \n","\n","source: 1 March 2001\n","output: 2001-03-01 \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lXyEZJGFSEEY"},"source":["**Here's what you should remember**\n","\n","- Machine translation models can be used to map from one sequence to another. They are useful not just for translating human languages (like French->English) but also for tasks like date format translation.\n","- An attention mechanism allows a network to focus on the most relevant parts of the input when producing a specific part of the output.\n","- A network using an attention mechanism can translate from inputs of length $T_x$ to outputs of length $T_y$, where $T_x$ and $T_y$ can be different.\n","- You can visualize attention weights $\\alpha^{\\langle t,t' \\rangle}$ to see what the network is paying attention to while generating each output."]},{"cell_type":"markdown","metadata":{"id":"b3JEGGvuSEEY"},"source":["# 2) BahdanauAttention\n","\n","<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" style=\"width:600;height:300px;\"> <br>\n","<caption><center>\n","\n","The input is put through an encoder model which gives us the encoder output of shape (batch_size, max_length, hidden_size) and the encoder hidden state of shape (batch_size, hidden_size).\n","\n","Here are the equations that are implemented:\n","\n","<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" style=\"width:600;height:300px;\"> <br>\n","<caption><center>"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T09:13:45.584119Z","start_time":"2021-04-10T09:13:44.498729Z"},"id":"SiMT1uypSEEY"},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dUUQ6xlSSEEY"},"source":["## 2.1 Data Loading"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T09:24:46.562361Z","start_time":"2021-04-10T09:24:46.551894Z"},"id":"eOcSYpcQSEEY"},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n","\n","def create_dataset(path, num_examples):\n","    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n","    word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')] for line in lines[:num_examples]]\n","    return zip(*word_pairs)\n","\n","def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    w = w.strip()\n","    w = '<start> ' + w + ' <end>'\n","    return w\n","\n","def tokenize(lang):\n","    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n","    lang_tokenizer.fit_on_texts(lang)\n","    tensor = lang_tokenizer.texts_to_sequences(lang)\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n","    # tensor = tf.one_hot(tensor, len(lang_tokenizer.index_word))\n","    return tensor, lang_tokenizer\n","\n","def load_dataset(path, num_examples=None):\n","    # creating cleaned input, output pairs\n","    targ_lang, inp_lang = create_dataset(path, num_examples)\n","    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T09:28:58.815856Z","start_time":"2021-04-10T09:28:58.488873Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"DUJo8mrxSEEZ","executionInfo":{"status":"ok","timestamp":1621097184389,"user_tz":-480,"elapsed":819,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"aa0872d2-4c5c-4cc1-b1f3-e1e6d49cef18"},"source":["num_examples = 64 * 15\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('datasets/spa.txt', num_examples)\n","print(input_tensor.shape, target_tensor.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(960, 9) (960, 7)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aiu2mYMlSEEZ"},"source":["## 2.2 Preparing Data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T16:18:52.835806Z","start_time":"2021-04-10T16:18:52.826635Z"},"id":"8IVkecc5SEEZ"},"source":["BUFFER_SIZE = len(input_tensor)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor) // BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index) + 1\n","vocab_tar_size = len(targ_lang.word_index) + 1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((\n","    {'inputs': input_tensor, 'dec_inputs': target_tensor},\n","    {'outputs': target_tensor}\n","))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T16:18:54.280043Z","start_time":"2021-04-10T16:18:54.249015Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"ZoiLWD61SEEZ","executionInfo":{"status":"ok","timestamp":1621097189589,"user_tz":-480,"elapsed":1099,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"b7f85962-90e2-4842-f0f8-ba08227b7691"},"source":["full_set = next(iter(dataset))\n","example_input_batch, example_target_batch = full_set[0]['inputs'], full_set[1]['outputs']\n","example_input_batch.shape, example_target_batch.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 9]), TensorShape([64, 7]))"]},"metadata":{"tags":[]},"execution_count":212}]},{"cell_type":"markdown","metadata":{"id":"ZFT0C8MqSEEa"},"source":["## 2.3 Model Building"]},{"cell_type":"code","metadata":{"id":"kH5lJgnYSEEa"},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","    def __init__(self, unit, name='BahdanauAttention'):\n","        super(BahdanauAttention, self).__init__(name=name)\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, query, values):\n","        # to broadcast with values (m, seq_len, hidden size)\n","        query_with_time_axis = tf.expand_dims(query, 1) # (m, n_h) --> # (m, 1, n_h)\n","        w1 = self.W1(query_with_time_axis) # (m, 1, n_h)\n","        w2 = self.W2(values) # (m, seq_len, n_h)\n","\n","        # attention weights\n","        score = tf.nn.tanh(w1 + w2) # (m, seq_len, n_h)\n","        score = self.V(score) # (m, seq_len, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1) # (m, seq_len, 1)\n","\n","        # context vector\n","        context_vector = attention_weights * values # (m, seq_len, n_h)\n","        context_vector = tf.reduce_sum(context_vector, axis=1) # (m, n_h)\n","        context_vector = tf.expand_dims(context_vector, 1) # (m, 1, n_h)\n","        return context_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-ppZJudSEEb"},"source":["def encoder_decoder(vocab_size_enc, vocab_size_dec, embedding_dim, n_h, batch_sz, Ty, name='Attention'):\n","\n","    # inputs\n","    enc_inputs = tf.keras.Input(shape=(None, ), name=\"enc_inputs\") # (m, seq_len_en)\n","    dec_inputs = tf.keras.Input(shape=(None, ), name=\"dec_inputs\")\n","\n","    # layers for encoder and decoder, should only initialized once\n","    embedding_enc = tf.keras.layers.Embedding(vocab_size_enc, embedding_dim)\n","    gru_enc = tf.keras.layers.GRU(n_h, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    attention = BahdanauAttention(n_h)\n","    embedding_dec = tf.keras.layers.Embedding(vocab_size_dec, embedding_dim)\n","    gru_dec = tf.keras.layers.GRU(n_h, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n","    fc_dec = tf.keras.layers.Dense(vocab_size_dec)\n","\n","    # encoder\n","    initial_enc_hidden = tf.zeros((batch_sz, n_h)) # (m, n_h)\n","    enc_x = embedding_enc(enc_inputs) # (m, seq_len_en, embedding_dim)\n","    enc_output, enc_hidden = gru_enc(enc_x, initial_state=initial_enc_hidden) # (m, seq_len, n_h), (m, n_h)\n","\n","    # decoder, one input at a step, only initialized once\n","    # first decoder input is <start>\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1) # (m, 1)\n","    dec_hidden = enc_hidden\n","    outputs = []\n","\n","    for t in range(1, Ty + 1):\n","        dec_x = embedding_dec(dec_inputs) # (m, 1, emb_dim)\n","        # first, enc attention + last enc hidden, then enc attention + last dec hidden\n","        context_vector = attention(dec_hidden, enc_output) # (m, n_h)\n","#         dec_x = tf.concat([tf.expand_dims(context_vector, 1), dec_x], axis=-1) # (m, 1, emb_dim + n_h)\n","\n","        # gru, depend on both context and previous input\n","        dec_output, dec_hidden = gru_dec(context_vector) # (m, 1, n_h) (m, n_h)\n","        dec_output = tf.reshape(dec_output, (-1, dec_output.shape[2])) # (m * 1, n_h)\n","        pred = fc_dec(dec_output) # (m, vocab_size)\n","\n","        # Teacher forcing - feeding the target as the next input\n","        dec_input = tf.expand_dims(dec_inputs[:, t], 1)\n","        outputs.append(pred)\n","\n","    outputs = tf.transpose(outputs, perm=[1, 0, 2]) # (m, seq_len, vocab_size)\n","    outputs = tf.nn.softmax(outputs, axis=-1)\n","    # outputs = tf.argmax(outputs, axis=-1) # (m, seq_len)\n","    # outputs = tf.cast(outputs, tf.float32)\n","    return tf.keras.Model(inputs=[enc_inputs, dec_inputs], outputs=outputs, name=name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2021-04-10T16:13:49.641468Z","start_time":"2021-04-10T16:13:49.637501Z"},"id":"5UVXjZDnSEEb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621097781021,"user_tz":-480,"elapsed":3536,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"b5eba1f9-bf3f-4a2b-84be-7ece722fa931"},"source":["# unit test\n","model_test = encoder_decoder(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, target_tensor.shape[1])\n","output = model_test([example_input_batch, example_target_batch])\n","output.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 7, 362])"]},"metadata":{"tags":[]},"execution_count":221}]},{"cell_type":"code","metadata":{"id":"GurLt3XeSEEb"},"source":["tf.keras.backend.clear_session()\n","inp = [input_tensor, target_tensor]\n","out = target_tensor\n","model = encoder_decoder(vocab_inp_size, vocab_tar_size, embedding_dim, units, BATCH_SIZE, target_tensor.shape[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kBPWBvc6SEEb"},"source":["MAX_LEN = target_tensor.shape[1] + 1\n","def loss_function(y_true, y_pred):\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LEN - 1))\n","    # SparseCategoricalCrossentropy as y_pred.shape = (m, seq_len, voab_size), while y_true.shape = (m, seq_len)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n","    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n","    loss = tf.multiply(loss, mask)\n","    return tf.reduce_mean(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e719pKIMSEEc"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","        self.warmup_steps = warmup_steps\n","\n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps**-1.5)\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKrJfgLYSEEc"},"source":["learning_rate = CustomSchedule(embedding_dim)\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","\n","def accuracy(y_true, y_pred):\n","    # ensure labels have shape (batch_size, MAX_LEN - 1)\n","    y_true = tf.reshape(y_true, shape=(-1, MAX_LEN - 1))\n","    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n","\n","model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"FjSIpks_SEEc","executionInfo":{"status":"ok","timestamp":1621098037877,"user_tz":-480,"elapsed":257526,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"62f86f43-cdf0-4cc9-8832-5e8ba467d151"},"source":["model.fit([input_tensor, target_tensor], target_tensor, batch_size=BATCH_SIZE, epochs=10, steps_per_epoch=steps_per_epoch)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","15/15 [==============================] - 37s 2s/step - loss: 4.4667 - accuracy: 0.0019\n","Epoch 2/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4657 - accuracy: 0.0612\n","Epoch 3/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4753 - accuracy: 0.1407\n","Epoch 4/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4716 - accuracy: 0.1429\n","Epoch 5/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4826 - accuracy: 0.1429\n","Epoch 6/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4621 - accuracy: 0.1429\n","Epoch 7/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4660 - accuracy: 0.1429\n","Epoch 8/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4454 - accuracy: 0.1429\n","Epoch 9/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.4587 - accuracy: 0.1429\n","Epoch 10/10\n","15/15 [==============================] - 24s 2s/step - loss: 4.3360 - accuracy: 0.1429\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f0dd8fce0d0>"]},"metadata":{"tags":[]},"execution_count":226}]},{"cell_type":"markdown","source":["# 3) Scaled Dot-Product Attention"],"metadata":{"id":"c7h6Bb9yFNRd"}},{"cell_type":"markdown","source":["## 3.1 Self Attention\n","\n","![](https://pbs.twimg.com/media/FzjhZk5X0AYAs_-?format=jpg&name=4096x4096)"],"metadata":{"id":"YR2jxz9xGUpE"}},{"cell_type":"code","source":["import torch\n","from torch import nn, Tensor"],"metadata":{"id":"wL2-lbosFSgX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, ori_dim:int=512, embed_dim:int=512) -> None:\n","        super().__init__()\n","        self.embed_dim = embed_dim\n","        self.dim_K = torch.tensor(embed_dim)\n","        self.query = nn.Linear(in_features=ori_dim, out_features=embed_dim, bias=True)\n","        self.key  = nn.Linear(in_features=ori_dim, out_features=embed_dim, bias=True)\n","        self.value = nn.Linear(in_features=ori_dim, out_features=embed_dim, bias=True)\n","\n","    def self_attention(self, Q:Tensor, K:Tensor, V:Tensor) -> Tensor:\n","        K_T = torch.transpose(K, 0, 1)\n","        score = torch.matmul(Q, K_T)  / torch.sqrt(self.dim_K)\n","        score = torch.softmax(score, dim=-1)\n","        Z = torch.matmul(score, V)\n","        return Z\n","\n","    def forward(self, x:Tensor) -> Tensor:\n","        Q = self.query(x)\n","        K = self.key(x)\n","        V = self.value(x)\n","        Z = self.self_attention(Q, K, V)\n","        return Z"],"metadata":{"id":"nQk5u5LYFPLi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.2 Multi-head Attention"],"metadata":{"id":"APt8N8K8GXrW"}},{"cell_type":"code","source":["class MultiheadAttention(nn.Module):\n","    \"\"\"https://arxiv.org/abs/1706.03762\"\"\"\n","\n","    def __init__(self, embed_dim:int=512, n_head:int=8) -> None:\n","        super().__init__()\n","        self.n_head = n_head\n","        self.embed_dim = embed_dim\n","        self.multihead = nn.ModuleList([\n","            Attention(embed_dim, embed_dim // n_head) for _ in range(n_head)\n","        ])\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        Z = torch.cat([head(x) for head in self.multihead], dim=1)\n","        return Z"],"metadata":{"id":"lRxqMKoIFU_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiQueryAttention(Attention):\n","    \"\"\"\n","    https://arxiv.org/pdf/1911.02150.pdf\n","\n","    The keys and values are shared across all of the different attention \"heads\",\n","    greatly reducing the size of these tensors and hence the memory bandwidth requirements\n","    of incremental decoding. We verify experimentally that the resulting models can indeed\n","    be much faster to decode, and incur only minor qualitydegradation from the baseline.\n","    \"\"\"\n","\n","    def __init__(self, embed_dim:int=512, n_query:int=8) -> None:\n","        super().__init__(embed_dim, embed_dim // n_query)\n","        self.n_query = n_query\n","        delattr(self, 'query')\n","\n","        self.querys = nn.ModuleList([\n","            nn.Linear(in_features=embed_dim, out_features=embed_dim // n_query, bias=True)\n","            for _ in range(n_query)\n","        ])\n","        self.key = nn.Linear(in_features=embed_dim, out_features=embed_dim // n_query, bias=True)\n","        self.value = nn.Linear(in_features=embed_dim, out_features=embed_dim // n_query, bias=True)\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        K = self.key(x)\n","        V = self.value(x)\n","\n","        # x should break into: n_query * q(m, seq_dim, dim)\n","        Z = torch.cat([self.self_attention(query(x), K, V) for query in self.querys], dim=1)\n","        return Z"],"metadata":{"id":"Bslm_wPOF5V0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class  GroupedQueryAttention(Attention):\n","    \"\"\"https://arxiv.org/pdf/2305.13245.pdf\"\"\"\n","\n","    def __init__(self, embed_dim: int=512, n_grouped:int=4, n_query_each_group:int=2) -> None:\n","        super().__init__(embed_dim, embed_dim // (n_query * n_query_each_group))\n","        delattr(self, 'query')\n","        delattr(self, 'key')\n","        delattr(self, 'value')\n","\n","        self.grouped = nn.ModuleList([\n","            MultiQueryAttention(embed_dim // (n_grouped * n_query_each_group), n_query=n_query_each_group)\n","            for _ in range(n_grouped)\n","        ])\n","\n","    def forward(self, x: Tensor) -> Tensor:\n","        # x should break into: n_grouped * q(m, seq_dim, dim)\n","        Z = torch.cat([head(x) for head in self.grouped], dim=1)\n","        return Z"],"metadata":{"id":"eOMQbDjiF9cF"},"execution_count":null,"outputs":[]}]}