{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/ksm26/LangChain-for-LLM-Application-Development/main/OutdoorClothingCatalog_1000.csv\n","!apt install subversion\n","!mkdir docs\n","!svn checkout https://github.com/ksm26/LangChain-Chat-with-Your-Data/trunk/docs/cs229_lectures docs\n","!pip install sentence_transformers openai langchain tiktoken docarray wikipedia google-search-results pypdf yt_dlp pydub chromadb lark"],"metadata":{"id":"3oage5ewMvmH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","import openai\n","import langchain\n","from datetime import date\n","from langchain.chat_models import ChatOpenAI\n","from langchain.llms import OpenAI\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.prompts import ChatPromptTemplate, PromptTemplate\n","from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains import (\n","    ConversationChain,\n","    LLMChain,\n","    SimpleSequentialChain,\n","    SequentialChain,\n","    RetrievalQA,\n","    ConversationalRetrievalChain\n",")\n","from langchain.chains.router import MultiPromptChain\n","from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n","from langchain.memory import (\n","    ConversationBufferMemory,\n","    ConversationBufferWindowMemory,\n","    ConversationSummaryBufferMemory,\n","    ConversationTokenBufferMemory\n",")\n","from langchain.evaluation.qa import QAGenerateChain, QAEvalChain\n","from langchain.document_loaders import CSVLoader\n","from langchain.vectorstores import DocArrayInMemorySearch\n","from langchain.indexes import VectorstoreIndexCreator\n","from IPython.display import display, Markdown\n","from langchain.agents.agent_toolkits import create_python_agent\n","from langchain.agents import load_tools, initialize_agent, AgentType, tool\n","from langchain.tools.python.tool import PythonREPLTool\n","from langchain.python import PythonREPL\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.document_loaders.generic import GenericLoader\n","from langchain.document_loaders.parsers import OpenAIWhisperParser\n","from langchain.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n","from langchain.document_loaders import WebBaseLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n","from langchain.text_splitter import TokenTextSplitter\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.retrievers.self_query.base import SelfQueryRetriever\n","from langchain.chains.query_constructor.base import AttributeInfo\n","from langchain.retrievers import ContextualCompressionRetriever, SVMRetriever, TFIDFRetriever\n","from langchain.retrievers.document_compressors import LLMChainExtractor\n","\n","os.environ['OPENAI_API_KEY'] = ''\n","os.environ['SERPAPI_API_KEY'] = ''\n","openai.api_key = os.environ['OPENAI_API_KEY']"],"metadata":{"id":"Ch2HCHuhMvpD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1) LangChain DataBase"],"metadata":{"id":"-BaM7UhnIxqK"}},{"cell_type":"markdown","source":["## 1.1 Data Loader"],"metadata":{"id":"Fdwk3PiRq1wu"}},{"cell_type":"code","source":["url=\"https://www.youtube.com/watch?v=jGwO_UgTS7I\"\n","save_dir=\"docs/youtube/\"\n","loader = GenericLoader(\n","    YoutubeAudioLoader([url],save_dir),\n","    OpenAIWhisperParser()\n",")\n","docs = loader.load()\n","docs[0].page_content[0:200]"],"metadata":{"id":"0O9eVwKmR8Qm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = WebBaseLoader(\"https://github.com/basecamp/handbook/blob/master/37signals-is-you.md\")\n","docs = loader.load()\n","print(docs[0].page_content[2000:2500])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NEbzHf9Ir4qM","executionInfo":{"status":"ok","timestamp":1690079544809,"user_tz":-480,"elapsed":520,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"bc969bb2-b0c5-42cb-e3c9-c1fdb77e5c4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ches to filter your results more quickly\n","\n","\n","\n","\n","\n","\n","\n","\n"," \n","\n","\n","\n","\n","\n","Name\n","\n","\n","\n","\n","\n","\n","Query\n","\n","\n","\n","            To see all available qualifiers, see our documentation.\n","          \n"," \n","\n","\n","\n","\n","     Cancel\n","\n","    Create saved search\n","\n","\n","\n","\n","\n","\n","\n","\n","              Sign in\n","            \n","\n","\n","              Sign up\n","            \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","You signed in with another tab or window. Reload to refresh your session.\n","You signed out in another tab or window. Reload to refresh your session.\n","You switched accounts on another tab or window. Reload to refresh \n"]}]},{"cell_type":"markdown","source":["## 1.2 Data Splitter\n","\n","[Langchain Documentation - Split](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/character_text_splitter)"],"metadata":{"id":"xjRG2oMIsOBy"}},{"cell_type":"code","source":["chunk_size = 450\n","chunk_overlap = 0\n","\n","c_splitter = CharacterTextSplitter(\n","    chunk_size=chunk_size,\n","    chunk_overlap=chunk_overlap,\n","    separator = ' '\n",")\n","\n","r_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=chunk_size,\n","    chunk_overlap=chunk_overlap,\n","    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",")"],"metadata":{"id":"LGc6VcoMmnhz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["some_text = \"\"\"\\\n","When writing documents, writers will use document structure to group content. \\\n","This can convey to the reader, which idea's are related. For example, closely related ideas \\\n","are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n","Paragraphs are often delimited with a carriage return or two carriage returns. \\\n","Carriage returns are the \"backslash n\" you see embedded in this string. \\\n","Sentences have a period at the end, but also, have a space.\\\n","and words are separated by space.\\\n","\"\"\"\n","\n","print(c_splitter.split_text(some_text))\n","print(r_splitter.split_text(some_text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPr4vQ9BsGhW","executionInfo":{"status":"ok","timestamp":1690079959515,"user_tz":-480,"elapsed":662,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"38192368-7137-49aa-b8a7-5665d2ef12fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,', 'have a space.and words are separated by space.']\n","[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\", 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']\n"]}]},{"cell_type":"code","source":["r_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=150,\n","    chunk_overlap=0,\n","    separators=[\"\\n\\n\", \"\\n\", \"\\. \", \" \", \"\"]\n",")\n","r_splitter.split_text(some_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8mBekbVm_Ao","executionInfo":{"status":"ok","timestamp":1690079960772,"user_tz":-480,"elapsed":4,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"e6f1e04a-89d7-437c-b3cb-abad236d8e30"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related\",\n"," '. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.',\n"," 'Paragraphs are often delimited with a carriage return or two carriage returns',\n"," '. Carriage returns are the \"backslash n\" you see embedded in this string',\n"," '. Sentences have a period at the end, but also, have a space.and words are separated by space.']"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## 1.3 Token splitting\n","\n","We can also split on token count explicity, if we want. This can be useful because LLMs often have context windows designated in tokens. Tokens are often ~4 characters."],"metadata":{"id":"NjOEZxlwo2tu"}},{"cell_type":"code","source":["text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)\n","text1 = \"foo bar bazzyfoo\"\n","print(text_splitter.split_text(text1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhU7gnqWo1I8","executionInfo":{"status":"ok","timestamp":1690080388085,"user_tz":-480,"elapsed":2728,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"664180db-e2bd-4619-ecc9-8bf6644abb35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['foo', ' bar', ' b', 'az', 'zy', 'foo']\n"]}]},{"cell_type":"markdown","source":["## 1.4 Vectorstores and Embeddings"],"metadata":{"id":"MntFqW5Kq2ey"}},{"cell_type":"markdown","source":["### 1.4.1 Similarity Search"],"metadata":{"id":"AmZ7GbqIxb6d"}},{"cell_type":"code","source":["loaders = [\n","    # Duplicate documents on purpose - messy data\n","    PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\"),\n","    PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\"),\n","    PyPDFLoader(\"docs/MachineLearning-Lecture02.pdf\"),\n","    PyPDFLoader(\"docs/MachineLearning-Lecture03.pdf\")\n","]\n","embedding = OpenAIEmbeddings()\n","\n","docs = []\n","for loader in loaders:\n","    docs.extend(loader.load())\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1500,\n","    chunk_overlap=150,\n","    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",")\n","splits = text_splitter.split_documents(docs)\n","print('split', len(splits))\n","\n","persist_directory = './chroma/'\n","vectordb = Chroma.from_documents(\n","    documents=splits,\n","    embedding=embedding,\n","    persist_directory=persist_directory\n",")\n","print('vectordb', vectordb._collection.count())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-C40u9KhqOwI","executionInfo":{"status":"ok","timestamp":1690081481384,"user_tz":-480,"elapsed":7833,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"16e85421-6e0b-4fde-d43c-aace529e56c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["split 209\n","vectordb 209\n"]}]},{"cell_type":"code","source":["splits[5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBb3urmxtReO","executionInfo":{"status":"ok","timestamp":1690081509003,"user_tz":-480,"elapsed":8,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"dc0a433e-a017-4422-f6d5-1a9c7302b2ee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(page_content=\"turns out to be extremely difficult to sit dow n and write a program to  fly a helicopter.  \\nBut in contrast, if you want to do things like to get software to fl y a helicopter or have \\nsoftware recognize handwritten digits, one very  successful approach is to use a learning \\nalgorithm and have a computer learn by its elf how to, say, recognize your handwriting. \\nAnd in fact, handwritten digit recognition, this is pretty much the only approach that \\nworks well. It uses applications that are hard to program by hand.  \\nLearning algorithms has also made I guess sign ificant inroads in what's sometimes called \\ndatabase mining. So, for example, with the growth of IT and computers, increasingly \\nmany hospitals are keeping around medical reco rds of what sort of patients, what \\nproblems they had, what their prognoses was,  what the outcome was. And taking all of \\nthese medical records, which started to be digitized only about maybe 15 years, applying \\nlearning algorithms to them can turn raw medi cal records into what I might loosely call \\nmedical knowledge in which we start to detect trends in medical practice and even start to \\nalter medical practice as a result of me dical knowledge that's derived by applying \\nlearning algorithms to the sorts of medical r ecords that hospitals have just been building \\nover the last 15, 20 years in an electronic format.  \\nTurns out that most of you probably use learning algorithms — I don't know — I think\", metadata={'source': 'docs/MachineLearning-Lecture01.pdf', 'page': 2})"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["splits[6]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K62-LoXqtS48","executionInfo":{"status":"ok","timestamp":1690081509004,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"0a2b8af2-d5c3-4945-a0e9-c0a3d3d3a722"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(page_content=\"over the last 15, 20 years in an electronic format.  \\nTurns out that most of you probably use learning algorithms — I don't know — I think \\nhalf a dozen times a day or maybe a dozen  times a day or more, and often without \\nknowing it. So, for example, every time you se nd mail via the US Postal System, turns \\nout there's an algorithm that tries to automa tically read the zip code you wrote on your \\nenvelope, and that's done by a learning al gorithm. So every time you send US mail, you \\nare using a learning algorithm, perhap s without even being aware of it.\", metadata={'source': 'docs/MachineLearning-Lecture01.pdf', 'page': 2})"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["# similarity search\n","question = \"is there an email i can ask for help\"\n","docs = vectordb.similarity_search(question, k=3)\n","print(len(docs))\n","print(docs[0].page_content)\n","vectordb.persist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A41-_O-eqO1a","executionInfo":{"status":"ok","timestamp":1690081565704,"user_tz":-480,"elapsed":1209,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"396a9a11-19d1-44e8-9423-83525a20106c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3\n","cs229-qa@cs.stanford.edu. This goes to an acc ount that's read by all the TAs and me. So \n","rather than sending us email individually, if you send email to this account, it will \n","actually let us get back to you maximally quickly with answers to your questions.  \n","If you're asking questions about homework probl ems, please say in the subject line which \n","assignment and which question the email refers to, since that will also help us to route \n","your question to the appropriate TA or to me  appropriately and get the response back to \n","you quickly.  \n","Let's see. Skipping ahead — let's see — for homework, one midterm, one open and term \n","project. Notice on the honor code. So one thi ng that I think will help you to succeed and \n","do well in this class and even help you to enjoy this cla ss more is if you form a study \n","group.  \n","So start looking around where you' re sitting now or at the end of class today, mingle a \n","little bit and get to know your classmates. I strongly encourage you to form study groups \n","and sort of have a group of people to study with and have a group of your fellow students \n","to talk over these concepts with. You can also  post on the class news group if you want to \n","use that to try to form a study group.  \n","But some of the problems sets in this cla ss are reasonably difficult.  People that have \n","taken the class before may tell you they were very difficult. And just I bet it would be \n","more fun for you, and you'd probably have a be tter learning experience if you form a\n"]}]},{"cell_type":"markdown","source":["### 1.4.2 MMR"],"metadata":{"id":"BarwzZPCxe_i"}},{"cell_type":"code","source":["question = \"what did they say about matlab?\"\n","docs_ss = vectordb.similarity_search(question, k=3)\n","print('doc1 without mmr: ', docs_ss[0].page_content[:100])\n","print('doc2 without mmr: ', docs_ss[1].page_content[:100])\n","\n","docs_mmr = vectordb.max_marginal_relevance_search(question, k=3)\n","print('\\ndoc1 with mmr: ', docs_mmr[0].page_content[:100])\n","print('doc2 with mmr: ', docs_mmr[1].page_content[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blQ0H4wAui28","executionInfo":{"status":"ok","timestamp":1690082464927,"user_tz":-480,"elapsed":1235,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9c03dde9-10a3-4627-8417-18dee898b5a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["doc1 without mmr:  those homeworks will be done in either MATLA B or in Octave, which is sort of — I \n","know some people \n","doc2 without mmr:  those homeworks will be done in either MATLA B or in Octave, which is sort of — I \n","know some people \n","\n","doc1 with mmr:  those homeworks will be done in either MATLA B or in Octave, which is sort of — I \n","know some people \n","doc2 with mmr:  algorithm then? So what’s different? How come  I was making all that noise earlier about \n","least squa\n"]}]},{"cell_type":"markdown","source":["### 1.4.3 MetaData Search"],"metadata":{"id":"dbVw45WTxi4a"}},{"cell_type":"code","source":["# filter with metadata\n","question = \"what did they say about regression in the third lecture?\"\n","docs = vectordb.similarity_search(\n","    question, k=3, filter={\"source\":\"docs/MachineLearning-Lecture03.pdf\"}\n",")\n","for d in docs:\n","    print(d.metadata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ofd-daH3uEeu","executionInfo":{"status":"ok","timestamp":1690082722979,"user_tz":-480,"elapsed":530,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"cfcf40b3-e64b-47fd-c006-0823485724f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'page': 0, 'source': 'docs/MachineLearning-Lecture03.pdf'}\n","{'page': 14, 'source': 'docs/MachineLearning-Lecture03.pdf'}\n","{'page': 4, 'source': 'docs/MachineLearning-Lecture03.pdf'}\n"]}]},{"cell_type":"code","source":["# metadata_field_info = [\n","#     AttributeInfo(\n","#         name=\"source\",\n","#         description=\"The lecture the chunk is from, should be one of `docs/MachineLearning-Lecture01.pdf`, `docs/MachineLearning-Lecture02.pdf`, or `docs/MachineLearning-Lecture03.pdf`\",\n","#         type=\"string\",\n","#     ),\n","#     AttributeInfo(\n","#         name=\"page\",\n","#         description=\"The page from the lecture\",\n","#         type=\"integer\",\n","#     ),\n","# ]\n","\n","# document_content_description = \"Lecture notes\"\n","# llm = OpenAI(temperature=0)\n","# retriever = SelfQueryRetriever.from_llm(\n","#     llm,\n","#     vectordb,\n","#     document_content_description,\n","#     metadata_field_info,\n","#     verbose=True\n","# )\n","\n","# question = \"what did they say about regression in the third lecture?\"\n","# docs = retriever.get_relevant_documents(question)\n","# for d in docs:\n","#     print(d.metadata)"],"metadata":{"id":"-wHiKrBaqO7Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.4.4 Contextual Compression Retriever\n","\n","A Contextual Compression Retriever is designed to improve the answers returned from vector store document similarity searches by better taking into account the context from the query.\n","\n","It wraps another retriever, and uses a Document Compressor as an intermediate step after the initial similarity search that removes information irrelevant to the initial query from the retrieved documents. This reduces the amount of distraction a subsequent chain has to deal with when parsing the retrieved documents and making its final judgements."],"metadata":{"id":"63CuUbSwzPoe"}},{"cell_type":"code","source":["def pretty_print_docs(docs):\n","    print(f\"\\n{'-' * 100}\\n\".join(\n","        [f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)])\n","    )\n","\n","llm = OpenAI(temperature=0)\n","compressor = LLMChainExtractor.from_llm(llm)\n","compression_retriever = ContextualCompressionRetriever(\n","    base_compressor=compressor,\n","    base_retriever=vectordb.as_retriever(search_type=\"mmr\")\n",")\n","question = \"what did they say about matlab?\"\n","compressed_docs = compression_retriever.get_relevant_documents(question)\n","pretty_print_docs(compressed_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSGUVHMnxQsB","executionInfo":{"status":"ok","timestamp":1690083237738,"user_tz":-480,"elapsed":5282,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"1f26eb09-c9b4-495b-d3e2-90cfa8106d47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Document 1:\n","\n","\"MATLAB is I guess part of the programming language that makes it very easy to write codes using matrices, to write code for numerical routines, to move data around, to plot data. And it's sort of an extremely easy to learn tool to use for implementing a lot of learning algorithms.\"\n","----------------------------------------------------------------------------------------------------\n","Document 2:\n","\n","\"And the student said, \"Oh, it was the MATLAB.\" So for those of you that don't know MATLAB yet, I hope you do learn it. It's not hard, and we'll actually have a short MATLAB tutorial in one of the discussion sections for those of you that don't know it.\"\n"]}]},{"cell_type":"markdown","source":["### 1.4.5 Retrieval with TfIdf / SVM"],"metadata":{"id":"dCVCi1T9z-v5"}},{"cell_type":"code","source":["loader = PyPDFLoader(\"docs/MachineLearning-Lecture01.pdf\")\n","pages = loader.load()\n","all_page_text = [p.page_content for p in pages]\n","joined_page_text = \" \".join(all_page_text)\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n","splits = text_splitter.split_text(joined_page_text)\n","print('pages', len(pages))\n","print('splits', len(splits))\n","\n","# retrieval\n","svm_retriever = SVMRetriever.from_texts(splits, embedding)\n","tfidf_retriever = TFIDFRetriever.from_texts(splits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-x-nE7ckz9OU","executionInfo":{"status":"ok","timestamp":1690083476745,"user_tz":-480,"elapsed":6117,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"77dcc588-f5bf-49b8-f95f-cf1696ee37a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pages 22\n","splits 45\n"]}]},{"cell_type":"code","source":["question = \"What are major topics for this class?\"\n","docs_svm=svm_retriever.get_relevant_documents(question)\n","docs_svm[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wIX0aRwkxQyZ","executionInfo":{"status":"ok","timestamp":1690083486228,"user_tz":-480,"elapsed":1118,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"68e74b0a-d588-452a-d409-a41c6441acb5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(page_content=\"let me just check what questions you have righ t now. So if there are no questions, I'll just \\nclose with two reminders, which are after class today or as you start to talk with other \\npeople in this class, I just encourage you again to start to form project partners, to try to \\nfind project partners to do your project with. And also, this is a good time to start forming \\nstudy groups, so either talk to your friends  or post in the newsgroup, but we just \\nencourage you to try to star t to do both of those today, okay? Form study groups, and try \\nto find two other project partners.  \\nSo thank you. I'm looking forward to teaching this class, and I'll see you in a couple of \\ndays.   [End of Audio]  \\nDuration: 69 minutes\", metadata={})"]},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["# not recommended\n","question = \"what did they say about matlab?\"\n","docs_tfidf = tfidf_retriever.get_relevant_documents(question)\n","docs_tfidf[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YN9NPW6N02U7","executionInfo":{"status":"ok","timestamp":1690083498861,"user_tz":-480,"elapsed":3,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"cf5b3fcf-55cc-4158-8218-afdb3d28f5bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(page_content=\"Saxena and Min Sun here did, wh ich is given an image like this, right? This is actually a \\npicture taken of the Stanford campus. You can apply that sort of cl ustering algorithm and \\ngroup the picture into regions. Let me actually blow that up so that you can see it more \\nclearly. Okay. So in the middle, you see the lines sort of groupi ng the image together, \\ngrouping the image into [inaudible] regions.  \\nAnd what Ashutosh and Min did was they then  applied the learning algorithm to say can \\nwe take this clustering and us e it to build a 3D model of the world? And so using the \\nclustering, they then had a lear ning algorithm try to learn what the 3D structure of the \\nworld looks like so that they could come up with a 3D model that you can sort of fly \\nthrough, okay? Although many people used to th ink it's not possible to take a single \\nimage and build a 3D model, but using a lear ning algorithm and that sort of clustering \\nalgorithm is the first step. They were able to.  \\nI'll just show you one more example. I like this  because it's a picture of Stanford with our \\nbeautiful Stanford campus. So again, taking th e same sort of clustering algorithms, taking \\nthe same sort of unsupervised learning algor ithm, you can group the pixels into different \\nregions. And using that as a pre-processing step, they eventually built this sort of 3D model of Stanford campus in a single picture.  You can sort of walk  into the ceiling, look\", metadata={})"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["## 1.5 QnA"],"metadata":{"id":"dLguhklP1YfR"}},{"cell_type":"code","source":["persist_directory = 'chroma/'\n","embedding = OpenAIEmbeddings()\n","vectordb = Chroma(\n","    persist_directory=persist_directory,\n","    embedding_function=embedding\n",")\n","print(vectordb._collection.count())\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever()\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fEQSO4b1atw","executionInfo":{"status":"ok","timestamp":1690083686809,"user_tz":-480,"elapsed":3,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"024b18d8-4fe0-4756-b9ec-1a05c5025d70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["209\n"]}]},{"cell_type":"code","source":["question = \"What are major topics for this class?\"\n","result = qa_chain({\"query\": question})\n","print(result[\"result\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2W7ENmJb1awZ","executionInfo":{"status":"ok","timestamp":1690083717554,"user_tz":-480,"elapsed":1798,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"5b8b4bae-3a77-4c3b-956f-d29f71b01fcf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" This class will cover topics such as statistics, algebra, and machine learning.\n"]}]},{"cell_type":"code","source":["# Build prompt and chain\n","template = \"\"\"\\\n","Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.\n","{context}\n","Question: {question}\n","Helpful Answer:\n","\"\"\"\n","\n","QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n","\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm,\n","    retriever=vectordb.as_retriever(),\n","    return_source_documents=True,\n","    # chain_type=\"map_reduce\",\n","    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",")\n","\n","question = \"Is probability a class topic?\"\n","result = qa_chain({\"query\": question})\n","print(result[\"result\"])\n","print(result[\"source_documents\"][0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q3PlNTVk1ay_","executionInfo":{"status":"ok","timestamp":1690084000265,"user_tz":-480,"elapsed":1350,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"aa86f57b-84f2-405d-f435-9aee498f1666"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Yes, probability is a class topic. We'll assume familiarity with basic probability and statistics, and most undergraduate statistics classes like Stat 116 taught here at Stanford will be more than enough. Thanks for asking!\n","page_content=\"of this class will not be very program ming intensive, although we will do some \\nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \\nI also assume familiarity with basic proba bility and statistics. So most undergraduate \\nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \\nassume all of you know what ra ndom variables are, that all of you know what expectation \\nis, what a variance or a random variable is. And in case of some of you, it's been a while \\nsince you've seen some of this material. At some of the discussion sections, we'll actually \\ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \\nI'll say a bit more about that later as well.  \\nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \\nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \\n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \\ngonna assume that all of you know what matrix es and vectors are, that you know how to \\nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \\nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \\nthe review sections.\" metadata={'page': 4, 'source': 'docs/MachineLearning-Lecture01.pdf'}\n"]}]},{"cell_type":"markdown","source":["## 1.6 Chatbot"],"metadata":{"id":"8vv2hNr73sQB"}},{"cell_type":"code","source":["import param\n","import panel as pn\n","\n","# initialize database and retriever chain\n","def load_db(file, chain_type, k):\n","    loader = PyPDFLoader(file)\n","    documents = loader.load()\n","    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n","    docs = text_splitter.split_documents(documents)\n","    embeddings = OpenAIEmbeddings()\n","    db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n","    retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": k})\n","\n","    # create a chatbot chain. Memory is managed externally.\n","    qa = ConversationalRetrievalChain.from_llm(\n","        llm=ChatOpenAI(model_name=llm, temperature=0),\n","        chain_type=chain_type,\n","        retriever=retriever,\n","        return_source_documents=True,\n","        return_generated_question=True,\n","    )\n","    return qa"],"metadata":{"id":"EEIrJOl01a4L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2) LangChain LLM\n","\n","set ```langchain.debug = True``` to see more context"],"metadata":{"id":"hUHRacSRIC2b"}},{"cell_type":"code","source":["chat = ChatOpenAI(temperature=0)"],"metadata":{"id":"MJAykC94R2a6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.1 Prompt"],"metadata":{"id":"3YpjILDOIJta"}},{"cell_type":"code","source":["template_string = \"\"\"\\\n","Translate the text \\\n","that is delimited by triple backticks \\\n","into a style that is {style}. \\\n","text: ```{text}```\n","\"\"\"\n","\n","prompt_template = ChatPromptTemplate.from_template(template_string)\n","print(f'input{prompt_template.messages[0].prompt.input_variables}')\n","\n","customer_style = \"\"\"Singaporean english in an exagerate tone\"\"\"\n","\n","customer_email = \"\"\"\\\n","Arrr, I be fuming that me blender lid \\\n","flew off and splattered me kitchen walls \\\n","with smoothie! And to make matters worse, \\\n","the warranty don't cover the cost of \\\n","cleaning up me kitchen. I need yer help \\\n","right now, matey!\n","\"\"\"\n","\n","customer_messages = prompt_template.format_messages(\n","    style=customer_style, text=customer_email\n",")\n","print(type(customer_messages[0]))\n","print('final prompt: ', customer_messages[0])\n","\n","customer_response = chat(customer_messages)\n","print('response: ', customer_response.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4YLJrzuRNTOk","executionInfo":{"status":"ok","timestamp":1690005987698,"user_tz":-480,"elapsed":4086,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"ddabbf22-4a47-4005-aaf6-82eff50bba53"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["input['style', 'text']\n","<class 'langchain.schema.messages.HumanMessage'>\n","final prompt:  content=\"Translate the text that is delimited by triple backticks into a style that is Singaporean english in an exagerate tone. text: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} example=False\n","response:  Arrr, I be fumin' like a volcano lah! Me blender lid just decided to fly off like a rocket and splatter me kitchen walls with smoothie, can you believe it? Wah, siao liao! And to make matters worse, the warranty don't even cover the cost of cleaning up me kitchen, can you imagine? So jialat lah! I really need yer help right now, matey! Don't play play, hurry up and come and save me from this mess!\n"]}]},{"cell_type":"code","source":["# output parser\n","customer_review = \"\"\"\\\n","This leaf blower is pretty amazing.  It has four settings:\\\n","candle blower, gentle breeze, windy city, and tornado. \\\n","It arrived in two days, just in time for my wife's \\\n","anniversary present. \\\n","I think my wife liked it so much she was speechless. \\\n","So far I've been the only one using it, and I've been \\\n","using it every other morning to clear the leaves on our lawn. \\\n","It's slightly more expensive than the other leaf blowers \\\n","out there, but I think it's worth it for the extra features.\n","\"\"\"\n","\n","review_template = \"\"\"\\\n","For the following text, extract the following information:\n","\n","gift: Was the item purchased as a gift for someone else? \\\n","Answer True if yes, False if not or unknown.\n","\n","delivery_days: How many days did it take for the product \\\n","to arrive? If this information is not found, output -1.\n","\n","price_value: Extract any sentences about the value or price,\\\n","and output them as a comma separated Python list.\n","\n","Format the output as JSON with the following keys:\n","gift\n","delivery_days\n","price_value\n","\n","text: {text}\n","\"\"\"\n","\n","prompt_template = ChatPromptTemplate.from_template(review_template)\n","messages = prompt_template.format_messages(text=customer_review)\n","chat = ChatOpenAI(temperature=0.0)\n","response = chat(messages)\n","print(response.content)\n","print('type: ', type(response.content))\n","\n","# parse to python dictionary\n","gift_schema = ResponseSchema(\n","    name=\"gift\",\n","    description=\"\\\n","    Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\\n","    \"\n",")\n","\n","delivery_days_schema = ResponseSchema(\n","    name=\"delivery_days\",\n","    description=\"\\\n","    How many days did it take for the product o arrive? \\\n","    If this information is not found, output -1.\\\n","    \"\n",")\n","\n","price_value_schema = ResponseSchema(\n","    name=\"price_value\",\n","    description=\"Extract any sentences about the value or price, \\\n","    and output them as a comma separated Python list.\\\n","    \"\n",")\n","\n","response_schemas = [\n","    gift_schema, delivery_days_schema, price_value_schema\n","]\n","\n","output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n","format_instructions = output_parser.get_format_instructions()\n","print('\\nGuide', format_instructions)\n","output_dict = output_parser.parse(response.content)\n","print('\\nOutput Parser', output_dict)\n","print(output_dict.get('delivery_days'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nTCUejCbIU0t","executionInfo":{"status":"ok","timestamp":1690006583192,"user_tz":-480,"elapsed":2550,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"6b2cdaae-7396-41df-fb71-9a982c7b8244"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"gift\": false,\n","  \"delivery_days\": 2,\n","  \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n","}\n","type:  <class 'str'>\n","\n","Guide The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n","\n","```json\n","{\n","\t\"gift\": string  //     Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.    \n","\t\"delivery_days\": string  //     How many days did it take for the product o arrive?     If this information is not found, output -1.    \n","\t\"price_value\": string  // Extract any sentences about the value or price,     and output them as a comma separated Python list.    \n","}\n","```\n","\n","Output Parser {'gift': False, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n","2\n"]}]},{"cell_type":"markdown","source":["## 2.2 Memory"],"metadata":{"id":"Woa2wydDPq3D"}},{"cell_type":"markdown","source":["### 2.2.1 Conversation Buffer Memory"],"metadata":{"id":"q0VBWdvQS7vi"}},{"cell_type":"code","source":["chat = ChatOpenAI(temperature=0.0)\n","memory = ConversationBufferMemory()\n","conversation = ConversationChain(\n","    llm=chat,\n","    verbose=True,\n","    memory = memory\n",")"],"metadata":{"id":"Ilsg-kvoNx4Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conversation.predict(input=\"Hi, my name is Kean\")\n","conversation.predict(input=\"What is 1+1?\")\n","conversation.predict(input=\"What is my name?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":750},"id":"E39Eeu2tR_y_","executionInfo":{"status":"ok","timestamp":1690007364334,"user_tz":-480,"elapsed":2861,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"4911679f-424d-4c06-fbe8-33ac7f9c9def"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","\n","Human: Hi, my name is Kean\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Kean\n","AI: Hello Kean! It's nice to meet you. How can I assist you today?\n","Human: What is 1+1?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","Human: Hi, my name is Kean\n","AI: Hello Kean! It's nice to meet you. How can I assist you today?\n","Human: What is 1+1?\n","AI: 1+1 is equal to 2.\n","Human: What is my name?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Your name is Kean.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["memory.load_memory_variables({})\n","print(memory.buffer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OcZ18KZJSDbS","executionInfo":{"status":"ok","timestamp":1690007432493,"user_tz":-480,"elapsed":4,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9651805e-c1d5-4067-a629-7705b464085e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: Hi, my name is Kean\n","AI: Hello Kean! It's nice to meet you. How can I assist you today?\n","Human: What is 1+1?\n","AI: 1+1 is equal to 2.\n","Human: What is my name?\n","AI: Your name is Kean.\n"]}]},{"cell_type":"code","source":["# storing k conversation\n","memory = ConversationBufferWindowMemory(k=1)\n","memory.save_context({\"input\": \"Hi\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n","memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wVaItxsdSjiV","executionInfo":{"status":"ok","timestamp":1690007653364,"user_tz":-480,"elapsed":616,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"50538d9a-dfb2-4d4e-8fb1-e795db77d1f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'Human: Not much, just hanging\\nAI: Cool'}"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["### 2.2.2 Conversation Token Buffer Memory"],"metadata":{"id":"1ofJtlMyTpOl"}},{"cell_type":"code","source":["memory = ConversationTokenBufferMemory(llm=chat, max_token_limit=30)\n","memory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})\n","memory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})\n","memory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})\n","memory.load_memory_variables({})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4D0l7jgTcB2","executionInfo":{"status":"ok","timestamp":1690007857998,"user_tz":-480,"elapsed":1235,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"a7963474-d75f-4a90-cf6c-b0e04db3c5d3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### 2.2.3 Conversation Summary Memory\n","Combines the summary and buffer. If max_limit allows, it will include historical conversation."],"metadata":{"id":"K9eCJ5MxUd_F"}},{"cell_type":"code","source":["# create a long string\n","memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=100)\n","conversation = ConversationChain(\n","    llm=chat, memory = memory, verbose=True\n",")\n","\n","schedule = \"\\\n","There is a meeting at 8am with your product team. \\\n","You will need your powerpoint presentation prepared. \\\n","9am-12pm have time to work on your LangChain \\\n","project which will go quickly because Langchain is such a powerful tool. \\\n","At Noon, lunch at the italian resturant with a customer who is driving \\\n","from over an hour away to meet you to understand the latest in AI. \\\n","Be sure to bring your laptop to show the latest LLM demo.\"\n","\n","memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n","memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n","memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})\n","\n","print(memory.load_memory_variables({}))\n","conversation.predict(input=\"What would be a good demo to show?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"id":"s1zoooc7UPgI","executionInfo":{"status":"ok","timestamp":1690008975307,"user_tz":-480,"elapsed":7745,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"04857fed-341a-46bf-cc41-2d032ef2218f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'history': 'System: The human greets the AI and the AI responds with a casual greeting. The human mentions that they are not doing much and the AI responds with a casual remark.\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.'}\n","\n","\n","\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","System: The human greets the AI and the AI responds with a casual greeting. The human mentions that they are not doing much and the AI responds with a casual remark.\n","Human: What is on the schedule today?\n","AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n","Human: What would be a good demo to show?\n","AI:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"A good demo to show would be the Language Model API. It showcases the capabilities of our AI technology by generating coherent and contextually relevant text based on user input. It's a great way to demonstrate the power and potential of our language processing capabilities.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## 2.3 Chain"],"metadata":{"id":"MfUD_wAOY3s9"}},{"cell_type":"code","source":["df = pd.read_csv('https://raw.githubusercontent.com/ksm26/LangChain-for-LLM-Application-Development/main/Data.csv')\n","llm = ChatOpenAI(temperature=0.9)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"c-wz13rSWnMw","executionInfo":{"status":"ok","timestamp":1690009163511,"user_tz":-480,"elapsed":10,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"6a795f26-76fb-4a53-db1f-1f3772b2ef52"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   Product                                             Review\n","0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n","1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n","2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n","3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n","4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."],"text/html":["\n","\n","  <div id=\"df-3a7bcb20-8fec-4349-adf3-117de4693bbf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Product</th>\n","      <th>Review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Queen Size Sheet Set</td>\n","      <td>I ordered a king size set. My only criticism w...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Waterproof Phone Pouch</td>\n","      <td>I loved the waterproof sac, although the openi...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Luxury Air Mattress</td>\n","      <td>This mattress had a small hole in the top of i...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Pillows Insert</td>\n","      <td>This is the best throw pillow fillers on Amazo...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Milk Frother Handheld\\n</td>\n","      <td>I loved this product. But they only seem to l...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a7bcb20-8fec-4349-adf3-117de4693bbf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-76df6832-edf5-4995-86da-f6a6b19a2ac7\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-76df6832-edf5-4995-86da-f6a6b19a2ac7')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-76df6832-edf5-4995-86da-f6a6b19a2ac7 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3a7bcb20-8fec-4349-adf3-117de4693bbf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3a7bcb20-8fec-4349-adf3-117de4693bbf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["### 2.3.1 Sequential Chain"],"metadata":{"id":"fKQLOT-abr9n"}},{"cell_type":"code","source":["# simple chain\n","prompt = ChatPromptTemplate.from_template(\n","    \"What is the best name to describe a company that makes {product}?\"\n",")\n","chain = LLMChain(llm=llm, prompt=prompt)\n","chain.run(\"Queen Size Sheet Set\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"uhX57lBhZCEE","executionInfo":{"status":"ok","timestamp":1690009280322,"user_tz":-480,"elapsed":508,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"50fec365-4e53-447b-f919-951095c018f0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'RegalRest'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# chain 1\n","first_prompt = ChatPromptTemplate.from_template(\n","    \"What is the best name to describe a company that makes {product}?\"\n",")\n","chain_one = LLMChain(llm=llm, prompt=first_prompt)\n","\n","# chain 2\n","second_prompt = ChatPromptTemplate.from_template(\n","    \"Write a 20 words description for the following company:{company_name}\"\n",")\n","chain_two = LLMChain(llm=llm, prompt=second_prompt)\n","\n","overall_simple_chain = SimpleSequentialChain(\n","    chains=[chain_one, chain_two], verbose=True\n",")\n","overall_simple_chain.run(\"Queen Size Sheet Set\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"kvmv1lQ5Zs5p","executionInfo":{"status":"ok","timestamp":1690009453346,"user_tz":-480,"elapsed":1725,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"d43c8a9e-db3f-422c-dd9e-697464311337"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n","\u001b[36;1m\u001b[1;3mRegalRest\u001b[0m\n","\u001b[33;1m\u001b[1;3mRegalRest is a luxury mattress company providing comfortable and premium quality mattresses for a restful sleep experience.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'RegalRest is a luxury mattress company providing comfortable and premium quality mattresses for a restful sleep experience.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["# chain 1: input=Review and output=English_Review\n","first_prompt = ChatPromptTemplate.from_template(\n","    \"Translate the following review to english:\"\n","    \"\\n\\n{Review}\")\n","chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n","\n","# chain 2: input=English_Review and output=summary\n","second_prompt = ChatPromptTemplate.from_template(\n","    \"Can you summarize the following review in 1 sentence:\"\n","    \"\\n\\n{English_Review}\")\n","chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n","\n","# chain 3: input=Review and output=language\n","third_prompt = ChatPromptTemplate.from_template(\n","    \"What language is the following review:\\n\\n{Review}\")\n","chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n","\n","# chain 4: input=summary, language and output=followup_message\n","fourth_prompt = ChatPromptTemplate.from_template(\n","    \"Write a follow up response to the following \"\n","    \"summary in the specified language:\"\n","    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",")\n","chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")\n","\n","# overall_chain: input=Review\n","# and output=English_Review, summary, followup_message\n","overall_chain = SequentialChain(\n","    chains=[chain_one, chain_two, chain_three, chain_four],\n","    input_variables=[\"Review\"],\n","    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n","    verbose=True\n",")\n","overall_chain(df['Review'][5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Stt-0bKXaHO1","executionInfo":{"status":"ok","timestamp":1690009768366,"user_tz":-480,"elapsed":17436,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"2795c837-078b-486a-bff8-914815417bc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n"," 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\",\n"," 'summary': 'The reviewer is disappointed with the taste and foam quality of the product, comparing it negatively to the ones bought in stores, questioning if it is an old batch or a counterfeit item.',\n"," 'followup_message': \"Réponse de suivi :\\n\\nCher(e) responsable des ventes,\\n\\nNous vous remercions d'avoir partagé votre expérience avec notre produit. Nous sommes désolés d'apprendre que vous avez été déçu(e) par le goût et la qualité de la mousse. Nous comprenons votre déception et nous tenons à nous excuser pour toute gêne occasionnée.\\n\\nLa qualité de nos produits est d'une importance capitale pour nous, et nous prenons vos commentaires très au sérieux. Nous souhaitons vous assurer que toutes nos marchandises sont fabriquées avec soin et soumises à des contrôles de qualité stricts avant d'être distribuées. Il est possible que vous ayez reçu un produit défectueux ou qui ne correspondait pas à nos normes habituelles.\\n\\nAfin de résoudre ce problème, nous vous encourageons à nous contacter directement avec plus de détails sur l'unité que vous avez achetée, tels que la date d'expiration, le numéro de lot ou encore le lieu d'achat. Nous pourrons ainsi effectuer des vérifications poussées afin de comprendre ce qui a pu se passer et prendre les mesures appropriées pour y remédier.\\n\\nVeuillez nous excuser pour cet inconvénient et soyez assuré(e) que nous mettrons tout en œuvre pour rectifier cette situation. Votre satisfaction est notre priorité et nous espérons avoir l'opportunité de vous prouver la qualité de nos produits dans le futur.\\n\\nNous sommes à votre disposition pour toute autre question ou préoccupation que vous pourriez avoir.\\n\\nCordialement,\\nL'équipe du service client\"}"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["### 2.3.2 Router Chain"],"metadata":{"id":"NkV6lTUjbzjC"}},{"cell_type":"code","source":["physics_template = \"\"\"\\\n","You are a very smart physics professor. \\\n","You are great at answering questions about physics in a concise\\\n","and easy to understand manner. \\\n","When you don't know the answer to a question you admit\\\n","that you don't know.\n","Here is a question:\n","\n","{input}\n","\"\"\"\n","\n","math_template = \"\"\"\\\n","You are a very good mathematician. \\\n","You are great at answering math questions. \\\n","You are so good because you are able to break down \\\n","hard problems into their component parts,\n","answer the component parts, and then put them together\\\n","to answer the broader question.\n","\n","Here is a question:\n","{input}\n","\"\"\"\n","\n","history_template = \"\"\"\\\n","You are a very good historian. \\\n","You have an excellent knowledge of and understanding of people,\\\n","events and contexts from a range of historical periods. \\\n","You have the ability to think, reflect, debate, discuss and \\\n","evaluate the past. You have a respect for historical evidence\\\n","and the ability to make use of it to support your explanations \\\n","and judgements.\n","\n","Here is a question:\n","{input}\n","\"\"\"\n","\n","computerscience_template = \"\"\"\\\n","You are a successful computer scientist.\\\n","You have a passion for creativity, collaboration,\\\n","forward-thinking, confidence, strong problem-solving capabilities,\\\n","understanding of theories and algorithms, and excellent communication \\\n","skills. You are great at answering coding questions. \\\n","You are so good because you know how to solve a problem by \\\n","describing the solution in imperative steps \\\n","that a machine can easily interpret and you know how to \\\n","choose a solution that has a good balance between \\\n","time complexity and space complexity.\n","\n","Here is a question:\n","{input}\"\"\"\n","\n","prompt_infos = [\n","    {\n","        \"name\": \"physics\",\n","        \"description\": \"Good for answering questions about physics\",\n","        \"prompt_template\": physics_template\n","    },\n","    {\n","        \"name\": \"math\",\n","        \"description\": \"Good for answering math questions\",\n","        \"prompt_template\": math_template\n","    },\n","    {\n","        \"name\": \"History\",\n","        \"description\": \"Good for answering history questions\",\n","        \"prompt_template\": history_template\n","    },\n","    {\n","        \"name\": \"computer science\",\n","        \"description\": \"Good for answering computer science questions\",\n","        \"prompt_template\": computerscience_template\n","    }\n","]\n","\n","destination_chains = {}\n","for p_info in prompt_infos:\n","    name = p_info[\"name\"]\n","    prompt_template = p_info[\"prompt_template\"]\n","    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n","    chain = LLMChain(llm=llm, prompt=prompt)\n","    destination_chains[name] = chain\n","\n","# default chain is used if model can't decide which one to use\n","destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n","destinations_str = \"\\n\".join(destinations)\n","default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n","default_chain = LLMChain(llm=llm, prompt=default_prompt)\n","\n","print('destination_chains', destination_chains)\n","print('\\ndestinations_str', destinations_str)\n","print('\\ndefault_prompt', default_prompt)\n","print('\\ndefault_chain', default_chain)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mxd4ElGtbknS","executionInfo":{"status":"ok","timestamp":1690012357674,"user_tz":-480,"elapsed":721,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"f62d3667-5f93-4e56-fc83-7c129cc6ae81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["destination_chains {'physics': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template=\"You are a very smart physics professor. You are great at answering questions about physics in a conciseand easy to understand manner. When you don't know the answer to a question you admitthat you don't know.\\nHere is a question:\\n\\n{input}\\n\", template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.9, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), 'math': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, \\nanswer the component parts, and then put them togetherto answer the broader question.\\n\\nHere is a question:\\n{input}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.9, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), 'History': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='You are a very good historian. You have an excellent knowledge of and understanding of people,events and contexts from a range of historical periods. You have the ability to think, reflect, debate, discuss and evaluate the past. You have a respect for historical evidenceand the ability to make use of it to support your explanations and judgements.\\n\\nHere is a question:\\n{input}\\n', template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.9, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), 'computer science': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='You are a successful computer scientist.You have a passion for creativity, collaboration,forward-thinking, confidence, strong problem-solving capabilities,understanding of theories and algorithms, and excellent communication skills. You are great at answering coding questions. You are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity. \\n\\nHere is a question:\\n{input}', template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.9, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={})}\n","\n","destinations_str physics: Good for answering questions about physics\n","math: Good for answering math questions\n","History: Good for answering history questions\n","computer science: Good for answering computer science questions\n","\n","default_prompt input_variables=['input'] output_parser=None partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='{input}', template_format='f-string', validate_template=True), additional_kwargs={})]\n","\n","default_chain memory=None callbacks=None callback_manager=None verbose=False tags=None metadata=None prompt=ChatPromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='{input}', template_format='f-string', validate_template=True), additional_kwargs={})]) llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.9, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None) output_key='text' output_parser=NoOpOutputParser() return_final_only=True llm_kwargs={}\n"]}]},{"cell_type":"code","source":["# \"DEFAULT\" would turn into None in destination chain\n","\n","MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\\\n","Given a raw text input to a \\\n","language model select the model prompt best suited for the input. \\\n","You will be given the names of the available prompts and a \\\n","description of what the prompt is best suited for. \\\n","You may also revise the original input if you think that revising\\\n","it will ultimately lead to a better response from the language model.\n","\n","<< FORMATTING >>\n","Return a markdown code snippet with a JSON object formatted to look like:\n","```json\n","{{{{\n","    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n","    \"next_inputs\": string \\ a potentially modified version of the original input\n","}}}}\n","```\n","\n","REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n","names specified below OR it can be \"DEFAULT\" if the input is not\\\n","well suited for any of the candidate prompts.\n","REMEMBER: \"next_inputs\" can just be the original input \\\n","if you don't think any modifications are needed.\n","\n","<< CANDIDATE PROMPTS >>\n","{destinations}\n","\n","<< INPUT >>\n","{{input}}\n","\n","<< OUTPUT (remember to include the ```json)>>\"\"\"\n","\n","router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n","    destinations=destinations_str\n",")\n","router_prompt = PromptTemplate(\n","    template=router_template,\n","    input_variables=[\"input\"],\n","    output_parser=RouterOutputParser(),\n",")\n","\n","router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n","chain = MultiPromptChain(\n","    router_chain=router_chain,\n","    destination_chains=destination_chains,\n","    default_chain=default_chain,\n","    verbose=True)\n","\n","chain.run(\"What is black body radiation?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"jT31RBXGca3d","executionInfo":{"status":"ok","timestamp":1690013947810,"user_tz":-480,"elapsed":10978,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"baa52a39-382e-4bc5-d6c0-e31de75226a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n","physics: {'input': 'What is black body radiation?'}\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'Black body radiation refers to the electromagnetic radiation emitted by an object that absorbs all incident radiation. It is called \"black body\" because an idealized object that perfectly absorbs all radiation without reflecting or transmitting any appears black.\\n\\nWhen an object is heated, it emits radiation at different wavelengths or frequencies, which depend on its temperature. Black body radiation follows a specific pattern called Planck\\'s law, which describes the intensity of the radiation at different wavelengths. As the temperature increases, the peak intensity shifts towards shorter wavelengths, corresponding to more energetic photons.\\n\\nOne important aspect of black body radiation is its color. At low temperatures, the radiation is primarily in the infrared range and not visible to the human eye. As the temperature increases, the object starts to glow red, then orange, and eventually becomes white-hot as the radiation spans the visible spectrum.\\n\\nBlack body radiation has been crucial in understanding quantum mechanics and the concept of quantized energy levels. Max Planck, a physicist, introduced Planck\\'s law to explain the observed experimental data, which ultimately led to the development of quantum theory.\\n\\nIn summary, black body radiation refers to the electromagnetic radiation emitted by an object that perfectly absorbs all incident radiation. Its properties depend on the object\\'s temperature, and it follows a specific pattern described by Planck\\'s law.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["# category is None\n","chain.run(\"Why does the chicken cross the road\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"GbGdOTzNcud4","executionInfo":{"status":"ok","timestamp":1690013957622,"user_tz":-480,"elapsed":9832,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"d32a30b8-7218-43ed-dffc-02314a394d45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n","None: {'input': 'Why does the chicken cross the road'}\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["\"There are numerous reasons people have come up with to explain why the chicken crosses the road. Here are a few popular humorous explanations:\\n\\n- To get to the other side: This is the classic punchline to the joke and is often used to highlight the simplicity of the question and subvert expectations.\\n- To show it's not a chicken: This is a play on words, suggesting that the chicken is crossing the road not because it is a chicken, but to prove it is not cowardly.\\n- Because its GPS told it to: This is a modern interpretation of the joke, implying that the chicken is being guided by technology like a GPS to navigate its way to the other side.\\n- To escape a bad situation: This is a more serious interpretation, suggesting that the chicken is crossing the road to avoid danger, such as a predator or some other threat.\\n- To find food or explore: This interpretation is based on the natural behavior of chickens, as they often wander and forage for food. The chicken may be crossing the road in search of better feeding grounds.\\nOverall, the joke about the chicken crossing the road is considered a light-hearted and often nonsensical riddle, and there is no definitive answer to the question.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["## 2.4 QnA\n","\n","[Medium Blog - 4 Ways to Do Question Answering in LangChain](https://towardsdatascience.com/4-ways-of-question-answering-in-langchain-188c6707cc5a)"],"metadata":{"id":"cGgtV5gDr2OE"}},{"cell_type":"code","source":["langchain.debug = False"],"metadata":{"id":"bCJmjQZlSNIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceBgeEmbeddings\n","embeddings = HuggingFaceBgeEmbeddings(\n","    model_name=\"BAAI/bge-base-en\",\n","    model_kwargs={'device': 'cuda'},\n","    encode_kwargs={'normalize_embeddings': True}\n",")\n","file = 'OutdoorClothingCatalog_1000.csv'\n","llm = ChatOpenAI(temperature=0.0)\n","loader = CSVLoader(file_path=file)\n","docs = loader.load()\n","index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch).from_loaders([loader])\n","# embeddings = OpenAIEmbeddings()\n","db = DocArrayInMemorySearch.from_documents(docs, embeddings)\n","pd.read_csv(file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"XttiZ-SysMdt","executionInfo":{"status":"ok","timestamp":1692005313300,"user_tz":-480,"elapsed":18452,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"7aec8e88-1e47-409d-c0dc-067986ead47f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Unnamed: 0                                               name  \\\n","0             0                           Women's Campside Oxfords   \n","1             1           Recycled Waterhog Dog Mat, Chevron Weave   \n","2             2  Infant and Toddler Girls' Coastal Chill Swimsu...   \n","3             3         Refresh Swimwear, V-Neck Tankini Contrasts   \n","4             4                             EcoFlex 3L Storm Pants   \n","..          ...                                                ...   \n","995         995                 Men's  Classic Denim, Standard Fit   \n","996         996                  CozyPrint Sweater Fleece Pullover   \n","997         997         Women's NRS Endurance Spray Paddling Pants   \n","998         998                          Women's Stop Flies Hoodie   \n","999         999                                 Modern Utility Bag   \n","\n","                                           description  \n","0    This ultracomfortable lace-to-toe Oxford boast...  \n","1    Protect your floors from spills and splashing ...  \n","2    She'll love the bright colors, ruffles and exc...  \n","3    Whether you're going for a swim or heading out...  \n","4    Our new TEK O2 technology makes our four-seaso...  \n","..                                                 ...  \n","995  Crafted from premium denim that will last wash...  \n","996  The ultimate sweater fleece - made from superi...  \n","997  These comfortable and affordable splash paddli...  \n","998  This great-looking hoodie uses No Fly Zone Tec...  \n","999  This US-made crossbody bag is built with the s...  \n","\n","[1000 rows x 3 columns]"],"text/html":["\n","\n","  <div id=\"df-714fb1ec-3dd5-49bf-b462-5d786a887fbc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>name</th>\n","      <th>description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Women's Campside Oxfords</td>\n","      <td>This ultracomfortable lace-to-toe Oxford boast...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Recycled Waterhog Dog Mat, Chevron Weave</td>\n","      <td>Protect your floors from spills and splashing ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Infant and Toddler Girls' Coastal Chill Swimsu...</td>\n","      <td>She'll love the bright colors, ruffles and exc...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Refresh Swimwear, V-Neck Tankini Contrasts</td>\n","      <td>Whether you're going for a swim or heading out...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>EcoFlex 3L Storm Pants</td>\n","      <td>Our new TEK O2 technology makes our four-seaso...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>995</td>\n","      <td>Men's  Classic Denim, Standard Fit</td>\n","      <td>Crafted from premium denim that will last wash...</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>996</td>\n","      <td>CozyPrint Sweater Fleece Pullover</td>\n","      <td>The ultimate sweater fleece - made from superi...</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>997</td>\n","      <td>Women's NRS Endurance Spray Paddling Pants</td>\n","      <td>These comfortable and affordable splash paddli...</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>998</td>\n","      <td>Women's Stop Flies Hoodie</td>\n","      <td>This great-looking hoodie uses No Fly Zone Tec...</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>999</td>\n","      <td>Modern Utility Bag</td>\n","      <td>This US-made crossbody bag is built with the s...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-714fb1ec-3dd5-49bf-b462-5d786a887fbc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-e41c1140-53bd-4326-8cac-4ac09200cb09\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e41c1140-53bd-4326-8cac-4ac09200cb09')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-e41c1140-53bd-4326-8cac-4ac09200cb09 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-714fb1ec-3dd5-49bf-b462-5d786a887fbc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-714fb1ec-3dd5-49bf-b462-5d786a887fbc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# convert documents and query into embedding, find the most similar document\n","# fit into llm to output\n","query =\"\\\n","Please list all your shirts with sun protection in a table in markdown and summarize each one.\\\n","\"\n","response = index.query(llm=OpenAI(), question=query, verbose=True, )\n","display(Markdown(response))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UEnJuuVAWhxn","executionInfo":{"status":"ok","timestamp":1692005410378,"user_tz":-480,"elapsed":6460,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"f695ff3a-0d4d-4f6a-9551-741b54a65796"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n","\u001b[0m{\n","  \"query\": \"Please list all your shirts with sun protection in a table in markdown and summarize each one.\"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n","\u001b[0m[inputs]\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n","\u001b[0m{\n","  \"question\": \"Please list all your shirts with sun protection in a table in markdown and summarize each one.\",\n","  \"context\": \": 618\\nname: Men's Tropical Plaid Short-Sleeve Shirt\\ndescription: Our lightest hot-weather shirt is rated UPF 50+ for superior protection from the sun's UV rays. With a traditional fit that is relaxed through the chest, sleeve, and waist, this fabric is made of 100% polyester and is wrinkle-resistant. With front and back cape venting that lets in cool breezes and two front bellows pockets, this shirt is imported and provides the highest rated sun protection possible. \\n\\nSun Protection That Won't Wear Off. Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.\\n\\n: 374\\nname: Men's Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun's harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\\n\\n: 535\\nname: Men's TropicVibe Shirt, Short-Sleeve\\ndescription: This Men’s sun-protection shirt with built-in UPF 50+ has the lightweight feel you want and the coverage you need when the air is hot and the UV rays are strong. Size & Fit: Traditional Fit: Relaxed through the chest, sleeve and waist. Fabric & Care: Shell: 71% Nylon, 29% Polyester. Lining: 100% Polyester knit mesh. UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry. Additional Features: Wrinkle resistant. Front and back cape venting lets in cool breezes. Two front bellows pockets. Imported.\\n\\nSun Protection That Won't Wear Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.\\n\\n: 255\\nname: Sun Shield Shirt by\\ndescription: \\\"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won't Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.\"\n","}\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:OpenAI] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n: 618\\nname: Men's Tropical Plaid Short-Sleeve Shirt\\ndescription: Our lightest hot-weather shirt is rated UPF 50+ for superior protection from the sun's UV rays. With a traditional fit that is relaxed through the chest, sleeve, and waist, this fabric is made of 100% polyester and is wrinkle-resistant. With front and back cape venting that lets in cool breezes and two front bellows pockets, this shirt is imported and provides the highest rated sun protection possible. \\n\\nSun Protection That Won't Wear Off. Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.\\n\\n: 374\\nname: Men's Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun's harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\\n\\n: 535\\nname: Men's TropicVibe Shirt, Short-Sleeve\\ndescription: This Men’s sun-protection shirt with built-in UPF 50+ has the lightweight feel you want and the coverage you need when the air is hot and the UV rays are strong. Size & Fit: Traditional Fit: Relaxed through the chest, sleeve and waist. Fabric & Care: Shell: 71% Nylon, 29% Polyester. Lining: 100% Polyester knit mesh. UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry. Additional Features: Wrinkle resistant. Front and back cape venting lets in cool breezes. Two front bellows pockets. Imported.\\n\\nSun Protection That Won't Wear Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.\\n\\n: 255\\nname: Sun Shield Shirt by\\ndescription: \\\"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won't Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.\\n\\nQuestion: Please list all your shirts with sun protection in a table in markdown and summarize each one.\\nHelpful Answer:\"\n","  ]\n","}\n","\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:OpenAI] [5.80s] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"\\n\\n| Name | Description |\\n| --- | --- |\\n| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection; 100% polyester fabric; front and back cape venting; two front bellows pockets. |\\n| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ rated sun protection; 52% polyester and 48% nylon fabric; SunSmart technology blocks 98% of the sun's harmful UV rays; front and back cape venting; two front bellows pockets. |\\n| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ rated sun protection; 71% nylon and 29% polyester shell; 100% polyester knit mesh lining; wrinkle resistant; front and back cape venting; two front bellows pockets. |\\n| Sun Shield Shirt by | UPF 50+ rated sun protection; 78% nylon and 22% Lycra Xtra Life fiber; wicks moisture for quick-drying comfort; abrasion resistant; fits comfortably over your favorite swimsuit. |\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\",\n","          \"logprobs\": null\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"completion_tokens\": 227,\n","      \"prompt_tokens\": 745,\n","      \"total_tokens\": 972\n","    },\n","    \"model_name\": \"text-davinci-003\"\n","  },\n","  \"run\": null\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [5.81s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"text\": \"\\n\\n| Name | Description |\\n| --- | --- |\\n| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection; 100% polyester fabric; front and back cape venting; two front bellows pockets. |\\n| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ rated sun protection; 52% polyester and 48% nylon fabric; SunSmart technology blocks 98% of the sun's harmful UV rays; front and back cape venting; two front bellows pockets. |\\n| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ rated sun protection; 71% nylon and 29% polyester shell; 100% polyester knit mesh lining; wrinkle resistant; front and back cape venting; two front bellows pockets. |\\n| Sun Shield Shirt by | UPF 50+ rated sun protection; 78% nylon and 22% Lycra Xtra Life fiber; wicks moisture for quick-drying comfort; abrasion resistant; fits comfortably over your favorite swimsuit. |\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [5.81s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output_text\": \"\\n\\n| Name | Description |\\n| --- | --- |\\n| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection; 100% polyester fabric; front and back cape venting; two front bellows pockets. |\\n| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ rated sun protection; 52% polyester and 48% nylon fabric; SunSmart technology blocks 98% of the sun's harmful UV rays; front and back cape venting; two front bellows pockets. |\\n| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ rated sun protection; 71% nylon and 29% polyester shell; 100% polyester knit mesh lining; wrinkle resistant; front and back cape venting; two front bellows pockets. |\\n| Sun Shield Shirt by | UPF 50+ rated sun protection; 78% nylon and 22% Lycra Xtra Life fiber; wicks moisture for quick-drying comfort; abrasion resistant; fits comfortably over your favorite swimsuit. |\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [6.09s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"result\": \"\\n\\n| Name | Description |\\n| --- | --- |\\n| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection; 100% polyester fabric; front and back cape venting; two front bellows pockets. |\\n| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ rated sun protection; 52% polyester and 48% nylon fabric; SunSmart technology blocks 98% of the sun's harmful UV rays; front and back cape venting; two front bellows pockets. |\\n| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ rated sun protection; 71% nylon and 29% polyester shell; 100% polyester knit mesh lining; wrinkle resistant; front and back cape venting; two front bellows pockets. |\\n| Sun Shield Shirt by | UPF 50+ rated sun protection; 78% nylon and 22% Lycra Xtra Life fiber; wicks moisture for quick-drying comfort; abrasion resistant; fits comfortably over your favorite swimsuit. |\"\n","}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"\n\n| Name | Description |\n| --- | --- |\n| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ rated sun protection; 100% polyester fabric; front and back cape venting; two front bellows pockets. |\n| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ rated sun protection; 52% polyester and 48% nylon fabric; SunSmart technology blocks 98% of the sun's harmful UV rays; front and back cape venting; two front bellows pockets. |\n| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ rated sun protection; 71% nylon and 29% polyester shell; 100% polyester knit mesh lining; wrinkle resistant; front and back cape venting; two front bellows pockets. |\n| Sun Shield Shirt by | UPF 50+ rated sun protection; 78% nylon and 22% Lycra Xtra Life fiber; wicks moisture for quick-drying comfort; abrasion resistant; fits comfortably over your favorite swimsuit. |"},"metadata":{}}]},{"cell_type":"code","source":["query = \"Please suggest a shirt with sunblocking\"\n","docs = db.similarity_search(query, k=4)\n","print('result: ', docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JaNf7xOlXyS2","executionInfo":{"status":"ok","timestamp":1692005283523,"user_tz":-480,"elapsed":591,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"35237fd4-3c88-4214-f1f4-7e928fc52b01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["result:  [Document(page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.', metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 255}), Document(page_content=\": 374\\nname: Men's Plaid Tropic Shirt, Short-Sleeve\\ndescription: Our Ultracomfortable sun protection is rated to UPF 50+, helping you stay cool and dry. Originally designed for fishing, this lightest hot-weather shirt offers UPF 50+ coverage and is great for extended travel. SunSmart technology blocks 98% of the sun's harmful UV rays, while the high-performance fabric is wrinkle-free and quickly evaporates perspiration. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets and an imported design. With UPF 50+ coverage, you can limit sun exposure and feel secure with the highest rated sun protection available.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 374}), Document(page_content=\": 535\\nname: Men's TropicVibe Shirt, Short-Sleeve\\ndescription: This Men’s sun-protection shirt with built-in UPF 50+ has the lightweight feel you want and the coverage you need when the air is hot and the UV rays are strong. Size & Fit: Traditional Fit: Relaxed through the chest, sleeve and waist. Fabric & Care: Shell: 71% Nylon, 29% Polyester. Lining: 100% Polyester knit mesh. UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry. Additional Features: Wrinkle resistant. Front and back cape venting lets in cool breezes. Two front bellows pockets. Imported.\\n\\nSun Protection That Won't Wear Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 535}), Document(page_content=\": 618\\nname: Men's Tropical Plaid Short-Sleeve Shirt\\ndescription: Our lightest hot-weather shirt is rated UPF 50+ for superior protection from the sun's UV rays. With a traditional fit that is relaxed through the chest, sleeve, and waist, this fabric is made of 100% polyester and is wrinkle-resistant. With front and back cape venting that lets in cool breezes and two front bellows pockets, this shirt is imported and provides the highest rated sun protection possible. \\n\\nSun Protection That Won't Wear Off. Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 618})]\n"]}]},{"cell_type":"code","source":["embed = embeddings.embed_query(\"Hi my name is Harrison\")\n","print(len(embed))\n","print(embed[:5])\n","\n","query = \"Please suggest a shirt with sunblocking\"\n","docs = db.similarity_search(query)\n","print('result: ', docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zDsxWjMtKUR","executionInfo":{"status":"ok","timestamp":1692005313301,"user_tz":-480,"elapsed":7,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"2e696147-f7d4-4377-80be-a704af7f9eef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["768\n","[-0.02024184912443161, 0.036355745047330856, -0.024344725534319878, -0.010754036717116833, 0.046086233109235764]\n","result:  [Document(page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.', metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 255}), Document(page_content=\": 679\\nname: Women's Tropical Tee, Sleeveless\\ndescription: Our five-star sleeveless button-up shirt has a fit to flatter and SunSmart™ protection to block the sun’s harmful UV rays. Size & Fit: Slightly Fitted: Softly shapes the body. Falls at hip. Fabric & Care: Shell: 71% nylon, 29% polyester. Cape lining: 100% polyester. Built-in SunSmart™ UPF 50+ rated – the highest rated sun protection possible. Machine wash and dry. Additional Features: Updated design with smoother buttons. Wrinkle resistant. Low-profile pockets and side shaping offer a more flattering fit. Front and back cape venting. Two front pockets, tool tabs and eyewear loop. Imported. Sun Protection That Won't Wear Off: Our high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun's harmful rays.\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 679}), Document(page_content=\": 709\\nname: Sunrise Tee\\ndescription: Stay cool, comfortable and dry on the hottest days in our women's UV-protective button down shirt. The lightweight, high-performance fabric wicks away moisture and dries quickly.\\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip.\\n\\nWhy We Love It\\nOur lightest hot-weather shirt lets you beat the heat. Originally designed for fishing, it's also a great choice for travel thanks to its wrinkle-free fabric and built-in sun protection with a rating of UPF 50+.\\n\\nFabric & Care\\nLightweight performance synthetic wicks moisture, resists wrinkles and dries fast. Shell: 71% nylon, 29% polyester. Cape lining: 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nBuilt-in SunSmart™ UPF 50+ rated – the highest rated sun protection possible. The high-performance fabric keeps you cool and comfortable by wicking perspiration away. Smoother buttons, low-profile pockets and side shaping for a flattering fit. Front and back cape venting. Two front pockets, tool tabs and eyewear loop. Wrinkle free. Imported.\\n\\nSun Protection That Won't Wear Off\\nOur\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 709}), Document(page_content=\": 619\\nname: Tropical Breeze Shirt\\ndescription: Beat the heat in this lightweight, breathable long-sleeve men’s UPF shirt, offering superior SunSmart™ protection from the sun’s harmful rays. The wrinkle-resistant and moisture-wicking fabric keeps you cool and comfortable.\\n\\nSize & Fit\\n\\nTraditional Fit: Relaxed through the chest, sleeve and waist.\\n\\nWhy We Love It\\n\\nWhen you spend a lot of time outdoors, limiting sun exposure is important. Originally designed for fishing, our lightest hot-weather shirt offers UPF 50+ coverage and is also a great choice for extended travel. Innovative SunSmart technology blocks 98% of the sun's harmful UV rays. The high-performance fabric is wrinkle free, dries quickly and keeps you cool by wicking perspiration away from your skin.\\n\\nFabric & Care\\n\\nShell: 71% nylon, 29% polyester.\\nCape lining: 100% polyester.\\nPolyester-mesh inserts.\\nUPF 50+ rated – the highest rated sun protection possible.\\nMachine wash and dry.\\n\\nAdditional Features\\n\\nWrinkle-resistant fabric. \\nFront and back cape venting lets in cool breezes.\\nTwo front bell\", metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 619})]\n"]}]},{"cell_type":"code","source":["# chain_type\n","# 1. stuff: feed all documents\n","# 2. map_reduce: feed different chunk and combine answer at the end\n","# 3. refine: fit different chumk + info from previous chunk\n","# 4. map_rerank: same as map_reduce but get score for each output\n","\n","retriever = db.as_retriever()\n","llm = ChatOpenAI(temperature=0.0)\n","\n","qa_stuff = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"map_rerank\",\n","    retriever=retriever,\n","    # verbose=True\n",")\n","\n","query = \"Please list all your shirts with summary of each of them in a table in markdown\"\n","# query = 'Recommend me the best shirt for winter'\n","response = qa_stuff.run(query)\n","display(Markdown(response))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"id":"OUDTMaKVxJEU","executionInfo":{"status":"ok","timestamp":1690035373478,"user_tz":-480,"elapsed":4668,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"8977debc-431c-463d-c2d9-6bde9b54b892"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:303: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"This document does not answer the question"},"metadata":{}}]},{"cell_type":"markdown","source":["## 2.5 Evaluation"],"metadata":{"id":"ONWQQR0XSYOl"}},{"cell_type":"code","source":["file = 'OutdoorClothingCatalog_1000.csv'\n","loader = CSVLoader(file_path=file)\n","data = loader.load()\n","index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch).from_loaders([loader])\n","llm = ChatOpenAI(temperature = 0.0)\n","\n","qa = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=index.vectorstore.as_retriever(),\n","    verbose=True,\n","    chain_type_kwargs = {\"document_separator\": \"<<<<>>>>>\"}\n",")"],"metadata":{"id":"cbagrMpgQTJP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["examples = [\n","    {\n","        \"query\": \"Do the Cozy Comfort Pullover Set have side pockets?\",\n","        \"answer\": \"Yes\"\n","    },\n","    {\n","        \"query\": \"What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\",\n","        \"answer\": \"The DownTek collection\"\n","    }\n","]\n","\n","example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(), verbose=True)\n","new_examples = example_gen_chain.apply_and_parse([{\"doc\": t} for t in data[:5]])\n","print('new_examples', new_examples)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VY4IFZdVSoCd","executionInfo":{"status":"ok","timestamp":1690035422699,"user_tz":-480,"elapsed":12307,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"fb7fce3c-9ee6-4de4-c8b2-11c8aedf8bbd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:303: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new QAGenerateChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mYou are a teacher coming up with questions to ask on a quiz. \n","Given the following document, please generate a question and answer based on that document.\n","\n","Example Format:\n","<Begin Document>\n","...\n","<End Document>\n","QUESTION: question here\n","ANSWER: answer here\n","\n","These questions should be detailed and be based explicitly on information in the document. Begin!\n","\n","<Begin Document>\n","page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\" metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0}\n","<End Document>\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mYou are a teacher coming up with questions to ask on a quiz. \n","Given the following document, please generate a question and answer based on that document.\n","\n","Example Format:\n","<Begin Document>\n","...\n","<End Document>\n","QUESTION: question here\n","ANSWER: answer here\n","\n","These questions should be detailed and be based explicitly on information in the document. Begin!\n","\n","<Begin Document>\n","page_content=': 1\\nname: Recycled Waterhog Dog Mat, Chevron Weave\\ndescription: Protect your floors from spills and splashing with our ultradurable recycled Waterhog dog mat made right here in the USA. \\n\\nSpecs\\nSmall - Dimensions: 18\" x 28\". \\nMedium - Dimensions: 22.5\" x 34.5\".\\n\\nWhy We Love It\\nMother nature, wet shoes and muddy paws have met their match with our Recycled Waterhog mats. Ruggedly constructed from recycled plastic materials, these ultratough mats help keep dirt and water off your floors and plastic out of landfills, trails and oceans. Now, that\\'s a win-win for everyone.\\n\\nFabric & Care\\nVacuum or hose clean.\\n\\nConstruction\\n24 oz. polyester fabric made from 94% recycled materials.\\nRubber backing.\\n\\nAdditional Features\\nFeatures an -exclusive design.\\nFeatures thick and thin fibers for scraping dirt and absorbing water.\\nDries quickly and resists fading, rotting, mildew and shedding.\\nUse indoors or out.\\nMade in the USA.\\n\\nHave questions? Reach out to our customer service team with any questions you may have.' metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 1}\n","<End Document>\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mYou are a teacher coming up with questions to ask on a quiz. \n","Given the following document, please generate a question and answer based on that document.\n","\n","Example Format:\n","<Begin Document>\n","...\n","<End Document>\n","QUESTION: question here\n","ANSWER: answer here\n","\n","These questions should be detailed and be based explicitly on information in the document. Begin!\n","\n","<Begin Document>\n","page_content=\": 2\\nname: Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece\\ndescription: She'll love the bright colors, ruffles and exclusive whimsical prints of this toddler's two-piece swimsuit! Our four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. Machine wash and line dry for best results. Imported.\" metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 2}\n","<End Document>\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mYou are a teacher coming up with questions to ask on a quiz. \n","Given the following document, please generate a question and answer based on that document.\n","\n","Example Format:\n","<Begin Document>\n","...\n","<End Document>\n","QUESTION: question here\n","ANSWER: answer here\n","\n","These questions should be detailed and be based explicitly on information in the document. Begin!\n","\n","<Begin Document>\n","page_content=\": 3\\nname: Refresh Swimwear, V-Neck Tankini Contrasts\\ndescription: Whether you're going for a swim or heading out on an SUP, this watersport-ready tankini top is designed to move with you and stay comfortable. All while looking great in an eye-catching colorblock style. \\n\\nSize & Fit\\nFitted: Sits close to the body.\\n\\nWhy We Love It\\nNot only does this swimtop feel good to wear, its fabric is good for the earth too. In recycled nylon, with Lycra® spandex for the perfect amount of stretch. \\n\\nFabric & Care\\nThe premium Italian-blend is breathable, quick drying and abrasion resistant. \\nBody in 82% recycled nylon with 18% Lycra® spandex. \\nLined in 90% recycled nylon with 10% Lycra® spandex. \\nUPF 50+ rated – the highest rated sun protection possible. \\nHandwash, line dry.\\n\\nAdditional Features\\nLightweight racerback straps are easy to get on and off, and won't get in your way. \\nFlattering V-neck silhouette. \\nImported.\\n\\nSun Protection That Won't Wear Off\\nOur high-performance fabric provides SPF\" metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 3}\n","<End Document>\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mYou are a teacher coming up with questions to ask on a quiz. \n","Given the following document, please generate a question and answer based on that document.\n","\n","Example Format:\n","<Begin Document>\n","...\n","<End Document>\n","QUESTION: question here\n","ANSWER: answer here\n","\n","These questions should be detailed and be based explicitly on information in the document. Begin!\n","\n","<Begin Document>\n","page_content=\": 4\\nname: EcoFlex 3L Storm Pants\\ndescription: Our new TEK O2 technology makes our four-season waterproof pants even more breathable. It's guaranteed to keep you dry and comfortable – whatever the activity and whatever the weather. Size & Fit: Slightly Fitted through hip and thigh. \\n\\nWhy We Love It: Our state-of-the-art TEK O2 technology offers the most breathability we've ever tested. Great as ski pants, they're ideal for a variety of outdoor activities year-round. Plus, they're loaded with features outdoor enthusiasts appreciate, including weather-blocking gaiters and handy side zips. Air In. Water Out. See how our air-permeable TEK O2 technology keeps you dry and comfortable. \\n\\nFabric & Care: 100% nylon, exclusive of trim. Machine wash and dry. \\n\\nAdditional Features: Three-layer shell delivers waterproof protection. Brand new TEK O2 technology provides enhanced breathability. Interior gaiters keep out rain and snow. Full side zips for easy on/off over boots. Two zippered hand pockets. Thigh pocket. Imported.\\n\\n – Official Supplier to the U.S. Ski Team\\nTHEIR WILL\" metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 4}\n","<End Document>\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","new_examples [{'query': \"What is the weight of the Women's Campside Oxfords?\", 'answer': \"The weight of the Women's Campside Oxfords is approximately 1 lb. 1 oz. per pair.\"}, {'query': 'What are the dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave?', 'answer': 'The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\".'}, {'query': \"What are some features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece?\", 'answer': \"The features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece include bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for a secure fit and maximum coverage, and it can be machine washed and line dried for best results.\"}, {'query': 'What is the fabric composition of the Refresh Swimwear, V-Neck Tankini Contrasts?', 'answer': 'The body of the tankini top is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.'}, {'query': 'What is the fabric composition of the EcoFlex 3L Storm Pants?', 'answer': 'The fabric composition of the EcoFlex 3L Storm Pants is 100% nylon, exclusive of trim.'}]\n"]}]},{"cell_type":"code","source":["examples += new_examples\n","query = examples[1][\"query\"]\n","print('query: ', query)\n","qa.run(query)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"IIKO-7HS4fPU","executionInfo":{"status":"ok","timestamp":1690035437482,"user_tz":-480,"elapsed":1911,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"b73fbb00-ad70-4fa1-effe-ecb2b289df4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["query:  What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\n","\n","\n","\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["'The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# llm to check evaluation\n","qa = RetrievalQA.from_chain_type(\n","    llm=llm,\n","    chain_type=\"stuff\",\n","    retriever=index.vectorstore.as_retriever(),\n","    verbose=False,\n","    chain_type_kwargs = {\"document_separator\": \"<<<<>>>>>\"}\n",")\n","\n","predictions = qa.apply(examples)\n","eval_chain = QAEvalChain.from_llm(llm, verbose=False)\n","graded_outputs = eval_chain.evaluate(examples, predictions)\n","\n","for i, eg in enumerate(examples):\n","    print(f\"Example {i}:\")\n","    print(\"Question: \" + predictions[i]['query'])\n","    print(\"Real Answer: \" + predictions[i]['answer'])\n","    print(\"Predicted Answer: \" + predictions[i]['result'])\n","    print(\"Predicted Grade: \" + graded_outputs[i]['results'])\n","    print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tnD3gYEB5LzT","executionInfo":{"status":"ok","timestamp":1690035683071,"user_tz":-480,"elapsed":19486,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"fb660f16-71cb-4119-a0d6-8bcdba80eb7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Example 0:\n","Question: Do the Cozy Comfort Pullover Set have side pockets?\n","Real Answer: Yes\n","Predicted Answer: Yes, the Cozy Comfort Pullover Set does have side pockets.\n","Predicted Grade: CORRECT\n","\n","Example 1:\n","Question: What collection is the Ultra-Lofty 850 Stretch Down Hooded Jacket from?\n","Real Answer: The DownTek collection\n","Predicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.\n","Predicted Grade: CORRECT\n","\n","Example 2:\n","Question: What is the weight of the Women's Campside Oxfords?\n","Real Answer: The weight of the Women's Campside Oxfords is approximately 1 lb. 1 oz. per pair.\n","Predicted Answer: The weight of the Women's Campside Oxfords is approximately 1 lb. 1 oz. per pair.\n","Predicted Grade: CORRECT\n","\n","Example 3:\n","Question: What are the dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave?\n","Real Answer: The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\".\n","Predicted Answer: The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\".\n","Predicted Grade: CORRECT\n","\n","Example 4:\n","Question: What are some features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece?\n","Real Answer: The features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece include bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom for a secure fit and maximum coverage, and it can be machine washed and line dried for best results.\n","Predicted Answer: Some features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece are:\n","- Bright colors and ruffles\n","- Exclusive whimsical prints\n","- Four-way-stretch and chlorine-resistant fabric\n","- UPF 50+ rated fabric for sun protection\n","- Crossover no-slip straps\n","- Fully lined bottom for secure fit and maximum coverage\n","- Machine washable and line dry for best results\n","- Imported\n","Predicted Grade: CORRECT\n","\n","Example 5:\n","Question: What is the fabric composition of the Refresh Swimwear, V-Neck Tankini Contrasts?\n","Real Answer: The body of the tankini top is made of 82% recycled nylon and 18% Lycra® spandex, while the lining is made of 90% recycled nylon and 10% Lycra® spandex.\n","Predicted Answer: The fabric composition of the Refresh Swimwear, V-Neck Tankini Contrasts is 82% recycled nylon with 18% Lycra® spandex for the body, and 90% recycled nylon with 10% Lycra® spandex for the lining.\n","Predicted Grade: CORRECT\n","\n","Example 6:\n","Question: What is the fabric composition of the EcoFlex 3L Storm Pants?\n","Real Answer: The fabric composition of the EcoFlex 3L Storm Pants is 100% nylon, exclusive of trim.\n","Predicted Answer: The fabric composition of the EcoFlex 3L Storm Pants is 100% nylon, exclusive of trim.\n","Predicted Grade: CORRECT\n","\n"]}]},{"cell_type":"markdown","source":["## 2.6 Agent"],"metadata":{"id":"Cn_1NP8cDx6y"}},{"cell_type":"markdown","source":["### 2.6.1 External Agent"],"metadata":{"id":"GnoJttItG4Kp"}},{"cell_type":"code","source":["llm = ChatOpenAI(temperature=0)\n","tools = load_tools([\"llm-math\",\"wikipedia\", \"serpapi\"], llm=llm)\n","agent= initialize_agent(\n","    tools,\n","    llm,\n","    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n","    handle_parsing_errors=True,\n","    verbose=True\n",")"],"metadata":{"id":"noIVpkEm-KCg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent(\"What is the 25% of 300?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rEUL4_kgEKX-","executionInfo":{"status":"ok","timestamp":1690037546526,"user_tz":-480,"elapsed":5612,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"8e05d07b-33b7-4cb7-f849-cedccae27ecf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mI can use the calculator tool to find the answer to this math question.\n","\n","Thought: I will use the calculator tool to calculate 25% of 300.\n","Action:\n","```\n","{\n","  \"action\": \"Calculator\",\n","  \"action_input\": \"25% of 300\"\n","}\n","```\n","\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mThe answer is 75.0.\n","Final Answer: 75.0\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"output_type":"execute_result","data":{"text/plain":["{'input': 'What is the 25% of 300?', 'output': '75.0'}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["question = \"\\\n","Tom M. Mitchell is an American computer scientist \\\n","and the Founders University Professor at Carnegie Mellon University (CMU)\\\n","what book did he write?\\\n","\"\n","agent(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jE6fyzC9EK4U","executionInfo":{"status":"ok","timestamp":1690037582878,"user_tz":-480,"elapsed":5535,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"8cc05f2c-c48e-48f4-bfe7-93af6c34e4bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mThought: I can use Wikipedia to find out what book Tom M. Mitchell wrote.\n","Action:\n","```\n","{\n","  \"action\": \"Wikipedia\",\n","  \"action_input\": \"Tom M. Mitchell\"\n","}\n","```\u001b[0m"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"output_type":"stream","name":"stdout","text":["\n","Observation: \u001b[33;1m\u001b[1;3mPage: Tom M. Mitchell\n","Summary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n","\n","Page: Tom Mitchell (Australian footballer)\n","Summary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season.\n","\n","\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mThe book that Tom M. Mitchell wrote is \"Machine Learning\".\n","Final Answer: Machine Learning\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}]},{"cell_type":"code","source":["langchain.debug = False"],"metadata":{"id":"COpBOflFGGeM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["question = \"Who is the oscar award winner in 2023\"\n","result = agent(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-Jty83CFt9A","executionInfo":{"status":"ok","timestamp":1690037783317,"user_tz":-480,"elapsed":27832,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"cd4a3452-fdeb-4540-fae9-6631550d4520"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mThought: I need to find out who won the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winner 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\n","\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI need to find the specific category and winner for the Oscar award in 2023.\n","Action:\n","```\n","{\n","  \"action\": \"Search\",\n","  \"action_input\": \"Oscar award winners 2023\"\n","}\n","```\u001b[0m\n","Observation: \u001b[38;5;200m\u001b[1;3mThe 95th Academy Awards ceremony, presented by the Academy of Motion Picture Arts and Sciences, took place on March 12, 2023, at the Dolby Theatre in Hollywood, Los Angeles. During the gala, the AMPAS presented Academy Awards in 23 categories honoring films released in 2022.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n","Final Answer: The specific category and winner for the Oscar award in 2023 is not provided.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### 2.6.2 Python Agent"],"metadata":{"id":"9U_xoeM2G77o"}},{"cell_type":"code","source":["langchain.debug = True\n","agent = create_python_agent(\n","    llm, tool=PythonREPLTool(), verbose=True\n",")\n","\n","customer_list = [\n","    [\"Harrison\", \"Chase\"],\n","    [\"Lang\", \"Chain\"],\n","    [\"Dolly\", \"Too\"],\n","    [\"Elle\", \"Elem\"],\n","    [\"Geoff\",\"Fusion\"],\n","    [\"Trance\",\"Former\"],\n","    [\"Jen\",\"Ayai\"]\n","]\n","\n","agent.run(f\"\"\"\\\n","Sort these customers by last name and then first name \\\n","and print the output: {customer_list} \\\n","\"\"\")\n","langchain.debug = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Ku8WpOxZG6qy","executionInfo":{"status":"ok","timestamp":1690037986867,"user_tz":-480,"elapsed":8384,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"b6ce0067-6418-4ab6-d1a8-52f83c8df117"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n","\u001b[0m{\n","  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']] \"\n","}\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n","\u001b[0m{\n","  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']] \",\n","  \"agent_scratchpad\": \"\",\n","  \"stop\": [\n","    \"\\nObservation:\",\n","    \"\\n\\tObservation:\"\n","  ]\n","}\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']] \\nThought:\"\n","  ]\n","}\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:langchain.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [4.75s] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"To sort the customers by last name and then first name, I can use the `sorted()` function and provide a custom key function that returns a tuple of the last name and first name. This will ensure that the sorting is done in the desired order.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\"\n","        },\n","        \"message\": {\n","          \"lc\": 1,\n","          \"type\": \"constructor\",\n","          \"id\": [\n","            \"langchain\",\n","            \"schema\",\n","            \"messages\",\n","            \"AIMessage\"\n","          ],\n","          \"kwargs\": {\n","            \"content\": \"To sort the customers by last name and then first name, I can use the `sorted()` function and provide a custom key function that returns a tuple of the last name and first name. This will ensure that the sorting is done in the desired order.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\",\n","            \"additional_kwargs\": {}\n","          }\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"prompt_tokens\": 329,\n","      \"completion_tokens\": 126,\n","      \"total_tokens\": 455\n","    },\n","    \"model_name\": \"gpt-3.5-turbo\"\n","  },\n","  \"run\": null\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [4.75s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"text\": \"To sort the customers by last name and then first name, I can use the `sorted()` function and provide a custom key function that returns a tuple of the last name and first name. This will ensure that the sorting is done in the desired order.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\"\n","}\n","\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Python_REPL] Entering Tool run with input:\n","\u001b[0m\"sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\"\n","\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:Python_REPL] [8.998000000000001ms] Exiting Tool run with output:\n","\u001b[0m\"\"\n","\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n","\u001b[0m{\n","  \"input\": \"Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']] \",\n","  \"agent_scratchpad\": \"To sort the customers by last name and then first name, I can use the `sorted()` function and provide a custom key function that returns a tuple of the last name and first name. This will ensure that the sorting is done in the desired order.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\\nObservation: \\nThought:\",\n","  \"stop\": [\n","    \"\\nObservation:\",\n","    \"\\n\\tObservation:\"\n","  ]\n","}\n","\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n","\u001b[0m{\n","  \"prompts\": [\n","    \"Human: You are an agent designed to write and execute python code to answer questions.\\nYou have access to a python REPL, which you can use to execute python code.\\nIf you get an error, debug your code and try again.\\nOnly use the output of your code to answer the question. \\nYou might know the answer without running any code, but you should still run the code to get the answer.\\nIf it does not seem like you can write code to answer the question, just return \\\"I don't know\\\" as the answer.\\n\\n\\nPython_REPL: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [Python_REPL]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: Sort these customers by last name and then first name and print the output: [['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']] \\nThought:To sort the customers by last name and then first name, I can use the `sorted()` function and provide a custom key function that returns a tuple of the last name and first name. This will ensure that the sorting is done in the desired order.\\nAction: Python_REPL\\nAction Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\\nObservation: \\nThought:\"\n","  ]\n","}\n","\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] [3.39s] Exiting LLM run with output:\n","\u001b[0m{\n","  \"generations\": [\n","    [\n","      {\n","        \"text\": \"The code provided will sort the list of customers by last name and then first name. The `key` parameter in the `sorted()` function specifies a custom key function that returns a tuple of the last name and first name. This ensures that the sorting is done in the desired order.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\",\n","        \"generation_info\": {\n","          \"finish_reason\": \"stop\"\n","        },\n","        \"message\": {\n","          \"lc\": 1,\n","          \"type\": \"constructor\",\n","          \"id\": [\n","            \"langchain\",\n","            \"schema\",\n","            \"messages\",\n","            \"AIMessage\"\n","          ],\n","          \"kwargs\": {\n","            \"content\": \"The code provided will sort the list of customers by last name and then first name. The `key` parameter in the `sorted()` function specifies a custom key function that returns a tuple of the last name and first name. This ensures that the sorting is done in the desired order.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\",\n","            \"additional_kwargs\": {}\n","          }\n","        }\n","      }\n","    ]\n","  ],\n","  \"llm_output\": {\n","    \"token_usage\": {\n","      \"prompt_tokens\": 461,\n","      \"completion_tokens\": 111,\n","      \"total_tokens\": 572\n","    },\n","    \"model_name\": \"gpt-3.5-turbo\"\n","  },\n","  \"run\": null\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] [3.40s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"text\": \"The code provided will sort the list of customers by last name and then first name. The `key` parameter in the `sorted()` function specifies a custom key function that returns a tuple of the last name and first name. This ensures that the sorting is done in the desired order.\\nFinal Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n","}\n","\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [8.16s] Exiting Chain run with output:\n","\u001b[0m{\n","  \"output\": \"[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\"\n","}\n"]},{"output_type":"execute_result","data":{"text/plain":["\"[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["@tool\n","def time(text: str) -> str:\n","    \"\"\"Returns todays date, use this for any \\\n","    questions related to knowing todays date. \\\n","    The input should always be an empty string, \\\n","    and this function will always return todays \\\n","    date - any date mathmatics should occur \\\n","    outside this function.\"\"\"\n","    return str(date.today())\n","\n","agent= initialize_agent(\n","    tools + [time],\n","    llm,\n","    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n","    handle_parsing_errors=True,\n","    verbose=True\n",")\n","\n","try:\n","    result = agent(\"whats the date today?\")\n","except:\n","    print(\"exception on external access\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35wTWzgUHBVX","executionInfo":{"status":"ok","timestamp":1690038242670,"user_tz":-480,"elapsed":2792,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"ecd415d6-42a2-4cb7-f4e5-04907b4de3bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mQuestion: What's the date today?\n","Thought: I can use the `time` tool to get the current date.\n","Action:\n","```\n","{\n","  \"action\": \"time\",\n","  \"action_input\": \"\"\n","}\n","```\n","\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m2023-07-22\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n","Final Answer: The date today is 2023-07-22.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}]}]}