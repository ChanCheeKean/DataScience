{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HtfUZkU07171"},"outputs":[],"source":["!pip install datasets transformers bitsandbytes peft accelerate trl flash-attn --no-build-isolation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbRf7aIqFwKX"},"outputs":[],"source":["import os, sys\n","import re\n","import logging\n","from copy import deepcopy\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from dataclasses import dataclass\n","from huggingface_hub import list_repo_files\n","import datasets, transformers\n","from accelerate import Accelerator\n","from datasets import DatasetDict, concatenate_datasets, load_dataset, load_from_disk\n","from peft import LoraConfig, PeftConfig, PeftModel\n","from trl import SFTTrainer, DPOTrainer\n","from transformers import (\n","    AutoTokenizer, BitsAndBytesConfig, PreTrainedTokenizer,\n","    AutoModelForCausalLM, TrainingArguments\n",")\n","\n","accelerator = Accelerator()\n","logger = logging.getLogger()"]},{"cell_type":"markdown","metadata":{"id":"blOEhjO_dWO3"},"source":["[Zephyr Training Repo](https://github.com/huggingface/alignment-handbook/tree/main)"]},{"cell_type":"markdown","metadata":{"id":"7s3MGjHVLTNS"},"source":["# 1) Supervised Training"]},{"cell_type":"markdown","metadata":{"id":"a5AuiLGQHtvC"},"source":["## 1.1 Dataset Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvJUilfLI8C6"},"outputs":[],"source":["def mix_datasets(d_config, splits, shuffle):\n","    raw_datasets = DatasetDict()\n","    raw_train_datasets = []\n","    raw_val_datasets = []\n","    fracs = []\n","\n","    # combine fraction of multiple data together\n","    for ds, frac in d_config.items():\n","        fracs.append(frac)\n","        for split in splits:\n","            dataset = load_dataset(ds, split=split)\n","            try:\n","                dataset = load_dataset(ds, split=split)\n","            except:\n","                dataset = load_from_disk(os.path.join(ds, split))\n","\n","            if \"train\" in split:\n","                raw_train_datasets.append(dataset)\n","            elif \"test\" in split:\n","                raw_val_datasets.append(dataset)\n","            else:\n","                raise ValueError(f\"Split type {split} not recognized as one of test or train.\")\n","\n","    if len(raw_train_datasets) > 0:\n","        train_subsets = []\n","        for dataset, frac in zip(raw_train_datasets, fracs):\n","            train_subset = dataset.select(range(int(frac * len(dataset))))\n","            train_subsets.append(train_subset)\n","\n","        if shuffle:\n","            raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n","        else:\n","            raw_datasets[\"train\"] = concatenate_datasets(train_subsets)\n","\n","    if len(raw_val_datasets) > 0:\n","        if shuffle:\n","            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(seed=42)\n","        else:\n","            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)\n","\n","    return raw_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14454,"status":"ok","timestamp":1702917966397,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"},"user_tz":-480},"id":"6H-dTbfULKHz","outputId":"3bfb0a78-ca19-4757-d70c-70c446a68c6f"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['prompt', 'prompt_id', 'messages'],\n","        num_rows: 20786\n","    })\n","    test: Dataset({\n","        features: ['prompt', 'prompt_id', 'messages'],\n","        num_rows: 23110\n","    })\n","})\n","\n","\n","{'content': 'How does the author propose to fix the problem of science alienation in our educational system? What changes does she suggest should be made to science education? Answer according to: Science education should be split into two tracks.\\nSplit K-12 science education into two tracks, for majors and nonmajors.\\nThose who want to specialize in science could take math and complex chemistry. Nonmajors would focus on science of the everyday—things like kitchen chemistry and CSI-style crime investigations.\\nSome years ago, when I was working as a newspaper science writer in California, I fell into a rather idle conversation with a physicist on the subject of science education. Idle for him, at least, because what he said—the way he defined the American system of teaching K-12 students about science—has stayed with me since.\\nThis conversation has returned to me many times over the years. On the day my older son, who spent his childhood joyfully chasing insects and reading natural history books, came home from high school to announce that “biology is the most boring subject in the world.” The time the student adviser at the University of Wisconsin journalism school, where I teach, told me that undergrads don’t want to take science-writing classes because they already felt “beaten up by science.” Even on days when I read science literacy statistics and am startled by the percentage of people who think that antibiotics kill viruses as well as bacteria or that the sun revolves around the Earth.\\nIt’s easy for me to join with the chorus of concern over whether we are keeping up as a science-literate country. But even so, science literacy is the wrong term for the worried point I’m making here. I’m concerned about science alienation. The way we teach K-12 science, with its emphasis on the intricacies of formula and mathematical structure necessary for the next generation of scientists and engineers, shuts too many people out. I don’t mean just shut out of the priesthood here; there’s no reason for everyone to become a scientist. But shut out of the comfort zone, of the ability to think of science as something useful in daily life.\\nAnd it’s the science-alienated who matter to me. Partly because as a science writer I find them the most interesting audience—I love the idea of seducing someone convinced that science is boring or meaningless into reading a research-centered story. Partly because I worry that what The New Yorker’s Michael Specter describes as a culture of science denialism is having a profound—possibly dangerous—effect on public policies. Think, for instance, of recent moves in Virginia, Texas, and North Carolina to deny that sea level rise is related to global climate change. And partly I just think that the filtered-out are cheated. They endure the mandatory classes, but they never really learn the main lesson—the way that science helps us understand ourselves and the world around us.\\nThe great psychologist and philosopher William James wrote more than 100 years ago that the science establishment would face consequences if it dismissed the importance of the lay public. In the year 1909, he said, it was easy to find people who no longer paid attention to the teachings of researchers. “They are indifferent to science,” he explained, “because science is so callously indifferent to their experiences.” Today, this is very relevant to K-12 science education: Our system breeds that indifference.\\nHow can we fix it? By requiring even more science education—but changing it dramatically. All high school students should have to take four years of science, instead of the minimal two usually required for graduation. Along with that, we should also offer separate “science tracks”—something like the old tracks for vocational or college-bound students. Those who want to specialize in science could take math, physics, and complex chemistry (and the kind of genetically detailed biology that my son disliked). Nonmajors, by contrast, would focus on what I think of as science of the everyday—things like kitchen chemistry, CSI-style crime investigation, the biology of health, and a class in the physics of sports that would include playing the games.\\nMy tracking system could easily build on existing science electives. My son’s school, for instance, teaches a forensic science unit which uses the classroom as an ever changing “crime scene,” exploring everything from DNA analysis to blood typing. It’s a way of putting science in context. More students would take such classes; more would be exposed to these ideas as they did the coursework needed to complete each track. Ideally, the tracks could be coordinated with the science classes offered for majors and nonmajors at the college level. Of course, occasionally a student might want to switch tracks—and we should allow for that, too. It could go both ways: Would-be scientists might realize that the field isn’t really for them and switch to the nonmajor track, while some students might be inspired by the CSI class to move into the more difficult sciences.\\nWe need to stop trying to teach every child as either a future scientist or a future failed scientist. We don’t want or need every student to be a scientist, but we do want each one to be a success. And success includes a solid understanding and appreciation of science, one that will remain useful to both themselves and society throughout their lives.\\nAlso in Slate’s special issue on science education: Fred Kaplan explains why another “Sputnik moment” would be impossible; Philip Plait explains why he became the “Bad Astronomer”; Paul Plotz describes how almost blowing up his parents’ basement made him a scientist; Tom Kalil says that the Obama administration is using the Make movement to encourage science education; and Dana Goldstein explains why you should make your daughter play video games. Also, share your ideas for fixing science education in the Hive. This article arises from Future Tense, a joint partnership of Slate, the New America Foundation, and Arizona State University.', 'role': 'user'}\n","\n","\n","{'content': 'The author proposes to fix the problem of science alienation in our educational system by splitting K-12 science education into two tracks, for majors and nonmajors. Those who want to specialize in science could take math and complex chemistry, while nonmajors would focus on science of the everyday – things like kitchen chemistry and CSI-style crime investigations. All high school students should take four years of science, and the tracking system could build on existing science electives. The goal is to teach every student a solid understanding and appreciation of science, one that will remain useful to both themselves and society throughout their lives.', 'role': 'assistant'}\n","\n","\n","{'content': 'Can you provide examples of how the proposed tracking system for science education could be implemented in schools?', 'role': 'user'}\n","\n","\n","{'content': 'There are several ways in which the proposed tracking system for science education could be implemented in schools. Here are a few examples:\\n\\n1. One approach could be to offer different science courses for majors and nonmajors starting in middle school. For example, in 6th grade, students could be divided into two tracks, and each track would take a different science course. In the major track, students would take a course that focuses on the foundations of science, including an emphasis on mathematics and physics. In the nonmajor track, students would take a course that focuses on science concepts that are relevant to everyday life, such as health and nutrition.\\n\\n2. Another approach could be to offer more advanced science courses in high school for students who wish to pursue a career in science. This could include courses in advanced physics, chemistry, and biology. For students who are not interested in a science career, science courses could be focused on more practical applications of science, such as environmental science or food science.\\n\\n3. Schools could also offer a wider range of science electives to cater to the varied interests of students. For example, students interested in engineering could take courses in robotics or renewable energy, while those interested in the medical field could take courses in anatomy and physiology.\\n\\n4. Hands-on science experiments and projects could be incorporated into science classes to make learning more engaging and relevant. For example, students could conduct experiments in their own kitchens to learn about chemical reactions, or participate in a mock crime scene investigation to learn about forensic science.\\n\\nOverall, the tracking system for science education proposed by the author could be implemented in a variety of ways, depending on the specific needs and interests of students in each school.', 'role': 'assistant'}\n","\n","\n","{'content': 'Can you explain how the proposed tracking system for science education would benefit students who are not interested in pursuing a career in science?', 'role': 'user'}\n","\n","\n","{'content': 'The proposed tracking system for science education would benefit students who are not interested in pursuing a career in science in several ways. First, it would help them develop an understanding and appreciation of science concepts that are relevant to everyday life, such as health, nutrition, and the environment. This would help them make informed decisions in their personal lives and understand how science impacts society as a whole.\\n\\nSecond, focusing on science concepts that are practical and engaging would make science more interesting and relevant to non-science students. This could help them overcome their science alienation and develop a sense of curiosity and wonder about science.\\n\\nLastly, the proposed tracking system would offer non-science students a wider range of science courses and electives to choose from, which would allow them to explore different areas of science according to their interests. For example, a student who is interested in sports could take a physics of sports class, while a student who is interested in cooking could take a kitchen chemistry class.\\n\\nOverall, the proposed tracking system for science education would benefit non-science students by providing them with a solid understanding and appreciation of science concepts that are relevant to their everyday lives, increasing their interest in science, and offering them more opportunities to explore science according to their interests.', 'role': 'assistant'}\n","\n","\n","{'content': 'Can you elaborate on how offering separate science tracks would inspire students to explore science further and possibly change their career path?', 'role': 'user'}\n","\n","\n","{'content': 'Offering separate science tracks would inspire students to explore science further and possibly change their career path by exposing them to a wider range of science concepts that are relevant to their interests and providing them with the opportunity to engage in hands-on, practical science activities that they may enjoy.\\n\\nFor example, a student who takes a kitchen chemistry class in the non-major track may discover a newfound interest in food science and decide to pursue a career in that field. Similarly, a student who takes a physics of sports class may develop an interest in sports science and decide to pursue that field.\\n\\nFurthermore, by offering science courses that are more practical and engaging, students who may have previously been disinterested in science may discover a passion for a particular subject or field within science. This could motivate them to pursue further studies in that area and even consider a career in science.\\n\\nOverall, by offering separate science tracks, schools would be creating opportunities for students to explore science according to their interests and discover new areas of science that they may be passionate about. This could inspire them to change their career path and pursue a career in science, even if they had not previously considered it. The more science-literate our society is, the more equipped we are to tackle challenges and advance scientific discoveries.', 'role': 'assistant'}\n"]}],"source":["dataset_config = {\n","    \"HuggingFaceH4/ultrachat_200k\": 0.1\n","}\n","raw_datasets = mix_datasets(dataset_config, splits=['train_sft', 'test_sft'], shuffle=False)\n","print(raw_datasets)\n","\n","for msg in raw_datasets['test'][0]['messages']:\n","    print(\"\\n\")\n","    print(msg)"]},{"cell_type":"markdown","metadata":{"id":"6vUSJCE8L9-o"},"source":["## 1.2 Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1M4tQK8L_7h"},"outputs":[],"source":["DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n","\n","def get_tokenizer(model_name_or_path, truncation_side=None):\n","    \"\"\"Get the tokenizer for the model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, revision='main')\n","    if tokenizer.pad_token_id is None:\n","        tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","    if truncation_side:\n","        tokenizer.truncation_side = truncation_side\n","\n","    # Set reasonable default for models without max length\n","    if tokenizer.model_max_length > 100_000:\n","        tokenizer.model_max_length = 2048\n","\n","    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n","    return tokenizer\n","\n","tokenizer = get_tokenizer('mistralai/Mistral-7B-v0.1')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1966,"status":"ok","timestamp":1702917968359,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"},"user_tz":-480},"id":"IV-b52zpPzkK","outputId":"c9e5e60b-f091-4a0d-c093-909c4e71588e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","<|system|>\n","</s>\n","<|user|>\n","Which famous landmarks should I visit in London, beyond the usual ones?</s>\n","<|assistant|>\n","1. Leadenhall Market - a beautiful indoor market with stunning Victorian architecture, also used as a filming location in the Harry Potter films.\n","\n","2. St. Dunstan in the East - a ruined church in the middle of the city that has been turned into a beautiful public garden.\n","\n","3. The Monument - a 202-foot-tall column commemorating the Great Fire of London, with a staircase leading to a viewing platform offering great views of the city.\n","\n","4. The Camden Town Markets - an eclectic collection of markets offering food, fashion, and vintage items, plus live music and street performers.\n","\n","5. Novelist's House - the former home of Charles Dickens, now a museum dedicated to his life and works.\n","\n","6. The Old Operating Theatre - a museum housed in the oldest surviving operating theatre in Europe, with exhibits on the history of surgery and medical practices.\n","\n","7. The Churchill War Rooms - an underground complex used as a British government command centre during World War II, now a museum dedicated to Winston Churchill and his leadership during the war.\n","\n","8. The Brunel Museum - a museum dedicated to the life and works of engineer Isambard Kingdom Brunel, with exhibitions on his revolutionary designs for bridges, ships, and tunnels.\n","\n","9. The Clink Prison Museum - a museum in the former site of the notorious medieval prison, with exhibits on the history of crime and punishment in London.\n","\n","10. The Royal Observatory Greenwich - an observatory and museum dedicated to astronomy and navigation, with exhibits on the history of timekeeping and space exploration.</s>\n","<|user|>\n","Hmm, those are interesting suggestions, but I've already seen so many touristy landmarks in London. Is there something more offbeat that you can recommend? Something that locals might know about?</s>\n","<|assistant|>\n","Absolutely! Here are some offbeat and lesser-known things to see in London that locals might recommend:\n","\n","1. God's Own Junkyard - a neon wonderland filled with vintage and new neon signs.\n","\n","2. Hornsey Town Hall Arts Centre - a grade II listed building with a rich history and home to many art exhibitions and events.\n","\n","3. Clapham Grand - a historic Victorian theatre that has been transformed into a music venue, cinema and a comedy club.\n","\n","4. Gordon Museum of Pathology - the pathology museum of King's College in London that holds a collection of over 8,000 medical specimens.\n","\n","5. John Soane's Museum - the former home of wealthy architect John Soane is a museum with eclectic collections of paintings, sculptures, and curiosities.\n","\n","6. The Viktor Wynd Museum of Curiosities - a small museum filled with strange and unusual objects, taxidermy, and rare books.\n","\n","7. The Cross Bones Graveyard - a medieval graveyard that became known as an outcast cemetery with a history connected to the sex industry, now a memorial garden.\n","\n","8. Highgate Cemetery - a beautiful Victorian graveyard known for its Gothic architecture and where many famous people, including Karl Marx, are buried.\n","\n","9. Dalston Eastern Curve Garden - a beautiful community garden near the Dalston Junction station that is run by volunteers and has regular events.\n","\n","10. The Geffrye Museum of the Home - a museum that explores the history of domestic interiors in England, with a beautiful garden and regular market events.\n","\n","There are many other hidden gems in London, and a quick Google search for ‘offbeat things in London’ will bring up many blogs and resources with more options.</s>\n","<|user|>\n","Wow, these offbeat suggestions are really interesting. I had no idea there were so many unique things to see in London. Thanks for the recommendations, you're much more helpful than I expected. Can you tell me more about the Viktor Wynd Museum of Curiosities? It sounds like my kind of place.</s>\n","<|assistant|>\n","Sure! The Viktor Wynd Museum of Curiosities, Fine Art & Natural History is a small museum located in Hackney, East London. The museum, founded by collector Viktor Wynd is a place where you can find taxidermy, books, anatomical models, and other curiosities. The museum is famous for its collection of shrunken heads, dodo bones, and the famous mermaid. It is also notable for its dimly lit, atmospheric decor, and its bar which serves cocktails and allows visitors to take drinks into the museum. The museum is open daily except for Mondays and Tuesdays, and it also frequently hosts events and exhibitions, so be sure to check the museum's website to see what's on.</s>\n","<|user|>\n","Woah, the Viktor Wynd Museum sounds like a wacky and fun place to visit. I don't think I've ever seen a shrunken head before. I wonder what kind of cocktails they serve there... Maybe I can convince some friends to check it out with me!</s>\n","<|assistant|>\n","Yes, the museum is definitely one of the more unusual places to visit in London! The cocktail menu at Viktor Wynd's bar is just as eclectic as the museum's collection, featuring brightly colored cocktails inspired by the exhibits, such as the \"Shrunken Head\" or \"Mermaid Tail\" cocktails. The museum and bar are on the smaller side, so it would make for a unique and intimate outing with friends. If you're looking for something different and memorable to do in London, the Viktor Wynd Museum is definitely worth checking out!</s>\n","<|user|>\n","Whoa, a bar in a museum?! That's wild. I'm definitely going to have to try one of those cocktails. Do you think they have anything with a shrunken head as a garnish? That would be a trip.</s>\n","<|assistant|>\n","I am not sure if the Viktor Wynd Museum of Curiosities serves cocktails with an actual shrunken head as a garnish. However, they might have some creative cocktails that incorporate a shrunken head replica or something similar as an element in the drink. Nevertheless, the bar offers a unique setting and a cocktail menu that includes some unusual ingredients so it's definitely worth checking out! It's likely to be an eccentric and memorable experience, and I'm sure you'll have a great time there with your friends.</s>\n","<|user|>\n","I can't wait to see that mermaid at the Viktor Wynd Museum. I wonder if it's real or just a replica. Either way, it's bound to be a fascinating item to behold. And I'll make sure to order that \"Mermaid Tail\" cocktail while I'm there!</s>\n","<|assistant|>\n","Yes, the mermaid at the Viktor Wynd Museum of Curiosities is definitely one of its most famous exhibits. It's actually a recreation made from the skeleton of a monkey and the tail of a fish, so it's not a real mermaid of course, but it's still an intriguing piece. The museum has many other curiosities to discover as well, and the atmosphere there is unlike anything you'll find in other museums. And don't forget to try that \"Mermaid Tail\" cocktail - it's the perfect accompaniment to the museum's quirky vibe!</s>\n","\n"]}],"source":["def apply_chat_template(example, tokenizer, task=\"sft\", assistant_prefix=\"<|assistant|>\\n\"):\n","    def _strip_prefix(s, pattern):\n","        # Use re.escape to escape any special characters in the pattern\n","        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n","\n","    if task in [\"sft\", \"generation\"]:\n","        messages = example[\"messages\"]\n","\n","        # add an empty system message if there is none\n","        if messages[0][\"role\"] != \"system\":\n","            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","        example[\"text\"] = tokenizer.apply_chat_template(\n","            messages, tokenize=False, add_generation_prompt=True if task == \"generation\" else False)\n","\n","    elif task == \"rm\":\n","        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n","            chosen_messages = example[\"chosen\"]\n","            rejected_messages = example[\"rejected\"]\n","\n","            # add an empty system message if there is none\n","            if chosen_messages[0][\"role\"] != \"system\":\n","                chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","            if rejected_messages[0][\"role\"] != \"system\":\n","                rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n","            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n","        else:\n","            raise ValueError(f\"Require `[chosen, rejected]` keys but found {list(example.keys())}\")\n","\n","    elif task == \"dpo\":\n","        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n","            # Compared to reward modeling, filter out the prompt\n","            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n","\n","            # Insert system message\n","            if example[\"chosen\"][0][\"role\"] != \"system\":\n","                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","            else:\n","                prompt_messages.insert(0, example[\"chosen\"][0])\n","\n","            # TODO: handle case where chosen/rejected also have system messages\n","            chosen_messages = example[\"chosen\"][1:]\n","            rejected_messages = example[\"rejected\"][1:]\n","            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n","            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n","            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n","                prompt_messages, tokenize=False, add_generation_prompt=True\n","            )\n","            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n","            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n","        else:\n","            raise ValueError(f\"Require `[chosen, rejected]` keys but found {list(example.keys())}\")\n","    else:\n","        raise ValueError(f\"Ensure provided task is one of {['sft', 'generation', 'rm', 'dpo']}\")\n","\n","    return example\n","\n","raw_datasets = raw_datasets.map(\n","    apply_chat_template, fn_kwargs={\"tokenizer\": tokenizer, \"task\": \"sft\"}\n",")\n","\n","print('\\n')\n","train_dataset = raw_datasets[\"train\"]\n","eval_dataset = raw_datasets[\"test\"]\n","print(train_dataset[1]['text'])"]},{"cell_type":"markdown","metadata":{"id":"wHtcUEumcBwy"},"source":["## 1.3 Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpAwP635cURO"},"outputs":[],"source":["def get_quantization_config(load_in_4bit=False, load_in_8bit=False):\n","\n","    if load_in_4bit:\n","        quantization_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_quant_type='nf4', # (fp4 or nf4)\n","            bnb_4bit_use_double_quant=False)\n","\n","    elif load_in_8bit:\n","        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","    else:\n","        quantization_config = None\n","\n","    return quantization_config\n","\n","# \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"]\n","torch_dtype = \"bfloat16\"\n","torch_dtype = torch_dtype if torch_dtype in [\"auto\", None] else getattr(torch, torch_dtype)\n","quantization_config = get_quantization_config(load_in_4bit=True)"]},{"cell_type":"markdown","metadata":{"id":"6Qv6aWFsgbuZ"},"source":["## 1.4 Model Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["148a211d7b8f4bca82e0957abc644b38","b7c074ee37014218b6a1215209b18217","d25bb5eb7736444790123ff524650475","8be1470212574c00a90fd0cf6aef699b","8dcfc16355364909a75b16f9d905497a","6c1e8d38be484abe9f92c9d352b6217a","6f4dcde0c82c4e479df28f8d2ad42062","a0dda0c414a949cd9b1c0a3148eb3f59","3d4acfadf90f43cb970c2ea7689003bb","dbdcfb038ff841099701835462187656","8d4fe5f884db42759f79f410770e6b87"]},"executionInfo":{"elapsed":94858,"status":"ok","timestamp":1702918063078,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"},"user_tz":-480},"id":"Z6Z6mr0ogcw_","outputId":"2836335f-5adc-4d6f-9508-5afd3a099f77"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"148a211d7b8f4bca82e0957abc644b38","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def get_current_device():\n","    return Accelerator().local_process_index if torch.cuda.is_available() else \"cpu\"\n","\n","def get_kbit_device_map():\n","    return {\"\": get_current_device()} if torch.cuda.is_available() else None\n","\n","model_kwargs = dict(\n","    revision='main',\n","    trust_remote_code=False,\n","    use_flash_attention_2=False,\n","    torch_dtype=torch_dtype,\n","    use_cache=False,\n","    device_map=get_kbit_device_map() if quantization_config is not None else None,\n","    quantization_config=quantization_config,\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    'mistralai/Mistral-7B-v0.1',\n","    **model_kwargs\n",")"]},{"cell_type":"markdown","metadata":{"id":"0RqO0LLrmmtl"},"source":["## 1.5 Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCflomUdm3F0"},"outputs":[],"source":["def get_peft_config(\n","        use_peft, lora_r, lora_alpha, lora_dropout,\n","        lora_target_modules, lora_modules_to_save\n","    ):\n","\n","    if use_peft is False:\n","        return None\n","\n","    peft_config = LoraConfig(\n","        r=lora_r,\n","        lora_alpha=lora_alpha,\n","        lora_dropout=lora_dropout,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=lora_target_modules,\n","        modules_to_save=lora_modules_to_save,\n","    )\n","    return peft_config\n","\n","lora_args = dict(\n","    use_peft=True,\n","    lora_r=4,\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n","    lora_modules_to_save=None\n",")\n","\n","peft_config = get_peft_config(**lora_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":741,"status":"ok","timestamp":1702918063801,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"},"user_tz":-480},"id":"G-_X8K_KihXs","outputId":"829d1af2-b5e5-4788-faf7-cc8492c082c5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:247: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n","  warnings.warn(\n"]}],"source":["# gradient_checkpointing use significantly memory with small decrease in training speed\n","training_args = TrainingArguments(\n","    bf16=False,\n","    evaluation_strategy='epoch',\n","    gradient_accumulation_steps=128,\n","    gradient_checkpointing=True,\n","    gradient_checkpointing_kwargs={'use_reentrant': False},\n","    learning_rate=2.0e-05,\n","    log_level='info',\n","    logging_steps=5,\n","    logging_strategy='steps',\n","    lr_scheduler_type='cosine',\n","    max_steps=-1,\n","    num_train_epochs=1,\n","    output_dir='data/zephyr-7b-sft-lora',\n","    overwrite_output_dir=True,\n","    per_device_eval_batch_size=8,\n","    per_device_train_batch_size=2,\n","    save_strategy=\"no\",\n","    seed=42,\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    # model_init_kwargs=model_kwargs,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    dataset_text_field=\"text\",\n","    max_seq_length=512,\n","    tokenizer=tokenizer,\n","    packing=True,\n","    peft_config=peft_config,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"RaNfel4zvcPo","outputId":"09bf1d3e-45ab-4331-cba9-34ffb7b7bafe"},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running training *****\n","  Num examples = 20,786\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 2\n","  Total train batch size (w. parallel, distributed & accumulation) = 256\n","  Gradient Accumulation steps = 128\n","  Total optimization steps = 81\n","  Number of trainable parameters = 3,407,872\n","Token indices sequence length is longer than the specified maximum sequence length for this model (3400 > 2048). Running this sequence through the model will result in indexing errors\n","You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 2/81 : < :, Epoch 0.01/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["train_result = trainer.train()\n","metrics = train_result.metrics\n","max_train_samples = 2000\n","metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)\n","trainer.save_state()"]},{"cell_type":"markdown","metadata":{"id":"UCJWw6I9wQYB"},"source":["## 1.6 Evaluation and Model Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d11BIUkxwRYO"},"outputs":[],"source":["metrics = trainer.evaluate()\n","max_eval_samples = 2000\n","metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n","trainer.log_metrics(\"eval\", metrics)\n","trainer.save_metrics(\"eval\", metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fo1CSdUwcdZ"},"outputs":[],"source":["trainer.save_model(training_args.output_dir)\n","logger.info(f\"Model saved to {training_args.output_dir}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7l-PFuIpxKqN"},"outputs":[],"source":["if accelerator.is_main_process:\n","    kwargs = {\n","        \"finetuned_from\": 'mistralai/Mistral-7B-v0.1',\n","        \"dataset\": list(dataset_config.keys()),\n","        \"dataset_tags\": list(dataset_config.keys()),\n","        \"tags\": [\"alignment-handbook\"],\n","    }\n","    trainer.create_model_card(**kwargs)\n","\n","    # Restore k,v cache for fast inference\n","    trainer.model.config.use_cache = True\n","    trainer.model.config.save_pretrained(training_args.output_dir)\n","\n","accelerator.wait_for_everyone()"]},{"cell_type":"markdown","metadata":{"id":"tfGTIlvh2ZEE"},"source":["# 2) DPO"]},{"cell_type":"markdown","source":["## 2.1 Dataset Loading"],"metadata":{"id":"hqyHHT9wvaRU"}},{"cell_type":"code","source":["model_name = 'alignment-handbook/zephyr-7b-sft-lora'\n","use_peft = True"],"metadata":{"id":"oHvkqxjI5D08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mix_datasets(d_config, splits, shuffle):\n","    raw_datasets = DatasetDict()\n","    raw_train_datasets = []\n","    raw_val_datasets = []\n","    fracs = []\n","\n","    # combine fraction of multiple data together\n","    for ds, frac in d_config.items():\n","        fracs.append(frac)\n","        for split in splits:\n","            dataset = load_dataset(ds, split=split)\n","            try:\n","                dataset = load_dataset(ds, split=split)\n","            except:\n","                dataset = load_from_disk(os.path.join(ds, split))\n","\n","            if \"train\" in split:\n","                raw_train_datasets.append(dataset)\n","            elif \"test\" in split:\n","                raw_val_datasets.append(dataset)\n","            else:\n","                raise ValueError(f\"Split type {split} not recognized as one of test or train.\")\n","\n","    if len(raw_train_datasets) > 0:\n","        train_subsets = []\n","        for dataset, frac in zip(raw_train_datasets, fracs):\n","            train_subset = dataset.select(range(int(frac * len(dataset))))\n","            train_subsets.append(train_subset)\n","\n","        if shuffle:\n","            raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n","        else:\n","            raw_datasets[\"train\"] = concatenate_datasets(train_subsets)\n","\n","    if len(raw_val_datasets) > 0:\n","        if shuffle:\n","            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(seed=42)\n","        else:\n","            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)\n","\n","    return raw_datasets"],"metadata":{"id":"Kj56xDY4xSnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHDPLTNF2aZe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704343345411,"user_tz":-480,"elapsed":27771,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"663cbe18-81b6-447f-daa7-c54f8fff0673"},"outputs":[{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n","        num_rows: 1239\n","    })\n","    test: Dataset({\n","        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n","        num_rows: 2000\n","    })\n","})\n","['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected']\n","Please answer the following question: Here's a logic test: Alan was playing with a ball. He noticed that when he rolled the ball across the tile floor, it moved easier than it did on the carpet. This is because the carpet is (A) rougher (B) smoother.  Choose the answer between \"tile floor\" and \"carpet\".\n","A:\n","[{'content': 'carpet', 'role': 'assistant'}]\n","[{'content': 'B:', 'role': 'assistant'}]\n","7.0\n","2.0\n"]}],"source":["dataset_config = {\n","    \"HuggingFaceH4/ultrafeedback_binarized\": 0.001\n","}\n","raw_datasets = mix_datasets(dataset_config, splits=['train_prefs', 'test_prefs'], shuffle=False)\n","column_names = list(raw_datasets[\"train\"].features)\n","print(raw_datasets)\n","print(column_names)\n","\n","idx = 200\n","print(raw_datasets['test'][idx]['prompt'])\n","print(raw_datasets['test'][idx]['chosen'][1:])\n","print(raw_datasets['test'][idx]['rejected'][1:])\n","print(raw_datasets['test'][idx]['score_chosen'])\n","print(raw_datasets['test'][idx]['score_rejected'])"]},{"cell_type":"markdown","source":["## 1.2 Tokenization"],"metadata":{"id":"ne9USKVAxuI5"}},{"cell_type":"code","source":["DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n","\n","def get_tokenizer(model_name_or_path, truncation_side=None):\n","    \"\"\"Get the tokenizer for the model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, revision='main')\n","    if tokenizer.pad_token_id is None:\n","        tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","    if truncation_side:\n","        tokenizer.truncation_side = truncation_side\n","\n","    # Set reasonable default for models without max length\n","    if tokenizer.model_max_length > 100_000:\n","        tokenizer.model_max_length = 2048\n","\n","    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n","    return tokenizer\n","\n","tokenizer = get_tokenizer(model_name, truncation_side='left')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["47a502787d3f456b95e4445d864c1862","a0a92bddf8804081bfea830813828dc9","d213db9be2024ecabef164e2f0f18968","929c5c5317af4c568232a802871211a0","b95aba2576ce49b581b74125777f83a3","723d8b870f414481990df6c03d34b663","1b92fcaf4fc942bf915d1a26468e0328","d2ab3b908feb49f4ac2c606605ca6a35","0e410a2482fa4267939107492151b3e5","82bde1f5a9c94e508414a5a349a1c424","464a55e073ec4015939742a4aad6cdf8","24d55d4699d44cff845d3c0c0039d80d","a3253f46e9df492a9646f8af3db31d58","fd13ee50b13d4f3a8af17706177840fc","9907a7c151db4c3d8902b39f5cd899f1","c37eece4f8964b929165a97ac1418369","1e2448b87717418e97a27014a7cd674d","0239d0db47a3427db48070e9ba805b1c","92533c253cf04b05ae944b013cdb8bce","993aee3fc7294d398161d09008c3ff52","c00d5ba7d2fe4cb594c4faf3b22f12db","9c4e70ff43b24e258f2c6363fb36bd22","f79a85f6709145598ddf0d06139e127c","6bbfc165f9d1422fa510fe173d7b59bd","8b24b7cf314540cea4d4bd60de5be593","65d2631db1724e0bbce1418a068e81de","cf956ea8f3194a79b66ec247ceab0f44","6e612084f4914e0c8b7fdf5c47af11f5","7dafea4eac90404285d74472bbb8da4c","9981df032f6c4983a35460b11c242594","cb98a7bbe5ed45b18bf1820ddbd9401f","5790b423464a4206ba8660c225ba82ec","5e0877463fc9485daf44bae1ca0ba71d"]},"id":"0mv0dmz-xlSK","executionInfo":{"status":"ok","timestamp":1704343042812,"user_tz":-480,"elapsed":13666,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"d9f3b30e-2c78-4f40-c6b3-1b466d3011e5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a502787d3f456b95e4445d864c1862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24d55d4699d44cff845d3c0c0039d80d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f79a85f6709145598ddf0d06139e127c"}},"metadata":{}}]},{"cell_type":"code","source":["def apply_chat_template(example, tokenizer, task=\"sft\", assistant_prefix=\"<|assistant|>\\n\"):\n","    def _strip_prefix(s, pattern):\n","        # Use re.escape to escape any special characters in the pattern\n","        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n","\n","    if task in [\"sft\", \"generation\"]:\n","        messages = example[\"messages\"]\n","\n","        # add an empty system message if there is none\n","        if messages[0][\"role\"] != \"system\":\n","            messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","        example[\"text\"] = tokenizer.apply_chat_template(\n","            messages, tokenize=False, add_generation_prompt=True if task == \"generation\" else False)\n","\n","    elif task == \"rm\":\n","        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n","            chosen_messages = example[\"chosen\"]\n","            rejected_messages = example[\"rejected\"]\n","\n","            # add an empty system message if there is none\n","            if chosen_messages[0][\"role\"] != \"system\":\n","                chosen_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","            if rejected_messages[0][\"role\"] != \"system\":\n","                rejected_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n","            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n","        else:\n","            raise ValueError(f\"Require `[chosen, rejected]` keys but found {list(example.keys())}\")\n","\n","    elif task == \"dpo\":\n","        if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n","            # compared to reward modeling, filter out the prompt\n","            prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n","\n","            # Insert system message\n","            if example[\"chosen\"][0][\"role\"] != \"system\":\n","                prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","            else:\n","                prompt_messages.insert(0, example[\"chosen\"][0])\n","\n","            # TODO: handle case where chosen/rejected also have system messages\n","            chosen_messages = example[\"chosen\"][1:]\n","            rejected_messages = example[\"rejected\"][1:]\n","            example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n","            example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n","            example[\"text_prompt\"] = tokenizer.apply_chat_template(\n","                prompt_messages, tokenize=False, add_generation_prompt=True\n","            )\n","            example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n","            example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n","        else:\n","            raise ValueError(f\"Require `[chosen, rejected]` keys but found {list(example.keys())}\")\n","    else:\n","        raise ValueError(f\"Ensure provided task is one of {['sft', 'generation', 'rm', 'dpo']}\")\n","\n","    return example\n","\n","raw_datasets = raw_datasets.map(\n","    apply_chat_template,\n","    fn_kwargs={\"tokenizer\": tokenizer, \"task\": \"dpo\"},\n","    num_proc=12,\n","    remove_columns=column_names,\n","    desc=\"Formatting comparisons with prompt template\",\n",")\n","\n","for split in [\"train\", \"test\"]:\n","    raw_datasets[split] = raw_datasets[split].rename_columns(\n","        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n","    )\n","\n","print('\\n')\n","train_dataset = raw_datasets[\"train\"]\n","eval_dataset = raw_datasets[\"test\"]\n","\n","idx = 1010\n","print(\"Prompt: \", train_dataset[idx]['prompt'])\n","print(\"Choose Example: \", train_dataset[idx]['chosen'])\n","print(\"Rejected Example: \",train_dataset[idx]['rejected'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":708,"referenced_widgets":["429e4dd630f14df993222ea0aa4dd0ac","5b778983b02a4677b95d1f148cba50ed","efd774e7d27d49b381c01402d9d3b1b7","68fe05cd1a774bee8c0ef5b4ec4ccc6c","8f24a784fed14197a4c804a398238988","5fff05c886b64cec97cde11a8aebd97a","e159caf615ef406186ac09ecb9443f69","86d2f3826fbf4787b206ddb4f3b50197","77d22a3041854781bcd3435b62665f91","2436c53be9d4480c871bbf5951343acb","70675aed911b4821b7189608c0222ed8","18e25cc41a104110b96735c4d4fab6c0","912781c5b95640b49ce84e649d723e97","fbfb3ff87fb54b4cbe0a663b0ca907c1","ee2dd25cdc514d50aeefc929bdcedbe0","c1e5ca1338214b528668c5c7663ef001","71281efb42cf4f7ea54d140fb34dc81c","d41528a54e304a8d83dc121736705bcf","ff91c218e3f44802880031e159d078d7","38f44329792d4dc788c4bd4e5d71da2c","b1085098b8d84f639b300533783095cd","3be615fb0cb2472c85db347593baa57a"]},"id":"3K9ihS3wzqh-","executionInfo":{"status":"ok","timestamp":1704343049656,"user_tz":-480,"elapsed":6853,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9158a420-a0bc-42e9-cbb2-cd5a9cab0b77"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Formatting comparisons with prompt template (num_proc=12):   0%|          | 0/1239 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"429e4dd630f14df993222ea0aa4dd0ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Formatting comparisons with prompt template (num_proc=12):   0%|          | 0/2000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e25cc41a104110b96735c4d4fab6c0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","Prompt:  <|system|>\n","</s>\n","<|user|>\n","Can you summarize the features and specifications of the Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl?: Recent double head nibble metal cutting sheet nibbler saw cutter 360 degree adjustable drill attachment power tool accessories cutting tools intl, the product is a popular item this coming year. the product is really a new item sold by Ttech store and shipped from China. Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl is sold at lazada.sg having a inexpensive cost of SGD16.34 (This price was taken on 15 June 2018, please check the latest price here). what are features and specifications this Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl, let's see the facts below.\n","Straight cutting and curve cutting direction capable.\n","Ergonomic design handle for comfort, durable, efficient and safe use.\n","Double cutting head selective and 360 degree adjustable.\n","Perfect for car repair and maintenance and metal sheet productions.\n","Compact size, easy to operate and labor-saving.\n","Excellent cutting effect in any direction and any desired angle.\n","For detailed product information, features, specifications, reviews, and guarantees or another question that is more comprehensive than this Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl products, please go directly to the vendor store that is due Ttech @lazada.sg.\n","Ttech is a trusted shop that already has experience in selling Hand Tools products, both offline (in conventional stores) and internet based. lots of their customers are very satisfied to purchase products through the Ttech store, that may seen with the many upscale reviews written by their clients who have purchased products inside the store. So you do not have to afraid and feel concerned with your product not up to the destination or not relative to what exactly is described if shopping within the store, because has lots of other clients who have proven it.\n","Furthermore Ttech offer discounts and product warranty returns if your product you acquire will not match what you ordered, of course using the note they feature. Including the product that we're reviewing this, namely \"Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl\", they dare to present discounts and product warranty returns if your products you can purchase don't match what is described.\n","So, if you wish to buy or look for Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl however recommend you acquire it at Ttech store through marketplace lazada.sg.\n","Why would you buy Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl at Ttech shop via lazada.sg?\n","Obviously there are several advantages and benefits that exist when you shop at lazada.sg, because lazada.sg is a trusted marketplace and also have a good reputation that can give you security from all of types of online fraud. Excess lazada.sg in comparison with other marketplace is lazada.sg often provide attractive promotions for example rebates, shopping vouchers, free postage, and sometimes hold flash sale and support which is fast and which is certainly safe. and what I liked is simply because lazada.sg can pay on the spot, that was not there in almost any other marketplace.</s>\n","<|assistant|>\n","\n","Choose Example:  The Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl is a versatile and efficient tool for cutting metal sheets. It is a popular item this year, sold by Ttech store and shipped from China. The product is available at a reasonable price of SGD16.34.\n","\n","Key features and specifications of this tool include:\n","\n","1. Straight cutting and curve cutting direction capabilities for a variety of cut shapes.\n","2. Ergonomic handle design for enhanced comfort, efficiency, and safety during use.\n","3. Double cutting head with the option to select between them and 360-degree adjustability for precise cutting.\n","4. Ideal for car repair and maintenance, as well as metal sheet production tasks.\n","5. Compact size for easy operation and labor-saving benefits.\n","6. Excellent cutting effect in any direction and at any desired angle for versatile usage.\n","\n","The Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter can be purchased through the trusted Ttech store on Lazada.sg, which offers discounts and product warranty returns if the item does not match the description provided. The store has received positive reviews from numerous clients who were satisfied with their purchases. Shopping at Lazada.sg ensures security against online fraud and provides attractive promotions, such as rebates, shopping vouchers, free shipping, and occasional flash sales with fast and safe support.</s>\n","\n","Rejected Example:  The Double Head Nibble Metal Cutting Sheet Nibbler Saw Cutter 360 Degree Adjustable Drill Attachment Power Tool Accessories Cutting Tools - intl is a power tool accessory that is designed for cutting and sawing metal sheets. It features double cutting heads that are adjustable at 360 degrees for straight and curve cutting. The tool is suitable for car repair and maintenance, as well as metal sheet productions. It is ergonomically designed with an easy to use handle for comfort, durability, and safety. Compact in size, it is labor-saving and produces excellent cutting effects at any desired angle.</s>\n","\n"]}]},{"cell_type":"markdown","source":["## 1.3 Quantization"],"metadata":{"id":"lM6EEej53eQx"}},{"cell_type":"code","source":["def get_quantization_config(load_in_4bit=False, load_in_8bit=False):\n","\n","    if load_in_4bit:\n","        quantization_config = BitsAndBytesConfig(\n","            load_in_4bit=True,\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_quant_type='nf4', # (fp4 or nf4)\n","            bnb_4bit_use_double_quant=False)\n","\n","    elif load_in_8bit:\n","        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","    else:\n","        quantization_config = None\n","\n","    return quantization_config\n","\n","# \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"]\n","torch_dtype = \"auto\"\n","torch_dtype = torch_dtype if torch_dtype in [\"auto\", None] else getattr(torch, torch_dtype)\n","quantization_config = get_quantization_config(load_in_4bit=True)"],"metadata":{"id":"a7qavrle3fcR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.4 Model Loading"],"metadata":{"id":"ecT97BlQ31w7"}},{"cell_type":"code","source":["def get_current_device():\n","    return Accelerator().local_process_index if torch.cuda.is_available() else \"cpu\"\n","\n","def get_kbit_device_map():\n","    return {\"\": get_current_device()} if torch.cuda.is_available() else None\n","\n","model_kwargs = dict(\n","    revision='main',\n","    trust_remote_code=False,\n","    use_flash_attention_2=False,\n","    torch_dtype=torch_dtype,\n","    use_cache=False,\n","    device_map=get_kbit_device_map() if quantization_config is not None else None,\n","    quantization_config=quantization_config,\n",")"],"metadata":{"id":"TISKixi23vPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note: to run QLora, you will need to merge the based model separately as the merged model in 16bit\n","# load the model, merge the adapter weights and unload the adapter\n","\n","def is_adapter_model(model_name_or_path: str, revision: str = \"main\") -> bool:\n","    try:\n","        repo_files = list_repo_files(model_name_or_path, revision=revision)\n","    except:\n","        repo_files = os.listdir(model_name_or_path)\n","    return \"adapter_model.safetensors\" in repo_files or \"adapter_model.bin\" in repo_files\n","\n","if is_adapter_model(model_name, 'main'):\n","    peft_config = PeftConfig.from_pretrained(model_name, revision='main')\n","    base_model = AutoModelForCausalLM.from_pretrained(\n","        peft_config.base_model_name_or_path,\n","        **model_kwargs,\n","    )\n","    model = PeftModel.from_pretrained(base_model, model_name, revision='main')\n","    model.eval()\n","    model = model.merge_and_unload()\n","    model_kwargs = None\n","\n","ref_model = model\n","ref_model_kwargs = model_kwargs\n","\n","# if model is peft, adapters are turned off will be used as the reference model\n","if use_peft is True:\n","    ref_model = None\n","    ref_model_kwargs = None"],"metadata":{"id":"FTK1ntzp4MIt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.5 Model Training"],"metadata":{"id":"EVn13Gi27q68"}},{"cell_type":"code","source":["def get_peft_config(\n","        use_peft, lora_r, lora_alpha, lora_dropout,\n","        lora_target_modules, lora_modules_to_save\n","    ):\n","\n","    if use_peft is False:\n","        return None\n","\n","    peft_config = LoraConfig(\n","        r=lora_r,\n","        lora_alpha=lora_alpha,\n","        lora_dropout=lora_dropout,\n","        bias=\"none\",\n","        task_type=\"CAUSAL_LM\",\n","        target_modules=lora_target_modules,\n","        modules_to_save=lora_modules_to_save,\n","    )\n","    return peft_config\n","\n","lora_args = dict(\n","    use_peft=use_peft,\n","    lora_r=16,\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n","    lora_modules_to_save=None\n",")\n","\n","peft_config = get_peft_config(**lora_args)\n","model.enable_input_require_grads()"],"metadata":{"id":"KBO4bH5z7sJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    bf16=False,\n","    evaluation_strategy='epoch',\n","    eval_steps=100,\n","    gradient_accumulation_steps=32,\n","    gradient_checkpointing=True,\n","    gradient_checkpointing_kwargs={'use_reentrant': False},\n","    learning_rate=5.0e-7,\n","    log_level='info',\n","    logging_steps=10,\n","    lr_scheduler_type='linear',\n","    num_train_epochs=1,\n","    optim='rmsprop',\n","    output_dir='data/zephyr-7b-dpo-lora',\n","    per_device_eval_batch_size=4,\n","    per_device_train_batch_size=2,\n","    save_strategy=\"no\",\n","    seed=42,\n","    warmup_ratio=0.1,\n",")\n","\n","dpo_trainer = DPOTrainer(\n","    model,\n","    ref_model,\n","    model_init_kwargs=model_kwargs,\n","    ref_model_init_kwargs=ref_model_kwargs,\n","    args=training_args,\n","    beta=0.1,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    tokenizer=tokenizer,\n","    max_length=1024,\n","    max_prompt_length=None,\n","    peft_config=peft_config,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Y1EKl8S6be1","executionInfo":{"status":"ok","timestamp":1702971173588,"user_tz":-480,"elapsed":5,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"492e90c0-21d9-4032-9187-eb55f5a71ab7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","source":["train_result = dpo_trainer.train()\n","metrics = train_result.metrics\n","max_train_samples = 2000\n","metrics[\"train_samples\"] = min(max_train_samples, len(raw_datasets[\"train\"]))\n","dpo_trainer.log_metrics(\"train\", metrics)\n","dpo_trainer.save_metrics(\"train\", metrics)\n","dpo_trainer.save_state()"],"metadata":{"id":"qVWkT8Ce9DWj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.6 Evaluation"],"metadata":{"id":"8NWe5Lrz9wDl"}},{"cell_type":"code","source":["logger.info(\"*** Evaluate ***\")\n","metrics = dpo_trainer.evaluate()\n","max_eval_samples = 200\n","metrics[\"eval_samples\"] = min(max_eval_samples, len(raw_datasets[\"test\"]))\n","dpo_trainer.log_metrics(\"eval\", metrics)\n","dpo_trainer.save_metrics(\"eval\", metrics)"],"metadata":{"id":"76yKTrsj9xco"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dpo_trainer.save_model(training_args.output_dir)\n","# Save everything else on main process\n","if accelerator.is_main_process:\n","    kwargs = {\n","        \"finetuned_from\": model_name,\n","        \"dataset\": list(dataset_config.keys()),\n","        \"dataset_tags\": list(dataset_config.keys()),\n","        \"tags\": [\"alignment-handbook\"],\n","    }\n","    dpo_trainer.create_model_card(**kwargs)\n","\n","    # Restore k,v cache for fast inference\n","    dpo_trainer.model.config.use_cache = True\n","    dpo_trainer.model.config.save_pretrained(training_args.output_dir)\n","\n","# Ensure we don't timeout on model save / push to Hub\n","accelerator.wait_for_everyone()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AucscBNY-GyK","executionInfo":{"status":"ok","timestamp":1702970979474,"user_tz":-480,"elapsed":1256,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"368e17b8-12bc-4bee-9840-9ecfeff4417a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to data/zephyr-7b-dpo-lora\n","tokenizer config file saved in data/zephyr-7b-dpo-lora/tokenizer_config.json\n","Special tokens file saved in data/zephyr-7b-dpo-lora/special_tokens_map.json\n","Dropping the following result as it does not have all the necessary fields:\n","{'dataset': {'name': 'HuggingFaceH4/ultrafeedback_binarized', 'type': 'HuggingFaceH4/ultrafeedback_binarized', 'config': None, 'split': 'None'}}\n","Configuration saved in data/zephyr-7b-dpo-lora/config.json\n"]}]},{"cell_type":"markdown","source":["# 3) DPO Implementation from scratch\n","\n","[Original Code](https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L194)"],"metadata":{"id":"YBEBe-AKftFz"}},{"cell_type":"markdown","source":["## 3.1 Data Loading\n","\n","Follow the dataset at Chapter 2"],"metadata":{"id":"_ab8-i-vthl5"}},{"cell_type":"code","source":["model_name = 'microsoft/phi-2'\n","accelerator = Accelerator()"],"metadata":{"id":"dDNpou9HjgFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def mix_datasets(d_config, splits, shuffle):\n","    raw_datasets = DatasetDict()\n","    raw_train_datasets = []\n","    raw_val_datasets = []\n","    fracs = []\n","\n","    # combine fraction of multiple data together\n","    for ds, frac in d_config.items():\n","        fracs.append(frac)\n","        for split in splits:\n","            dataset = load_dataset(ds, split=split)\n","            try:\n","                dataset = load_dataset(ds, split=split)\n","            except:\n","                dataset = load_from_disk(os.path.join(ds, split))\n","\n","            if \"train\" in split:\n","                raw_train_datasets.append(dataset)\n","            elif \"test\" in split:\n","                raw_val_datasets.append(dataset)\n","            else:\n","                raise ValueError(f\"Split type {split} not recognized as one of test or train.\")\n","\n","    if len(raw_train_datasets) > 0:\n","        train_subsets = []\n","        for dataset, frac in zip(raw_train_datasets, fracs):\n","            train_subset = dataset.select(range(int(frac * len(dataset))))\n","            train_subsets.append(train_subset)\n","\n","        if shuffle:\n","            raw_datasets[\"train\"] = concatenate_datasets(train_subsets).shuffle(seed=42)\n","        else:\n","            raw_datasets[\"train\"] = concatenate_datasets(train_subsets)\n","\n","    if len(raw_val_datasets) > 0:\n","        if shuffle:\n","            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets).shuffle(seed=42)\n","        else:\n","            raw_datasets[\"test\"] = concatenate_datasets(raw_val_datasets)\n","\n","    return raw_datasets\n","\n","# loading data from huggingface\n","dataset_config = {\n","    \"HuggingFaceH4/ultrafeedback_binarized\": 0.0002\n","}\n","raw_datasets = mix_datasets(dataset_config, splits=['train_prefs', 'test_prefs'], shuffle=False)\n","column_names = list(raw_datasets[\"train\"].features)\n","print(raw_datasets)\n","print(column_names)\n","\n","idx = 200\n","print(raw_datasets['test'][idx]['prompt'])\n","print(raw_datasets['test'][idx]['chosen'][1:])\n","print(raw_datasets['test'][idx]['rejected'][1:])\n","print(raw_datasets['test'][idx]['score_chosen'])\n","print(raw_datasets['test'][idx]['score_rejected'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TpusZbACf2aF","executionInfo":{"status":"ok","timestamp":1704352977544,"user_tz":-480,"elapsed":35162,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"365ea95f-c85f-400f-d692-c931888becae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n","        num_rows: 12\n","    })\n","    test: Dataset({\n","        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n","        num_rows: 2000\n","    })\n","})\n","['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected']\n","Please answer the following question: Here's a logic test: Alan was playing with a ball. He noticed that when he rolled the ball across the tile floor, it moved easier than it did on the carpet. This is because the carpet is (A) rougher (B) smoother.  Choose the answer between \"tile floor\" and \"carpet\".\n","A:\n","[{'content': 'carpet', 'role': 'assistant'}]\n","[{'content': 'B:', 'role': 'assistant'}]\n","7.0\n","2.0\n"]}]},{"cell_type":"markdown","source":["## 3.2 Data Processing"],"metadata":{"id":"5GZfVhIUgNq_"}},{"cell_type":"code","source":["DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n","\n","def get_tokenizer(model_name_or_path, truncation_side=None):\n","    \"\"\"Get the tokenizer for the model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, revision='main')\n","    if tokenizer.pad_token_id is None:\n","        tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","    if truncation_side:\n","        tokenizer.truncation_side = truncation_side\n","\n","    # set reasonable default for models without max length\n","    if tokenizer.model_max_length > 100_000:\n","        tokenizer.model_max_length = 2048\n","\n","    tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE\n","    return tokenizer\n","\n","tokenizer = get_tokenizer(model_name, truncation_side='left')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ziSx1Ujvf2kX","executionInfo":{"status":"ok","timestamp":1704352980621,"user_tz":-480,"elapsed":3115,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"0fd39aa3-39de-40c6-888e-d76a64bf13c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["def apply_chat_template(example, tokenizer, task=\"sft\", assistant_prefix=\"<|assistant|>\\n\"):\n","    def _strip_prefix(s, pattern):\n","        # Use re.escape to escape any special characters in the pattern\n","        return re.sub(f\"^{re.escape(pattern)}\", \"\", s)\n","\n","    if all(k in example.keys() for k in (\"chosen\", \"rejected\")):\n","        # compared to reward modeling, filter out the prompt\n","        prompt_messages = [[msg for msg in example[\"chosen\"] if msg[\"role\"] == \"user\"][0]]\n","\n","        # Insert system message\n","        if example[\"chosen\"][0][\"role\"] != \"system\":\n","            prompt_messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n","        else:\n","            prompt_messages.insert(0, example[\"chosen\"][0])\n","\n","        # TODO: handle case where chosen/rejected also have system messages\n","        chosen_messages = example[\"chosen\"][1:]\n","        rejected_messages = example[\"rejected\"][1:]\n","        example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n","        example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n","        example[\"text_prompt\"] = tokenizer.apply_chat_template(\n","            prompt_messages, tokenize=False, add_generation_prompt=True\n","        )\n","        example[\"text_chosen\"] = _strip_prefix(example[\"text_chosen\"], assistant_prefix)\n","        example[\"text_rejected\"] = _strip_prefix(example[\"text_rejected\"], assistant_prefix)\n","    else:\n","        raise ValueError(f\"Require `[chosen, rejected]` keys but found {list(example.keys())}\")\n","\n","    return example\n","\n","raw_datasets = raw_datasets.map(\n","    apply_chat_template,\n","    fn_kwargs={\"tokenizer\": tokenizer, \"task\": \"dpo\"},\n","    num_proc=12,\n","    remove_columns=column_names,\n","    desc=\"Formatting comparisons with prompt template\",\n",")\n","\n","for split in [\"train\", \"test\"]:\n","    raw_datasets[split] = raw_datasets[split].rename_columns(\n","        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"})\n","\n","print('\\n')\n","train_dataset = raw_datasets[\"train\"]\n","eval_dataset = raw_datasets[\"test\"]\n","\n","idx = 11\n","print(\"Prompt: \", train_dataset[idx]['prompt'])\n","print(\"Choose Example: \", train_dataset[idx]['chosen'])\n","print(\"Rejected Example: \", train_dataset[idx]['rejected'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dXcHnSwyf2u-","executionInfo":{"status":"ok","timestamp":1704352981302,"user_tz":-480,"elapsed":701,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"1099ac53-a64b-456a-a148-45a6243c1e03"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Prompt:  <|system|>\n","<|endoftext|>\n","<|user|>\n","What are cardigans made of? Leather or wood?<|endoftext|>\n","<|assistant|>\n","\n","Choose Example:  Cardigans are not typically made of leather or wood. Instead, they are made from various types of fabrics, such as wool, cotton, and synthetic materials like polyester or nylon. Some cardigans may also be made from a blend of different materials for added comfort and durability. The choice of fabric depends on factors such as the desired texture, weight, and seasonality (e.g., thicker, warmer materials for colder weather).<|endoftext|>\n","\n","Rejected Example:  Cardigans are typically made of wool, cotton, or a combination of both.<|endoftext|>\n","\n"]}]},{"cell_type":"markdown","source":["## 3.3 Tokenization"],"metadata":{"id":"53ayY10CkA5d"}},{"cell_type":"code","source":["def build_tokenized_answer(tokenizer, prompt, answer):\n","    \"\"\"\n","    Llama tokenizer does satisfy `enc(a + b) = enc(a) + enc(b)`.\n","    It does ensure `enc(a + b) = enc(a) + enc(a + b)[len(enc(a)):]`.\n","    \"\"\"\n","\n","    full_tokenized = tokenizer(prompt + answer, add_special_tokens=False)\n","    prompt_input_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n","\n","    # to ensure tokenizer is doing the right way\n","    answer_input_ids = full_tokenized[\"input_ids\"][len(prompt_input_ids): ]\n","    answer_attention_mask = full_tokenized[\"attention_mask\"][len(prompt_input_ids): ]\n","\n","    # concat tokens to form `enc(a) + enc(a + b)[len(enc(a)):]`\n","    full_concat_input_ids = np.concatenate([prompt_input_ids, answer_input_ids])\n","\n","    # Prepare input tokens for token by token comparison\n","    full_input_ids = np.array(full_tokenized[\"input_ids\"])\n","\n","    if len(full_input_ids) != len(full_concat_input_ids):\n","        raise ValueError(\"Prompt input ids and answer input ids should have the same length.\")\n","\n","    # in some occasions, last token from the prompt being different when tokenized on its own\n","    # If tokenized prompt is different than both prompt+answer, then it means the\n","    # last token has changed due to merging.\n","\n","    response_token_ids_start_idx = len(prompt_input_ids)\n","    if prompt_input_ids != full_tokenized[\"input_ids\"][:response_token_ids_start_idx]:\n","        response_token_ids_start_idx -= 1\n","\n","    prompt_input_ids = full_tokenized[\"input_ids\"][:response_token_ids_start_idx]\n","    prompt_attention_mask = full_tokenized[\"attention_mask\"][:response_token_ids_start_idx]\n","\n","    answer_input_ids = full_tokenized[\"input_ids\"][response_token_ids_start_idx:]\n","    answer_attention_mask = full_tokenized[\"attention_mask\"][response_token_ids_start_idx:]\n","\n","    return dict(\n","        prompt_input_ids=prompt_input_ids,\n","        prompt_attention_mask=prompt_attention_mask,\n","        input_ids=answer_input_ids,\n","        attention_mask=answer_attention_mask,\n","    )\n","\n","def tokenize_row(\n","    feature, tokenizer, model=None, max_length=4096, is_encoder_decoder=False,\n","    truncation_mode='keep_end', max_prompt_length=2048, max_target_length=2048):\n","    \"\"\"\n","    Tokenize a single row from a DPO specific dataset.\n","    truncate the prompt; if still too long, truncate the chosen/rejected.\n","    Also create the labels for the chosen/rejected responses, which are of length equal to\n","    the sum of the length of the prompt and the chosen/rejected response, with\n","    label_pad_token_id  for the prompt tokens.\n","    \"\"\"\n","\n","    batch = {}\n","    prompt = feature[\"prompt\"]\n","    chosen = feature[\"chosen\"]\n","    rejected = feature[\"rejected\"]\n","\n","    if not is_encoder_decoder:\n","\n","        # will generate 'attention mask' and 'input ids'\n","        prompt_tokens = tokenizer(prompt, add_special_tokens=False)\n","        prompt_tokens = {f\"prompt_{k}\": v for k, v in prompt_tokens.items()}\n","        chosen_tokens = build_tokenized_answer(tokenizer, prompt, chosen)\n","        rejected_tokens = build_tokenized_answer(tokenizer, prompt, rejected)\n","\n","        # add BOS token (50256) to head of prompt, seq_len + 1\n","        prompt_tokens[\"prompt_input_ids\"] = [tokenizer.bos_token_id] + prompt_tokens[\"prompt_input_ids\"]\n","        chosen_tokens[\"prompt_input_ids\"] = [tokenizer.bos_token_id] + chosen_tokens[\"prompt_input_ids\"]\n","        rejected_tokens[\"prompt_input_ids\"] = [tokenizer.bos_token_id] + rejected_tokens[\"prompt_input_ids\"]\n","        prompt_tokens[\"prompt_attention_mask\"] = [1] + prompt_tokens[\"prompt_attention_mask\"]\n","        chosen_tokens[\"prompt_attention_mask\"] = [1] + chosen_tokens[\"prompt_attention_mask\"]\n","        rejected_tokens[\"prompt_attention_mask\"] = [1] + rejected_tokens[\"prompt_attention_mask\"]\n","\n","        # add EOS token to end of answer\n","        chosen_tokens[\"input_ids\"].append(tokenizer.eos_token_id)\n","        chosen_tokens[\"attention_mask\"].append(1)\n","        rejected_tokens[\"input_ids\"].append(tokenizer.eos_token_id)\n","        rejected_tokens[\"attention_mask\"].append(1)\n","\n","        longer_response_length = max(len(chosen_tokens[\"input_ids\"]), len(rejected_tokens[\"input_ids\"]))\n","\n","        # if combined sequence is too long, truncate the prompt\n","        for answer_tokens in [chosen_tokens, rejected_tokens, prompt_tokens]:\n","            if len(answer_tokens[\"prompt_input_ids\"]) + longer_response_length > max_length:\n","                if truncation_mode == \"keep_start\":\n","                    for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n","                        answer_tokens[k] = answer_tokens[k][: max_prompt_length]\n","                elif truncation_mode == \"keep_end\":\n","                    for k in [\"prompt_input_ids\", \"prompt_attention_mask\"]:\n","                        answer_tokens[k] = answer_tokens[k][-max_prompt_length: ]\n","\n","        # if that's still too long, truncate the response\n","        for answer_tokens in [chosen_tokens, rejected_tokens]:\n","            if len(answer_tokens[\"prompt_input_ids\"]) + longer_response_length > max_length:\n","                for k in [\"input_ids\", \"attention_mask\"]:\n","                    answer_tokens[k] = answer_tokens[k][: max_length - max_prompt_length]\n","\n","        # combine prompt_input_ids and input_ids\n","        chosen_sequence_tokens = {\n","            k: chosen_tokens[f\"prompt_{k}\"] + chosen_tokens[k] for k in [\"input_ids\", \"attention_mask\"]\n","        }\n","        rejected_sequence_tokens = {\n","            k: rejected_tokens[f\"prompt_{k}\"] + rejected_tokens[k] for k in [\"input_ids\", \"attention_mask\"]\n","        }\n","\n","        # create labels: -100 for prompt token (to be ignored during loss)\n","        chosen_sequence_tokens[\"labels\"] = chosen_sequence_tokens[\"input_ids\"][:]\n","        chosen_sequence_tokens[\"labels\"][: len(chosen_tokens[\"prompt_input_ids\"])] = [-100] * \\\n","            len(chosen_tokens[\"prompt_input_ids\"])\n","\n","        rejected_sequence_tokens[\"labels\"] = rejected_sequence_tokens[\"input_ids\"][:]\n","        rejected_sequence_tokens[\"labels\"][: len(rejected_tokens[\"prompt_input_ids\"])] = [-100] * \\\n","            len(rejected_tokens[\"prompt_input_ids\"])\n","\n","        # combine all:\n","        # 'chosen_input_ids', 'chosen_attention_mask', 'chosen_labels'\n","        # 'rejected_input_ids', 'rejected_attention_mask', 'rejected_labels'\n","        # 'prompt_input_ids', 'prompt_attention_mask'\n","\n","        for k, toks in {\n","            \"chosen_\": chosen_sequence_tokens,\n","            \"rejected_\": rejected_sequence_tokens,\n","            \"\": prompt_tokens,\n","        }.items():\n","            for type_key, tokens in toks.items():\n","                if type_key == \"token_type_ids\":\n","                    continue\n","                batch[f\"{k}{type_key}\"] = tokens\n","\n","    else:\n","        chosen_tokens = tokenizer(\n","            chosen, truncation=True, max_length=max_target_length, add_special_tokens=True\n","        )\n","        rejected_tokens = tokenizer(\n","            rejected, truncation=True, max_length=max_target_length, add_special_tokens=True\n","        )\n","        prompt_tokens = tokenizer(\n","            prompt, truncation=True, max_length=max_prompt_length, add_special_tokens=True\n","        )\n","\n","        batch[\"chosen_labels\"] = chosen_tokens[\"input_ids\"]\n","        batch[\"rejected_labels\"] = rejected_tokens[\"input_ids\"]\n","        batch[\"prompt_input_ids\"] = prompt_tokens[\"input_ids\"]\n","        batch[\"prompt_attention_mask\"] = prompt_tokens[\"attention_mask\"]\n","\n","        if model is not None and hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n","            batch[\"rejected_decoder_input_ids\"] = model.prepare_decoder_input_ids_from_labels(\n","                labels=batch[\"rejected_labels\"]\n","            )\n","            batch[\"chosen_decoder_input_ids\"] = model.prepare_decoder_input_ids_from_labels(\n","                labels=batch[\"chosen_labels\"]\n","            )\n","\n","    return batch"],"metadata":{"id":"ftiK_mVnoB49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_dataset = train_dataset.map(\n","    tokenize_row,\n","    fn_kwargs={\n","        \"tokenizer\": tokenizer,\n","        \"max_length\": 1024,\n","        \"is_encoder_decoder\" : False,\n","        \"truncation_mode\" : \"keep_end\",\n","        \"max_prompt_length\": 512,\n","        \"max_target_length\": 512\n","    },\n",")\n","\n","for k, v in processed_dataset[1].items():\n","    print(k, '\\n', v)\n","    print('\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-bF83vKoCCl","executionInfo":{"status":"ok","timestamp":1704352981305,"user_tz":-480,"elapsed":31,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"0458476d-7ec4-4e43-9eed-69157ef2b518"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["chosen \n"," Sure, I'd be happy to help you with that! Here are 25 engaging and nutritious activities specifically designed for children throughout the long, hot months of summer:\n","\n","1. Swimming: Swimming is a great activity for kids during the summer months. It's a great way to cool off, get some exercise, and have fun. You can take your kids to a local pool, beach, or water park. Just make sure to pack plenty of sunscreen and water.\n","2. Hiking: Hiking is a great way to get some exercise and enjoy the outdoors. Look for local hiking trails that are suitable for children and take plenty of water and snacks.\n","3. Gardening: Gardening is a great way to get kids outside and teach them about nature. You can plant fruits and vegetables, or even start a small herb garden.\n","4. Cooking Classes: Cooking classes are a great way to teach kids about healthy eating habits and how to prepare nutritious meals. Look for local cooking classes or camps that cater to children.\n","5. Arts and Crafts: Arts and crafts are a great way to keep kids engaged and creative during the summer months. You can set up a craft station at home or look for local art classes or camps.\n","6. Team Sports: Team sports are a great way to get kids moving and socializing. Look for local sports leagues or camps that cater to children.\n","7. Yoga: Yoga is a great way to help kids develop flexibility, strength, and mindfulness. Look for local yoga classes or camps that cater to children.\n","8. Dance Camp: Dance camp is a great way to get kids moving and expressing themselves. Look for local dance camps or classes that cater to children.\n","9. Outdoor Games: Outdoor games like capture the flag, soccer, and frisbee are a great way to get kids moving and having fun.\n","10. Scavenger Hunts: Scavenger hunts are a great way to keep kids engaged and active during the summer months. You can create your own scavenger hunt at home or look for local scavenger hunt events.\n","11. Nature Walks: Nature walks are a great way to get kids outside and teach them about nature. Look for local nature trails and bring along a guidebook to help identify plants and animals.\n","12. Water Balloon Fights: Water balloon fights are a fun way to help kids cool off during the summer months. Just be sure to use biodegradable balloons and avoid throwing them at people's faces.\n","13. Kite Flying: Kite flying is a great way to get kids outside and enjoy the wind. Look for local parks or open spaces that are suitable for kite flying.\n","14. Obstacle Courses: Obstacle courses are a great way to get kids moving and challenged. You can set up your own obstacle course at home or look for local obstacle course races or camps.\n","15. Board Game Olympics: Board game Olympics are a great way to keep kids engaged and active during the summer months. You can set up a board game tournament at home or look for local board game events.\n","16. Water Play: Water play is a great way to help kids cool off during the summer months. You can set up a water table or splash pad at home or visit a local water park or beach.\n","17. STEM Activities: STEM activities are a great way to keep kids engaged and learning during the summer months. Look for local STEM camps or classes that cater to children.\n","18. Book Club: A book club is a great way to keep kids reading and engaged during the summer months. You can start a book club at home or look for local book clubs that cater to children.\n","19. photography: Photography is a great way to help kids develop their creativity and document their summer experiences. Look for local photography classes or camps that cater to children.\n","20. Outdoor Movie Nights: Outdoor movie nights are a great way to get kids outside and enjoying the summer evenings. You can set up an outdoor movie screen at home or look for local outdoor movie events.\n","21. Fishing: Fishing is a great way to get kids outside and teach them about nature. Look for local fishing spots and be sure to follow all safety guidelines.\n","22. Bug Hunts: Bug hunts are a great way<|endoftext|>\n","\n","\n","\n","rejected \n"," 1. Swimming: A fun, refreshing activity that promotes cardiovascular health and endurance, as well as flexibility and strength. Ensure children wear appropriate swim gear and closely supervise them around water.\n","2. Hiking: A great opportunity to explore nature, improve balance and coordination, and learn about local flora and fauna. Provide sturdy, comfortable footwear and pack plenty of water and snacks.\n","3. Gardening: An enriching activity that instills curiosity and appreciation for nature, while also encouraging physical activity and healthy eating habits. Provide gloves, hats, and tools, along with a plot of land (in a garden or pot) and seeds/plants.\n","4. Cooking classes: A wonderful way to develop culinary skills, creativity, and healthy eating habits. Children should be supervised by a responsible adult and have access to appropriate kitchen equipment and ingredients.\n","5. Arts and crafts: A versatile activity that sparks imagination and self-expression while improving fine motor skills and hand-eye coordination. Provide art materials such as paint, markers, paper, and clay, along with safe workspaces.\n","6. Team sports: A sociable, team-oriented activity that promotes physical fitness, social skills, and sportsmanship. Ensure proper equipment and supervision, and consider age-appropriate rules and skill levels.\n","7. Yoga: A gentle, mindful activity that enhances flexibility, balance, and mental wellbeing. Provide comfortable mats and engage a qualified instructor, ideally one with experience working with children.\n","8. Dance: A fun, expressive activity that helps children develop rhythm, coordination, and confidence. Offer age-appropriate music and movements, along with a safe, uncluttered space to move and a responsible adult supervisor.\n","9. Martial arts: A discipline-building activity that promotes strength, coordination, and concentration. Provide comfortable clothing and equipment, and engage a professional instructor familiar with working with children.\n","10. Running: An excellent cardio workout that increases endurance, improves heart and lung health, and fosters a sense of achievement. Encourage children to set and reach personal goals and provide appropriate shoes and clothing.\n","11. Biking: A fun, adventurous activity that strengthens leg muscles, improves balance, and burns calories. Provide bicycles that fit the chosen terrain, wear helmets, and supervise children.\n","12. Outdoor games: A traditional, interactive activity that fosters teamwork and socialization, such as tag, hide-and-seek, frisbee or soccer. Select games appropriate for the child's age and skill level and supervise accordingly.\n","13. Music lessons: A stimulating and enriching activity that promotes brain development, fine motor skills, and self-expression. Find a qualified, age-appropriate music instructor, and ensure proper equipment like a keyboard or guitar for practice.\n","14. Sailing or boating: A water-based activity that develops coordination, balance, and motor skills while promoting relaxation and fun. Ensure proper sailing instruction and the use of life jackets in boat or sailing situations.\n","15. Rock climbing: An endurance-building, problem-solving activity that strengthens upper body muscles and fosters perseverance and teamwork. Hire a certified guide for safe climbing practices and provide proper harnesses and equipment.\n","16. Martial arts-inspired exercises: A range of fun yet challenging physical activities designed to increase strength, agility, and balance such as zen gardens, parks, or playgrounds with fitness-related structures.\n","17. Nature or eco-exploration: Hiking, nature hunting or bird watching to learn about local flora, fauna, or geology while getting exercise and developing observational and scientific skills.\n","18. Team cooking: Similar to cooking classes but with a hands-on approach suitable for a social activity. Assign tasks and roles for each participant under the supervision of a responsible adult.\n","19. Beach or lake time: An idyllic summertime activity that combines water play and sunning with beneficial physical activity. Ensure proper sun protection and water safety gear.\n","20. Dance group lessons: A group-based activity that fosters a sense of community, creativity, and teamwork, promoting physical fitness and mental well<|endoftext|>\n","\n","\n","\n","prompt \n"," <|system|>\n","<|endoftext|>\n","<|user|>\n","Using a range of physical activities and creative pursuits, and taking into account factors such as age, gender, and locality, develop a comprehensive list of 25 engaging and nutritious activities specifically designed for children throughout the long, hot months of summer. Consider activities such as swimming, hiking, gardening, cooking classes, arts and crafts, and team sports, and ensure that each suggestion promotes healthy habits and positive attitudes towards exercise and wellbeing. Be sure to provide detailed explanations for each activity, outlining any supplies or props required, necessary supervision, and potential hazards or precautions.<|endoftext|>\n","<|assistant|>\n","\n","\n","\n","chosen_input_ids \n"," [50256, 27, 91, 10057, 91, 29, 198, 50256, 198, 27, 91, 7220, 91, 29, 198, 12814, 257, 2837, 286, 3518, 4568, 290, 7325, 45968, 11, 290, 2263, 656, 1848, 5087, 884, 355, 2479, 11, 5279, 11, 290, 48036, 11, 1205, 257, 9815, 1351, 286, 1679, 11932, 290, 48102, 4568, 5734, 3562, 329, 1751, 3690, 262, 890, 11, 3024, 1933, 286, 3931, 13, 12642, 4568, 884, 355, 14899, 11, 24522, 11, 46072, 11, 10801, 6097, 11, 10848, 290, 28229, 11, 290, 1074, 5701, 11, 290, 4155, 326, 1123, 13052, 21068, 5448, 13870, 290, 3967, 14479, 3371, 5517, 290, 40013, 13, 1355, 1654, 284, 2148, 6496, 18681, 329, 1123, 3842, 11, 36995, 597, 9416, 393, 25744, 2672, 11, 3306, 20865, 11, 290, 2785, 27491, 393, 31320, 13, 50256, 198, 27, 91, 562, 10167, 91, 29, 198, 19457, 11, 314, 1549, 307, 3772, 284, 1037, 345, 351, 326, 0, 3423, 389, 1679, 11932, 290, 48102, 4568, 5734, 3562, 329, 1751, 3690, 262, 890, 11, 3024, 1933, 286, 3931, 25, 198, 198, 16, 13, 2451, 27428, 25, 2451, 27428, 318, 257, 1049, 3842, 329, 3988, 1141, 262, 3931, 1933, 13, 632, 338, 257, 1049, 835, 284, 3608, 572, 11, 651, 617, 5517, 11, 290, 423, 1257, 13, 921, 460, 1011, 534, 3988, 284, 257, 1957, 5933, 11, 10481, 11, 393, 1660, 3952, 13, 2329, 787, 1654, 284, 2353, 6088, 286, 44221, 290, 1660, 13, 198, 17, 13, 367, 14132, 25, 367, 14132, 318, 257, 1049, 835, 284, 651, 617, 5517, 290, 2883, 262, 24349, 13, 6803, 329, 1957, 24522, 19196, 326, 389, 11080, 329, 1751, 290, 1011, 6088, 286, 1660, 290, 28487, 13, 198, 18, 13, 12790, 3101, 25, 12790, 3101, 318, 257, 1049, 835, 284, 651, 3988, 2354, 290, 4545, 606, 546, 3450, 13, 921, 460, 4618, 15921, 290, 13701, 11, 393, 772, 923, 257, 1402, 21047, 11376, 13, 198, 19, 13, 33774, 38884, 25, 33774, 6097, 389, 257, 1049, 835, 284, 4545, 3988, 546, 5448, 6600, 13870, 290, 703, 284, 8335, 48102, 13840, 13, 6803, 329, 1957, 10801, 6097, 393, 11665, 326, 20825, 284, 1751, 13, 198, 20, 13, 11536, 290, 15745, 82, 25, 11536, 290, 28229, 389, 257, 1049, 835, 284, 1394, 3988, 7953, 290, 7325, 1141, 262, 3931, 1933, 13, 921, 460, 900, 510, 257, 5977, 4429, 379, 1363, 393, 804, 329, 1957, 1242, 6097, 393, 11665, 13, 198, 21, 13, 4816, 7092, 25, 4816, 5701, 389, 257, 1049, 835, 284, 651, 3988, 3867, 290, 1919, 2890, 13, 6803, 329, 1957, 5701, 16861, 393, 11665, 326, 20825, 284, 1751, 13, 198, 22, 13, 32856, 25, 32856, 318, 257, 1049, 835, 284, 1037, 3988, 1205, 13688, 11, 4202, 11, 290, 35502, 13, 6803, 329, 1957, 20351, 6097, 393, 11665, 326, 20825, 284, 1751, 13, 198, 23, 13, 17349, 5425, 25, 17349, 1413, 318, 257, 1049, 835, 284, 651, 3988, 3867, 290, 16621, 2405, 13, 6803, 329, 1957, 9280, 11665, 393, 6097, 326, 20825, 284, 1751, 13, 198, 24, 13, 43047, 5776, 25, 43047, 1830, 588, 8006, 262, 6056, 11, 11783, 11, 290, 1216, 271, 20963, 389, 257, 1049, 835, 284, 651, 3988, 3867, 290, 1719, 1257, 13, 198, 940, 13, 1446, 4005, 1362, 31926, 25, 1446, 4005, 1362, 42984, 389, 257, 1049, 835, 284, 1394, 3988, 7953, 290, 4075, 1141, 262, 3931, 1933, 13, 921, 460, 2251, 534, 898, 35998, 1362, 12601, 379, 1363, 393, 804, 329, 1957, 35998, 1362, 12601, 2995, 13, 198, 1157, 13, 10362, 6445, 591, 25, 10362, 11114, 389, 257, 1049, 835, 284, 651, 3988, 2354, 290, 4545, 606, 546, 3450, 13, 6803, 329, 1957, 3450, 19196, 290, 2222, 1863, 257, 5698, 2070, 284, 1037, 5911, 6134, 290, 4695, 13, 198, 1065, 13, 5638, 47821, 376, 2337, 25, 5638, 21190, 11418, 389, 257, 1257, 835, 284, 1037, 3988, 3608, 572, 1141, 262, 3931, 1933, 13, 2329, 307, 1654, 284, 779, 3182, 1098, 9744, 540, 35485, 290, 3368, 9644, 606, 379, 661, 338, 6698, 13, 198, 1485, 13, 509, 578, 19903, 25, 509, 578, 7348, 318, 257, 1049, 835, 284, 651, 3988, 2354, 290, 2883, 262, 2344, 13, 6803, 329, 1957, 14860, 393, 1280, 9029, 326, 389, 11080, 329, 479, 578, 7348, 13, 198, 1415, 13, 46378, 6008, 2734, 8448, 25, 46378, 6008, 10902, 389, 257, 1049, 835, 284, 651, 3988, 3867, 290, 12827, 13, 921, 460, 900, 510, 534, 898, 22007, 1781, 379, 1363, 393, 804, 329, 1957, 22007, 1781, 9558, 393, 11665, 13, 198, 1314, 13, 5926, 3776, 14935, 25, 5926, 983, 14935, 389, 257, 1049, 835, 284, 1394, 3988, 7953, 290, 4075, 1141, 262, 3931, 1933, 13, 921, 460, 900, 510, 257, 3096, 983, 7756, 379, 1363, 393, 804, 329, 1957, 3096, 983, 2995, 13, 198, 1433, 13, 5638, 3811, 25, 5638, 711, 318, 257, 1049, 835, 284, 1037, 3988, 3608, 572, 1141, 262, 3931, 1933, 13, 921, 460, 900, 510, 257, 1660, 3084, 393, 22870, 14841, 379, 1363, 393, 3187, 257, 1957, 1660, 3952, 393, 10481, 13, 198, 1558, 13, 34133, 36270, 25, 34133, 4568, 389, 257, 1049, 835, 284, 1394, 3988, 7953, 290, 4673, 1141, 262, 3931, 1933, 13, 6803, 329, 1957, 34133, 11665, 393, 6097, 326, 20825, 284, 1751, 13, 198, 1507, 13, 4897, 6289, 25, 317, 1492, 3430, 318, 257, 1049, 835, 284, 1394, 3988, 3555, 290, 7953, 1141, 262, 3931, 1933, 13, 921, 460, 923, 257, 1492, 3430, 379, 1363, 393, 804, 329, 1957, 1492, 9784, 326, 20825, 284, 1751, 13, 198, 1129, 13, 16223, 25, 32461, 318, 257, 1049, 835, 284, 1037, 3988, 1205, 511, 16389, 290, 3188, 511, 3931, 6461, 13, 6803, 329, 1957, 16223, 6097, 393, 11665, 326, 20825, 284, 1751, 13, 198, 1238, 13, 43047, 15875, 27760, 25, 43047, 3807, 12513, 389, 257, 1049, 835, 284, 651, 3988, 2354, 290, 13226, 262, 3931, 37119, 13, 921, 460, 900, 510, 281, 15162, 3807, 3159, 379, 1363, 393, 804, 329, 1957, 15162, 3807, 2995, 13, 198, 2481, 13, 28944, 25, 28944, 318, 257, 1049, 835, 284, 651, 3988, 2354, 290, 4545, 606, 546, 3450, 13, 6803, 329, 1957, 12478, 10222, 290, 307, 1654, 284, 1061, 477, 3747, 9949, 13, 198, 1828, 13, 15217, 31926, 25, 15217, 42984, 389, 257, 1049, 835, 50256, 198, 50256]\n","\n","\n","chosen_attention_mask \n"," [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","\n","chosen_labels \n"," [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 19457, 11, 314, 1549, 307, 3772, 284, 1037, 345, 351, 326, 0, 3423, 389, 1679, 11932, 290, 48102, 4568, 5734, 3562, 329, 1751, 3690, 262, 890, 11, 3024, 1933, 286, 3931, 25, 198, 198, 16, 13, 2451, 27428, 25, 2451, 27428, 318, 257, 1049, 3842, 329, 3988, 1141, 262, 3931, 1933, 13, 632, 338, 257, 1049, 835, 284, 3608, 572, 11, 651, 617, 5517, 11, 290, 423, 1257, 13, 921, 460, 1011, 534, 3988, 284, 257, 1957, 5933, 11, 10481, 11, 393, 1660, 3952, 13, 2329, 787, 1654, 284, 2353, 6088, 286, 44221, 290, 1660, 13, 198, 17, 13, 367, 14132, 25, 367, 14132, 318, 257, 1049, 835, 284, 651, 617, 5517, 290, 2883, 262, 24349, 13, 6803, 329, 1957, 24522, 19196, 326, 389, 11080, 329, 1751, 290, 1011, 6088, 286, 1660, 290, 28487, 13, 198, 18, 13, 12790, 3101, 25, 12790, 3101, 318, 257, 1049, 835, 284, 651, 3988, 2354, 290, 4545, 606, 546, 3450, 13, 921, 460, 4618, 15921, 290, 13701, 11, 393, 772, 923, 257, 1402, 21047, 11376, 13, 198, 19, 13, 33774, 38884, 25, 33774, 6097, 389, 257, 1049, 835, 284, 4545, 3988, 546, 5448, 6600, 13870, 290, 703, 284, 8335, 48102, 13840, 13, 6803, 329, 1957, 10801, 6097, 393, 11665, 326, 20825, 284, 1751, 13, 198, 20, 13, 11536, 290, 15745, 82, 25, 11536, 290, 28229, 389, 257, 1049, 835, 284, 1394, 3988, 7953, 290, 7325, 1141, 262, 3931, 1933, 13, 921, 460, 900, 510, 257, 5977, 4429, 379, 1363, 393, 804, 329, 1957, 1242, 6097, 393, 11665, 13, 198, 21, 13, 4816, 7092, 25, 4816, 5701, 389, 257, 1049, 835, 284, 651, 3988, 3867, 290, 1919, 2890, 13, 6803, 329, 1957, 5701, 16861, 393, 11665, 326, 20825, 284, 1751, 13, 198, 22, 13, 32856, 25, 32856, 318, 257, 1049, 835, 284, 1037, 3988, 1205, 13688, 11, 4202, 11, 290, 35502, 13, 6803, 329, 1957, 20351, 6097, 393, 11665, 326, 20825, 284, 1751, 13, 198, 23, 13, 17349, 5425, 25, 17349, 1413, 318, 257, 1049, 835, 284, 651, 3988, 3867, 290, 16621, 2405, 13, 6803, 329, 1957, 9280, 11665, 393, 6097, 326, 20825, 284, 1751, 13, 198, 24, 13, 43047, 5776, 25, 43047, 1830, 588, 8006, 262, 6056, 11, 11783, 11, 290, 1216, 271, 20963, 389, 257, 1049, 835, 284, 651, 3988, 3867, 290, 1719, 1257, 13, 198, 940, 13, 1446, 4005, 1362, 31926, 25, 1446, 4005, 1362, 42984, 389, 257, 1049, 835, 284, 1394, 3988, 7953, 290, 4075, 1141, 262, 3931, 1933, 13, 921, 460, 2251, 534, 898, 35998, 1362, 12601, 379, 1363, 393, 804, 329, 1957, 35998, 1362, 12601, 2995, 13, 198, 1157, 13, 10362, 6445, 591, 25, 10362, 11114, 389, 257, 1049, 835, 284, 651, 3988, 2354, 290, 4545, 606, 546, 3450, 13, 6803, 329, 1957, 3450, 19196, 290, 2222, 1863, 257, 5698, 2070, 284, 1037, 5911, 6134, 290, 4695, 13, 198, 1065, 13, 5638, 47821, 376, 2337, 25, 5638, 21190, 11418, 389, 257, 1257, 835, 284, 1037, 3988, 3608, 572, 1141, 262, 3931, 1933, 13, 2329, 307, 1654, 284, 779, 3182, 1098, 9744, 540, 35485, 290, 3368, 9644, 606, 379, 661, 338, 6698, 13, 198, 1485, 13, 509, 578, 19903, 25, 509, 578, 7348, 318, 257, 1049, 835, 284, 651, 3988, 2354, 290, 2883, 262, 2344, 13, 6803, 329, 1957, 14860, 393, 1280, 9029, 326, 389, 11080, 329, 479, 578, 7348, 13, 198, 1415, 13, 46378, 6008, 2734, 8448, 25, 46378, 6008, 10902, 389, 257, 1049, 835, 284, 651, 3988, 3867, 290, 12827, 13, 921, 460, 900, 510, 534, 898, 22007, 1781, 379, 1363, 393, 804, 329, 1957, 22007, 1781, 9558, 393, 11665, 13, 198, 1314, 13, 5926, 3776, 14935, 25, 5926, 983, 14935, 389, 257, 1049, 835, 284, 1394, 3988, 7953, 290, 4075, 1141, 262, 3931, 1933, 13, 921, 460, 900, 510, 257, 3096, 983, 7756, 379, 1363, 393, 804, 329, 1957, 3096, 983, 2995, 13, 198, 1433, 13, 5638, 3811, 25, 5638, 711, 318, 257, 1049, 835, 284, 1037, 3988, 3608, 572, 1141, 262, 3931, 1933, 13, 921, 460, 900, 510, 257, 1660, 3084, 393, 22870, 14841, 379, 1363, 393, 3187, 257, 1957, 1660, 3952, 393, 10481, 13, 198, 1558, 13, 34133, 36270, 25, 34133, 4568, 389, 257, 1049, 835, 284, 1394, 3988, 7953, 290, 4673, 1141, 262, 3931, 1933, 13, 6803, 329, 1957, 34133, 11665, 393, 6097, 326, 20825, 284, 1751, 13, 198, 1507, 13, 4897, 6289, 25, 317, 1492, 3430, 318, 257, 1049, 835, 284, 1394, 3988, 3555, 290, 7953, 1141, 262, 3931, 1933, 13, 921, 460, 923, 257, 1492, 3430, 379, 1363, 393, 804, 329, 1957, 1492, 9784, 326, 20825, 284, 1751, 13, 198, 1129, 13, 16223, 25, 32461, 318, 257, 1049, 835, 284, 1037, 3988, 1205, 511, 16389, 290, 3188, 511, 3931, 6461, 13, 6803, 329, 1957, 16223, 6097, 393, 11665, 326, 20825, 284, 1751, 13, 198, 1238, 13, 43047, 15875, 27760, 25, 43047, 3807, 12513, 389, 257, 1049, 835, 284, 651, 3988, 2354, 290, 13226, 262, 3931, 37119, 13, 921, 460, 900, 510, 281, 15162, 3807, 3159, 379, 1363, 393, 804, 329, 1957, 15162, 3807, 2995, 13, 198, 2481, 13, 28944, 25, 28944, 318, 257, 1049, 835, 284, 651, 3988, 2354, 290, 4545, 606, 546, 3450, 13, 6803, 329, 1957, 12478, 10222, 290, 307, 1654, 284, 1061, 477, 3747, 9949, 13, 198, 1828, 13, 15217, 31926, 25, 15217, 42984, 389, 257, 1049, 835, 50256, 198, 50256]\n","\n","\n","rejected_input_ids \n"," [50256, 27, 91, 10057, 91, 29, 198, 50256, 198, 27, 91, 7220, 91, 29, 198, 12814, 257, 2837, 286, 3518, 4568, 290, 7325, 45968, 11, 290, 2263, 656, 1848, 5087, 884, 355, 2479, 11, 5279, 11, 290, 48036, 11, 1205, 257, 9815, 1351, 286, 1679, 11932, 290, 48102, 4568, 5734, 3562, 329, 1751, 3690, 262, 890, 11, 3024, 1933, 286, 3931, 13, 12642, 4568, 884, 355, 14899, 11, 24522, 11, 46072, 11, 10801, 6097, 11, 10848, 290, 28229, 11, 290, 1074, 5701, 11, 290, 4155, 326, 1123, 13052, 21068, 5448, 13870, 290, 3967, 14479, 3371, 5517, 290, 40013, 13, 1355, 1654, 284, 2148, 6496, 18681, 329, 1123, 3842, 11, 36995, 597, 9416, 393, 25744, 2672, 11, 3306, 20865, 11, 290, 2785, 27491, 393, 31320, 13, 50256, 198, 27, 91, 562, 10167, 91, 29, 198, 16, 13, 2451, 27428, 25, 317, 1257, 11, 23056, 3842, 326, 21068, 21134, 1535, 290, 22965, 11, 355, 880, 355, 13688, 290, 4202, 13, 48987, 1751, 5806, 5035, 9422, 7733, 290, 7173, 29745, 786, 606, 1088, 1660, 13, 198, 17, 13, 367, 14132, 25, 317, 1049, 3663, 284, 7301, 3450, 11, 2987, 5236, 290, 19877, 11, 290, 2193, 546, 1957, 48037, 290, 277, 32837, 13, 44290, 28055, 11, 6792, 43463, 290, 2353, 6088, 286, 1660, 290, 28487, 13, 198, 18, 13, 12790, 3101, 25, 1052, 22465, 278, 3842, 326, 916, 2171, 20136, 290, 19163, 329, 3450, 11, 981, 635, 12577, 3518, 3842, 290, 5448, 6600, 13870, 13, 44290, 18051, 11, 23910, 11, 290, 4899, 11, 1863, 351, 257, 7110, 286, 1956, 357, 259, 257, 11376, 393, 1787, 8, 290, 11904, 14, 489, 1187, 13, 198, 19, 13, 33774, 6097, 25, 317, 7932, 835, 284, 1205, 35956, 4678, 11, 16389, 11, 290, 5448, 6600, 13870, 13, 8990, 815, 307, 28679, 416, 257, 4497, 4044, 290, 423, 1895, 284, 5035, 9592, 5112, 290, 9391, 13, 198, 20, 13, 11536, 290, 28229, 25, 317, 21362, 3842, 326, 38306, 13843, 290, 2116, 12, 38011, 981, 10068, 3734, 5584, 4678, 290, 1021, 12, 25379, 19877, 13, 44290, 1242, 5696, 884, 355, 7521, 11, 19736, 11, 3348, 11, 290, 21558, 11, 1863, 351, 3338, 2499, 43076, 13, 198, 21, 13, 4816, 5701, 25, 317, 1307, 3379, 11, 1074, 12, 17107, 3842, 326, 21068, 3518, 13547, 11, 1919, 4678, 11, 290, 5701, 25428, 13, 48987, 1774, 5112, 290, 20865, 11, 290, 2074, 2479, 12, 13335, 3173, 290, 5032, 2974, 13, 198, 22, 13, 32856, 25, 317, 10296, 11, 33798, 3842, 326, 32479, 13688, 11, 5236, 11, 290, 5110, 40013, 13, 44290, 6792, 46054, 290, 8209, 257, 10617, 21187, 11, 30274, 530, 351, 1998, 1762, 351, 1751, 13, 198, 23, 13, 17349, 25, 317, 1257, 11, 38084, 3842, 326, 5419, 1751, 1205, 18662, 11, 19877, 11, 290, 6628, 13, 33085, 2479, 12, 13335, 2647, 290, 8650, 11, 1863, 351, 257, 3338, 11, 555, 565, 46322, 2272, 284, 1445, 290, 257, 4497, 4044, 21277, 13, 198, 24, 13, 28526, 10848, 25, 317, 12883, 12, 16894, 3842, 326, 21068, 4202, 11, 19877, 11, 290, 10368, 13, 44290, 6792, 9528, 290, 5112, 11, 290, 8209, 257, 4708, 21187, 5385, 351, 1762, 351, 1751, 13, 198, 940, 13, 18162, 25, 1052, 6275, 47926, 17578, 326, 5732, 22965, 11, 19575, 2612, 290, 12317, 1535, 11, 290, 26546, 364, 257, 2565, 286, 13293, 13, 14711, 32885, 1751, 284, 900, 290, 3151, 2614, 4661, 290, 2148, 5035, 10012, 290, 9528, 13, 198, 1157, 13, 347, 14132, 25, 317, 1257, 11, 39770, 3842, 326, 49286, 1232, 12749, 11, 19575, 5236, 11, 290, 20246, 14653, 13, 44290, 35426, 326, 4197, 262, 7147, 15510, 11, 5806, 28359, 11, 290, 29745, 786, 1751, 13, 198, 1065, 13, 43047, 1830, 25, 317, 4569, 11, 14333, 3842, 326, 26546, 364, 48424, 290, 1919, 1634, 11, 884, 355, 7621, 11, 7808, 12, 392, 12, 36163, 11, 1216, 271, 20963, 393, 11783, 13, 9683, 1830, 5035, 329, 262, 1200, 338, 2479, 290, 5032, 1241, 290, 29745, 786, 16062, 13, 198, 1485, 13, 7849, 11658, 25, 317, 35640, 290, 22465, 278, 3842, 326, 21068, 3632, 2478, 11, 3734, 5584, 4678, 11, 290, 2116, 12, 38011, 13, 9938, 257, 10617, 11, 2479, 12, 13335, 2647, 21187, 11, 290, 4155, 1774, 5112, 588, 257, 10586, 393, 10047, 329, 3357, 13, 198, 1415, 13, 311, 11608, 393, 1489, 803, 25, 317, 1660, 12, 3106, 3842, 326, 21126, 19877, 11, 5236, 11, 290, 5584, 4678, 981, 11560, 34205, 290, 1257, 13, 48987, 1774, 28499, 12064, 290, 262, 779, 286, 1204, 35129, 287, 8848, 393, 28499, 7445, 13, 198, 1314, 13, 4631, 14281, 25, 1052, 22965, 12, 16894, 11, 1917, 12, 82, 10890, 3842, 326, 49286, 6727, 1767, 12749, 290, 26546, 364, 45712, 590, 290, 48424, 13, 367, 557, 257, 15704, 5698, 329, 3338, 14281, 6593, 290, 2148, 1774, 19356, 274, 290, 5112, 13, 198, 1433, 13, 28526, 10848, 12, 24194, 13565, 25, 317, 2837, 286, 1257, 1865, 9389, 3518, 4568, 3562, 284, 2620, 4202, 11, 33546, 11, 290, 5236, 884, 355, 1976, 268, 25476, 11, 14860, 11, 393, 24817, 82, 351, 13547, 12, 5363, 8573, 13, 198, 1558, 13, 10362, 393, 34286, 12, 20676, 6944, 25, 367, 14132, 11, 3450, 10988, 393, 6512, 4964, 284, 2193, 546, 1957, 48037, 11, 277, 32837, 11, 393, 4903, 1435, 981, 1972, 5517, 290, 5922, 40118, 290, 5654, 4678, 13, 198, 1507, 13, 4816, 10801, 25, 11014, 284, 10801, 6097, 475, 351, 257, 2832, 12, 261, 3164, 11080, 329, 257, 1919, 3842, 13, 2195, 570, 8861, 290, 9176, 329, 1123, 18399, 739, 262, 20865, 286, 257, 4497, 4044, 13, 198, 1129, 13, 8511, 393, 13546, 640, 25, 1052, 4686, 25727, 291, 3931, 2435, 3842, 326, 21001, 1660, 711, 290, 4252, 768, 351, 13205, 3518, 3842, 13, 48987, 1774, 4252, 4800, 290, 1660, 3747, 7733, 13, 198, 1238, 13, 17349, 1448, 11658, 25, 317, 1448, 12, 3106, 3842, 326, 26546, 364, 257, 2565, 286, 2055, 11, 16389, 11, 290, 48424, 11, 11560, 3518, 13547, 290, 5110, 880, 50256, 198, 50256]\n","\n","\n","rejected_attention_mask \n"," [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","\n","rejected_labels \n"," [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16, 13, 2451, 27428, 25, 317, 1257, 11, 23056, 3842, 326, 21068, 21134, 1535, 290, 22965, 11, 355, 880, 355, 13688, 290, 4202, 13, 48987, 1751, 5806, 5035, 9422, 7733, 290, 7173, 29745, 786, 606, 1088, 1660, 13, 198, 17, 13, 367, 14132, 25, 317, 1049, 3663, 284, 7301, 3450, 11, 2987, 5236, 290, 19877, 11, 290, 2193, 546, 1957, 48037, 290, 277, 32837, 13, 44290, 28055, 11, 6792, 43463, 290, 2353, 6088, 286, 1660, 290, 28487, 13, 198, 18, 13, 12790, 3101, 25, 1052, 22465, 278, 3842, 326, 916, 2171, 20136, 290, 19163, 329, 3450, 11, 981, 635, 12577, 3518, 3842, 290, 5448, 6600, 13870, 13, 44290, 18051, 11, 23910, 11, 290, 4899, 11, 1863, 351, 257, 7110, 286, 1956, 357, 259, 257, 11376, 393, 1787, 8, 290, 11904, 14, 489, 1187, 13, 198, 19, 13, 33774, 6097, 25, 317, 7932, 835, 284, 1205, 35956, 4678, 11, 16389, 11, 290, 5448, 6600, 13870, 13, 8990, 815, 307, 28679, 416, 257, 4497, 4044, 290, 423, 1895, 284, 5035, 9592, 5112, 290, 9391, 13, 198, 20, 13, 11536, 290, 28229, 25, 317, 21362, 3842, 326, 38306, 13843, 290, 2116, 12, 38011, 981, 10068, 3734, 5584, 4678, 290, 1021, 12, 25379, 19877, 13, 44290, 1242, 5696, 884, 355, 7521, 11, 19736, 11, 3348, 11, 290, 21558, 11, 1863, 351, 3338, 2499, 43076, 13, 198, 21, 13, 4816, 5701, 25, 317, 1307, 3379, 11, 1074, 12, 17107, 3842, 326, 21068, 3518, 13547, 11, 1919, 4678, 11, 290, 5701, 25428, 13, 48987, 1774, 5112, 290, 20865, 11, 290, 2074, 2479, 12, 13335, 3173, 290, 5032, 2974, 13, 198, 22, 13, 32856, 25, 317, 10296, 11, 33798, 3842, 326, 32479, 13688, 11, 5236, 11, 290, 5110, 40013, 13, 44290, 6792, 46054, 290, 8209, 257, 10617, 21187, 11, 30274, 530, 351, 1998, 1762, 351, 1751, 13, 198, 23, 13, 17349, 25, 317, 1257, 11, 38084, 3842, 326, 5419, 1751, 1205, 18662, 11, 19877, 11, 290, 6628, 13, 33085, 2479, 12, 13335, 2647, 290, 8650, 11, 1863, 351, 257, 3338, 11, 555, 565, 46322, 2272, 284, 1445, 290, 257, 4497, 4044, 21277, 13, 198, 24, 13, 28526, 10848, 25, 317, 12883, 12, 16894, 3842, 326, 21068, 4202, 11, 19877, 11, 290, 10368, 13, 44290, 6792, 9528, 290, 5112, 11, 290, 8209, 257, 4708, 21187, 5385, 351, 1762, 351, 1751, 13, 198, 940, 13, 18162, 25, 1052, 6275, 47926, 17578, 326, 5732, 22965, 11, 19575, 2612, 290, 12317, 1535, 11, 290, 26546, 364, 257, 2565, 286, 13293, 13, 14711, 32885, 1751, 284, 900, 290, 3151, 2614, 4661, 290, 2148, 5035, 10012, 290, 9528, 13, 198, 1157, 13, 347, 14132, 25, 317, 1257, 11, 39770, 3842, 326, 49286, 1232, 12749, 11, 19575, 5236, 11, 290, 20246, 14653, 13, 44290, 35426, 326, 4197, 262, 7147, 15510, 11, 5806, 28359, 11, 290, 29745, 786, 1751, 13, 198, 1065, 13, 43047, 1830, 25, 317, 4569, 11, 14333, 3842, 326, 26546, 364, 48424, 290, 1919, 1634, 11, 884, 355, 7621, 11, 7808, 12, 392, 12, 36163, 11, 1216, 271, 20963, 393, 11783, 13, 9683, 1830, 5035, 329, 262, 1200, 338, 2479, 290, 5032, 1241, 290, 29745, 786, 16062, 13, 198, 1485, 13, 7849, 11658, 25, 317, 35640, 290, 22465, 278, 3842, 326, 21068, 3632, 2478, 11, 3734, 5584, 4678, 11, 290, 2116, 12, 38011, 13, 9938, 257, 10617, 11, 2479, 12, 13335, 2647, 21187, 11, 290, 4155, 1774, 5112, 588, 257, 10586, 393, 10047, 329, 3357, 13, 198, 1415, 13, 311, 11608, 393, 1489, 803, 25, 317, 1660, 12, 3106, 3842, 326, 21126, 19877, 11, 5236, 11, 290, 5584, 4678, 981, 11560, 34205, 290, 1257, 13, 48987, 1774, 28499, 12064, 290, 262, 779, 286, 1204, 35129, 287, 8848, 393, 28499, 7445, 13, 198, 1314, 13, 4631, 14281, 25, 1052, 22965, 12, 16894, 11, 1917, 12, 82, 10890, 3842, 326, 49286, 6727, 1767, 12749, 290, 26546, 364, 45712, 590, 290, 48424, 13, 367, 557, 257, 15704, 5698, 329, 3338, 14281, 6593, 290, 2148, 1774, 19356, 274, 290, 5112, 13, 198, 1433, 13, 28526, 10848, 12, 24194, 13565, 25, 317, 2837, 286, 1257, 1865, 9389, 3518, 4568, 3562, 284, 2620, 4202, 11, 33546, 11, 290, 5236, 884, 355, 1976, 268, 25476, 11, 14860, 11, 393, 24817, 82, 351, 13547, 12, 5363, 8573, 13, 198, 1558, 13, 10362, 393, 34286, 12, 20676, 6944, 25, 367, 14132, 11, 3450, 10988, 393, 6512, 4964, 284, 2193, 546, 1957, 48037, 11, 277, 32837, 11, 393, 4903, 1435, 981, 1972, 5517, 290, 5922, 40118, 290, 5654, 4678, 13, 198, 1507, 13, 4816, 10801, 25, 11014, 284, 10801, 6097, 475, 351, 257, 2832, 12, 261, 3164, 11080, 329, 257, 1919, 3842, 13, 2195, 570, 8861, 290, 9176, 329, 1123, 18399, 739, 262, 20865, 286, 257, 4497, 4044, 13, 198, 1129, 13, 8511, 393, 13546, 640, 25, 1052, 4686, 25727, 291, 3931, 2435, 3842, 326, 21001, 1660, 711, 290, 4252, 768, 351, 13205, 3518, 3842, 13, 48987, 1774, 4252, 4800, 290, 1660, 3747, 7733, 13, 198, 1238, 13, 17349, 1448, 11658, 25, 317, 1448, 12, 3106, 3842, 326, 26546, 364, 257, 2565, 286, 2055, 11, 16389, 11, 290, 48424, 11, 11560, 3518, 13547, 290, 5110, 880, 50256, 198, 50256]\n","\n","\n","prompt_input_ids \n"," [50256, 27, 91, 10057, 91, 29, 198, 50256, 198, 27, 91, 7220, 91, 29, 198, 12814, 257, 2837, 286, 3518, 4568, 290, 7325, 45968, 11, 290, 2263, 656, 1848, 5087, 884, 355, 2479, 11, 5279, 11, 290, 48036, 11, 1205, 257, 9815, 1351, 286, 1679, 11932, 290, 48102, 4568, 5734, 3562, 329, 1751, 3690, 262, 890, 11, 3024, 1933, 286, 3931, 13, 12642, 4568, 884, 355, 14899, 11, 24522, 11, 46072, 11, 10801, 6097, 11, 10848, 290, 28229, 11, 290, 1074, 5701, 11, 290, 4155, 326, 1123, 13052, 21068, 5448, 13870, 290, 3967, 14479, 3371, 5517, 290, 40013, 13, 1355, 1654, 284, 2148, 6496, 18681, 329, 1123, 3842, 11, 36995, 597, 9416, 393, 25744, 2672, 11, 3306, 20865, 11, 290, 2785, 27491, 393, 31320, 13, 50256, 198, 27, 91, 562, 10167, 91, 29, 198]\n","\n","\n","prompt_attention_mask \n"," [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","\n","\n"]}]},{"cell_type":"markdown","source":["## 3.4 DPO Data Collator"],"metadata":{"id":"cNqYyYMvTcKS"}},{"cell_type":"code","source":["from torch.nn.utils.rnn import pad_sequence\n","\n","@dataclass\n","class DPODataCollatorWithPadding:\n","\n","    def __init__(self, pad_token_id=0, label_pad_token_id=-100, is_encoder_decoder=False):\n","        self.pad_token_id = pad_token_id\n","        self.label_pad_token_id = label_pad_token_id\n","        self.is_encoder_decoder = is_encoder_decoder\n","\n","    def __call__(self, features):\n","\n","        # pad everything to the same length (max length)\n","        padded_batch = {}\n","        for k in features[0].keys():\n","\n","            if k.endswith(\"_input_ids\") or k.endswith(\"_attention_mask\") or k.endswith(\"_labels\"):\n","                if self.is_encoder_decoder:\n","                    to_pad = [torch.LongTensor(ex[k]) for ex in features]\n","\n","                    if (k.startswith(\"prompt\")) and (k.endswith(\"input_ids\")):\n","                        padding_value = tokenizer.pad_token_id\n","                    elif k.endswith(\"_attention_mask\"):\n","                        padding_value = 0\n","                    elif (k.startswith(\"chosen\")) or (k.startswith(\"rejected\")) or (\"decoder\" in k):\n","                        padding_value = self.label_pad_token_id\n","                    else:\n","                        raise ValueError(f\"Unexpected key in batch '{k}'\")\n","                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)\n","\n","                else:\n","                    # prompt needed to pad on left side\n","                    if \"prompt\" in k:\n","                        to_pad = [torch.LongTensor(ex[k][::-1]) for ex in features]\n","                    else:\n","                        to_pad = [torch.LongTensor(ex[k]) for ex in features]\n","\n","                    if k.endswith(\"_input_ids\"):\n","                        padding_value = self.pad_token_id\n","                    elif k.endswith(\"_labels\"):\n","                        padding_value = self.label_pad_token_id\n","                    elif k.endswith(\"_attention_mask\"):\n","                        padding_value = 0\n","                    else:\n","                        raise ValueError(f\"Unexpected key in batch '{k}'\")\n","\n","                    padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)\n","                    # for the prompt, flip back so padding is on left side\n","                    if \"prompt\" in k:\n","                        padded_batch[k] = padded_batch[k].flip(dims=[1])\n","\n","            elif k.endswith(\"_logps\"):\n","                # the cached reference model logprobs\n","                padded_batch[k] = torch.tensor([ex[k] for ex in features])\n","            else:\n","                padded_batch[k] = [ex[k] for ex in features]\n","\n","        return padded_batch"],"metadata":{"id":"S5px9uj6gscn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# the data collator will be integrated when creating data loader\n","data_collator = DPODataCollatorWithPadding(\n","    pad_token_id=tokenizer.pad_token_id,\n","    label_pad_token_id=-100,\n","    is_encoder_decoder=False,\n",")\n","test_set = data_collator(processed_dataset)\n","for k, v in test_set.items():\n","    print(k, '\\n', v[10], '\\n')"],"metadata":{"id":"7Je2cQITtYZf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704352981929,"user_tz":-480,"elapsed":641,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"5a76282d-bd1b-470d-a665-111449bbe64b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["chosen \n"," Sure, I'd be happy to help you with your questions about The Giver! Can you please provide more context or specify which aspects of the novel you would like to know more about? For instance, you might be interested in the plot, characters, themes, or historical context. I'll do my best to provide a helpful and informative response.<|endoftext|>\n"," \n","\n","rejected \n"," Of course! I'd be happy to help you with any questions you have about The Giver. Please go ahead and ask, and I'll do my best to provide accurate and helpful answers.<|endoftext|>\n"," \n","\n","prompt \n"," <|system|>\n","<|endoftext|>\n","<|user|>\n","I have some questions about The Giver.<|endoftext|>\n","<|assistant|>\n"," \n","\n","chosen_input_ids \n"," tensor([50256,    27,    91,  ..., 50256, 50256, 50256]) \n","\n","chosen_attention_mask \n"," tensor([1, 1, 1,  ..., 0, 0, 0]) \n","\n","chosen_labels \n"," tensor([-100, -100, -100,  ..., -100, -100, -100]) \n","\n","rejected_input_ids \n"," tensor([50256,    27,    91, 10057,    91,    29,   198, 50256,   198,    27,\n","           91,  7220,    91,    29,   198,    40,   423,   617,  2683,   546,\n","          383,   402,  1428,    13, 50256,   198,    27,    91,   562, 10167,\n","           91,    29,   198,  5189,  1781,     0,   314,  1549,   307,  3772,\n","          284,  1037,   345,   351,   597,  2683,   345,   423,   546,   383,\n","          402,  1428,    13,  4222,   467,  4058,   290,  1265,    11,   290,\n","          314,  1183,   466,   616,  1266,   284,  2148,  7187,   290,  7613,\n","         7429,    13, 50256,   198, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256]) \n","\n","rejected_attention_mask \n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \n","\n","rejected_labels \n"," tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  5189,  1781,     0,   314,  1549,   307,  3772,\n","          284,  1037,   345,   351,   597,  2683,   345,   423,   546,   383,\n","          402,  1428,    13,  4222,   467,  4058,   290,  1265,    11,   290,\n","          314,  1183,   466,   616,  1266,   284,  2148,  7187,   290,  7613,\n","         7429,    13, 50256,   198, 50256,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100]) \n","\n","prompt_input_ids \n"," tensor([50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n","           27,    91, 10057,    91,    29,   198, 50256,   198,    27,    91,\n","         7220,    91,    29,   198,    40,   423,   617,  2683,   546,   383,\n","          402,  1428,    13, 50256,   198,    27,    91,   562, 10167,    91,\n","           29,   198]) \n","\n","prompt_attention_mask \n"," tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1]) \n","\n"]}]},{"cell_type":"markdown","source":["## 3.5 Model Loading"],"metadata":{"id":"mbX9vcaZtk_S"}},{"cell_type":"code","source":["# loading model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name, torch_dtype=\"auto\", device_map=\"cuda\", trust_remote_code=True)\n","\n","# create reference model\n","parameter_names = [n for n, _ in model.named_parameters()]\n","ref_model = deepcopy(model)\n","\n","# if no layers are shared, return copy of model\n","for param_name in parameter_names:\n","    param = ref_model.get_parameter(param_name)\n","    param.requires_grad = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["b19ce09f106443a49f5b97adfa642848","75ac6169981142f0916e5811e3b97d25","fb31a8cc9eeb44ce9d913a58de7e98b4","28ace6c5e7404d49ab79e477c552e89f","92af225991814ee38713d9ad1ed913dd","3c7c2c912c0f4187a1c9f8618bda9408","36c87f576a8343f39cf30ae4d2d3d476","09e7b8c9bb5e4adab9c376653569940a","e29123e753be434f8c95e4fe415b073b","3eec2372e998477baaacf5bf6d659216","d592da67e76941e0a5e0c8f9803b80da"]},"id":"w21tdyVfQEl6","executionInfo":{"status":"ok","timestamp":1704352997603,"user_tz":-480,"elapsed":15680,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"5a467ac8-469c-4467-c43e-7996e762b3f5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b19ce09f106443a49f5b97adfa642848"}},"metadata":{}}]},{"cell_type":"code","source":["# disable dropout\n","def disable_dropout_in_model(model: torch.nn.Module) -> None:\n","    for module in model.modules():\n","        if isinstance(module, torch.nn.Dropout):\n","            module.p = 0\n","\n","disable_dropout_in_model(model)\n","disable_dropout_in_model(ref_model)"],"metadata":{"id":"PAExRqYd1XXs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.6 Data Loader"],"metadata":{"id":"0oCLE2zLW9eA"}},{"cell_type":"code","source":["def pad_to_length(tensor, length, pad_value, dim=-1):\n","    if tensor.size(dim) >= length:\n","        return tensor\n","    else:\n","        pad_size = list(tensor.shape)\n","        pad_size[dim] = length - tensor.size(dim)\n","        return torch.cat(\n","            [tensor, pad_value * torch.ones(*pad_size, dtype=tensor.dtype, device=tensor.device)],\n","            dim=dim)\n","\n","def get_batch_logps(\n","        logits, labels, average_log_prob=False, label_pad_token_id=-100, is_encoder_decoder=False):\n","        \"\"\"Compute the log probabilities of the given labels under the given logits.\"\"\"\n","\n","        if not is_encoder_decoder:\n","            # shift right, since input first token is bos\n","            # (m, seq_len)\n","            labels = labels[:, 1:].clone()\n","            # (m, seq_len, vocab_size)\n","            logits = logits[:, :-1, :]\n","        loss_mask = labels != label_pad_token_id\n","\n","        # dummy token to be ignored in losses\n","        labels[labels == label_pad_token_id] = 0\n","\n","        # (m, seq_len)\n","        per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n","\n","        if average_log_prob:\n","            return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n","        else:\n","            return (per_token_logps * loss_mask).sum(-1)\n","\n","def concatenated_inputs(\n","    batch, is_encoder_decoder=False, label_pad_token_id=-100, padding_value=0, device=None):\n","\n","    concatenated_batch = {}\n","\n","    # find out the max length between chosen and rejected\n","    if is_encoder_decoder:\n","        max_length = max(batch[\"chosen_labels\"].shape[1], batch[\"rejected_labels\"].shape[1])\n","    else:\n","        max_length = max(batch[\"chosen_input_ids\"].shape[1], batch[\"rejected_input_ids\"].shape[1])\n","\n","    # chosen_input_ids, chosen_attention_mask, chosen_labels\n","    for k in batch:\n","        if k.startswith(\"chosen\") and isinstance(batch[k], torch.Tensor):\n","            if \"labels\" in k or is_encoder_decoder:\n","                pad_value = label_pad_token_id\n","            elif k.endswith(\"_input_ids\"):\n","                pad_value = padding_value\n","            elif k.endswith(\"_attention_mask\"):\n","                pad_value = 0\n","            concatenated_key = k.replace(\"chosen\", \"concatenated\")\n","            # (m, max_length)\n","            concatenated_batch[concatenated_key] = pad_to_length(\n","                batch[k], max_length, pad_value=pad_value)\n","\n","    # rejected_input_ids, rejected_attention_mask, rejected_labels\n","    for k in batch:\n","        if k.startswith(\"rejected\") and isinstance(batch[k], torch.Tensor):\n","            if \"labels\" in k or is_encoder_decoder:\n","                pad_value = label_pad_token_id\n","            elif k.endswith(\"_input_ids\"):\n","                pad_value = padding_value\n","            elif k.endswith(\"_attention_mask\"):\n","                pad_value = 0\n","            concatenated_key = k.replace(\"rejected\", \"concatenated\")\n","\n","             # (2 * m, max_length)\n","            concatenated_batch[concatenated_key] = torch.cat(\n","                (\n","                    concatenated_batch[concatenated_key],\n","                    pad_to_length(batch[k], max_length, pad_value=pad_value),\n","                ),\n","                dim=0,\n","            ).to(device=device)\n","\n","    if is_encoder_decoder:\n","        concatenated_batch[\"concatenated_input_ids\"] = batch[\"prompt_input_ids\"].repeat(2, 1).to(device=device)\n","        concatenated_batch[\"concatenated_attention_mask\"] = (\n","            batch[\"prompt_attention_mask\"].repeat(2, 1).to(device=device)\n","        )\n","\n","    # concatenated_input_ids, concatenated_attention_mask\n","    return concatenated_batch\n","\n","def concatenated_forward(model, batch, padding_value, is_encoder_decoder=False):\n","    \"\"\"\n","    Run the given model on the given batch of inputs,\n","    concatenating the chosen and rejected inputs together.\n","    We do this to avoid doing two forward passes, because it's faster for FSDP.\n","    \"\"\"\n","\n","    # combine both rejected and chosen\n","    # concatenated: input_ids, attention_mask, labels\n","    concatenated_batch = concatenated_inputs(\n","        batch,\n","        is_encoder_decoder=is_encoder_decoder,\n","        label_pad_token_id=-100,\n","        padding_value=padding_value,\n","        device=accelerator.device,\n","    )\n","\n","    # batch size\n","    len_chosen = batch[\"chosen_labels\"].shape[0]\n","\n","    model_kwargs = (\n","        {\n","            \"labels\": concatenated_batch[\"concatenated_labels\"],\n","            \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n","        }\n","        if is_encoder_decoder\n","        else {}\n","    )\n","\n","    # [m, seq_len, vocab_size]\n","    all_logits = model(\n","        concatenated_batch[\"concatenated_input_ids\"],\n","        attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n","        **model_kwargs,\n","    ).logits\n","\n","    # (m), log_softmax score for each sentence (summing all tokens)\n","    all_logps = get_batch_logps(\n","        all_logits,\n","        concatenated_batch[\"concatenated_labels\"],\n","        average_log_prob=False,\n","        is_encoder_decoder=is_encoder_decoder,\n","        label_pad_token_id=-100,\n","    )\n","\n","    # after merging, first half is choosen , 2nd half is rejected\n","    chosen_logps = all_logps[:len_chosen]\n","    rejected_logps = all_logps[len_chosen:]\n","    chosen_logits = all_logits[:len_chosen]\n","    rejected_logits = all_logits[len_chosen:]\n","    return (chosen_logps, rejected_logps, chosen_logits, rejected_logits)\n","\n","def compute_reference_log_probs(padded_batch, ref_model):\n","    \"\"\"\n","    Computes log probabilities of the reference model for a\n","    single padded batch of a DPO specific dataset.\n","    \"\"\"\n","\n","    with torch.no_grad():\n","        ref_chosen_logps, ref_reject_logps, _, _ = concatenated_forward(\n","            ref_model, padded_batch, tokenizer.pad_token_id)\n","\n","    return ref_chosen_logps, ref_reject_logps"],"metadata":{"id":"JN27QkocJ6qO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_train_dataloader(dataset, dataloader_params, ref_model):\n","\n","    # create data loader with multiple batch\n","    data_loader = DataLoader(dataset, **dataloader_params)\n","    reference_chosen_logps = []\n","    reference_rejected_logps = []\n","\n","    for padded_batch in tqdm(iterable=data_loader, desc=\"Train dataset reference log probs\"):\n","        # log softmax probability for chosen and rejected based on the label index\n","        reference_chosen_logp, reference_rejected_logp = compute_reference_log_probs(padded_batch, ref_model)\n","        reference_chosen_logps.append(reference_chosen_logp.cpu())\n","        reference_rejected_logps.append(reference_rejected_logp.cpu())\n","\n","    all_reference_chosen_logps = torch.cat(reference_chosen_logps).float().numpy()\n","    all_reference_rejected_logps = torch.cat(reference_rejected_logps).float().numpy()\n","\n","    dataset = dataset.add_column(\n","        name=\"reference_chosen_logps\", column=all_reference_chosen_logps\n","    )\n","\n","    dataset = dataset.add_column(\n","        name=\"reference_rejected_logps\", column=all_reference_rejected_logps\n","    )\n","\n","    data_loader = DataLoader(dataset, **dataloader_params)\n","\n","    return data_loader\n","\n","dataloader_params = {\n","    \"batch_size\": 2,\n","    \"collate_fn\": data_collator,\n","    \"num_workers\": 2,\n","    \"pin_memory\": False,\n","    \"shuffle\": False,\n","}\n","\n","dataloader = get_train_dataloader(processed_dataset, dataloader_params, ref_model)\n","print({k: len(v) for k, v in next(iter(dataloader)).items()})"],"metadata":{"id":"z6-OhmFb3NHX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704353005827,"user_tz":-480,"elapsed":8238,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"61a19648-1375-4d4b-ab39-1f0ec3f54af8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Train dataset reference log probs: 100%|██████████| 6/6 [00:08<00:00,  1.37s/it]\n"]},{"output_type":"stream","name":"stdout","text":["{'chosen': 2, 'rejected': 2, 'prompt': 2, 'chosen_input_ids': 2, 'chosen_attention_mask': 2, 'chosen_labels': 2, 'rejected_input_ids': 2, 'rejected_attention_mask': 2, 'rejected_labels': 2, 'prompt_input_ids': 2, 'prompt_attention_mask': 2, 'reference_chosen_logps': 2, 'reference_rejected_logps': 2}\n"]}]},{"cell_type":"markdown","source":["## 3.7 DPO Loss Metric"],"metadata":{"id":"ydFOIoD5Ccef"}},{"cell_type":"code","source":["def dpo_loss(\n","        policy_chosen_logps, policy_rejected_logps, reference_chosen_logps,\n","        reference_rejected_logps, loss_type='sigmoid', beta=0.1, label_smoothing=0,\n","        reference_free=False):\n","\n","    \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\"\"\"\n","\n","    # higher the better, diff of chosen vs rejected\n","    pi_logratios = (policy_chosen_logps - policy_rejected_logps).cpu()\n","\n","    # If True,  ignore the _provided_ reference model\n","    # implicitly use a reference model that assigns equal probability to all responses\n","    if reference_free:\n","        ref_logratios = 0\n","    else:\n","        ref_logratios = reference_chosen_logps - reference_rejected_logps\n","\n","    # (batch_size, ) diff between policy and reference, good if it's positive\n","    logits = pi_logratios - ref_logratios\n","\n","    # beta = temperature to regularise DPO loss (normally 0.1 - 0.5)\n","    # ignore the reference model if beta == 0\n","    # label_smoothing encodes uncertainty about the labels and calculates a conservative loss.\n","    if loss_type == \"sigmoid\":\n","        losses = (-F.logsigmoid(beta * logits) * (1 - label_smoothing)) - \\\n","            (F.logsigmoid(-beta * logits) * label_smoothing)\n","\n","    elif loss_type == \"hinge\":\n","        losses = torch.relu(1 - beta * logits)\n","    elif loss_type == \"ipo\":\n","        losses = (logits - 1 / (2 * beta)) ** 2\n","    elif loss_type == \"kto_pair\":\n","        chosen_KL = (policy_chosen_logps - reference_chosen_logps).mean().clamp(min=0)\n","        rejected_KL = (policy_rejected_logps - reference_rejected_logps).mean().clamp(min=0)\n","        chosen_logratios = policy_chosen_logps - reference_chosen_logps\n","        rejected_logratios = policy_rejected_logps - reference_rejected_logps\n","        # As described in the KTO report, the KL term for chosen (rejected)\n","        # is estimated using the rejected (chosen) half.\n","        losses = torch.cat(\n","            (\n","                1 - F.sigmoid(beta * (chosen_logratios - rejected_KL)),\n","                1 - F.sigmoid(beta * (chosen_KL - rejected_logratios)),\n","            ),\n","        )\n","\n","    # chosen_rewards targeted to be high and while rejected_reward targeted to be low\n","    chosen_rewards = beta * (policy_chosen_logps.cpu() - reference_chosen_logps.cpu()).detach()\n","    rejected_rewards = beta * (policy_rejected_logps.cpu() - reference_rejected_logps.cpu()).detach()\n","\n","    return losses, chosen_rewards, rejected_rewards"],"metadata":{"id":"MoLd8R6FUM9Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_batch_loss_metrics(model, ref_model, batch, train_eval=\"train\"):\n","        \"\"\"compute dpo loss for each batch\"\"\"\n","        metrics = {}\n","        with torch.no_grad():\n","            # the original model to be updated (also called as policy model)\n","            # (m), log_softmax score for each sentence (summing all index tokens prob in labels)\n","            chosen_logps, rejected_logps, chosen_logits, rejected_logits = concatenated_forward(\n","                model, batch, tokenizer.pad_token_id)\n","\n","        # use reference_chosen_logps and reference_rejected_logps if exists\n","        if \"reference_chosen_logps\" in batch and \"reference_rejected_logps\" in batch:\n","            ref_chosen_logps = batch[\"reference_chosen_logps\"]\n","            ref_rejected_logps = batch[\"reference_rejected_logps\"]\n","        else:\n","            with torch.no_grad():\n","                ref_chosen_logps, ref_rejected_logps, _, _, = concatenated_forward(ref_model, batch)\n","\n","        losses, chosen_rewards, rejected_rewards = dpo_loss(\n","            chosen_logps, rejected_logps, ref_chosen_logps, ref_rejected_logps)\n","        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n","\n","        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n","        metrics[f\"{prefix}rewards/chosen\"] = chosen_rewards.mean().cpu()\n","        metrics[f\"{prefix}rewards/rejected\"] = rejected_rewards.mean().cpu()\n","        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.mean().cpu()\n","        metrics[f\"{prefix}rewards/margins\"] = (chosen_rewards - rejected_rewards).mean().cpu()\n","        metrics[f\"{prefix}logps/rejected\"] = rejected_logps.detach().mean().cpu()\n","        metrics[f\"{prefix}logps/chosen\"] = chosen_logps.detach().mean().cpu()\n","        metrics[f\"{prefix}logits/rejected\"] = rejected_logits.detach().mean().cpu()\n","        metrics[f\"{prefix}logits/chosen\"] = chosen_logits.detach().mean().cpu()\n","        return losses.mean(), metrics\n","\n","loss, metrics = get_batch_loss_metrics(model, ref_model, next(iter(dataloader)), train_eval=\"train\",)\n","print(\"Loss: \", loss)\n","print(metrics)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2c5RawzCGcl","executionInfo":{"status":"ok","timestamp":1704353935184,"user_tz":-480,"elapsed":2941,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"6d1369fb-2234-4dda-ece9-4b22a8c7c6f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 0.]) tensor([0.6931, 0.6931]) tensor([0.6931, 0.6931])\n","Loss:  tensor(0.6931)\n","{'rewards/chosen': tensor(0.), 'rewards/rejected': tensor(0.), 'rewards/accuracies': tensor(0.), 'rewards/margins': tensor(0.), 'logps/rejected': tensor(-887.3922), 'logps/chosen': tensor(-972.4705), 'logits/rejected': tensor(0.3401), 'logits/chosen': tensor(0.2800)}\n"]}]},{"cell_type":"markdown","source":["## 3.8 Evaluation"],"metadata":{"id":"-K0nNX1pF2s7"}},{"cell_type":"code","source":["def prediction_step(model, inputs, prediction_loss_only=False, ignore_keys=None,):\n","\n","    if ignore_keys is None:\n","        if hasattr(model, \"config\"):\n","            ignore_keys = getattr(model.config, \"keys_to_ignore_at_inference\", [])\n","        else:\n","            ignore_keys = []\n","\n","    with torch.no_grad():\n","        loss, metrics = get_batch_loss_metrics(model, inputs, train_eval=\"eval\")\n","\n","    if prediction_loss_only:\n","        return (loss.detach(), None, None)\n","\n","    # logits for the chosen and rejected samples from model\n","    logits_dict = {\n","        \"eval_logits/chosen\": metrics[\"eval_logits/chosen\"],\n","        \"eval_logits/rejected\": metrics[\"eval_logits/rejected\"],\n","    }\n","    logits = tuple(v.unsqueeze(dim=0) for k, v in logits_dict.items() if k not in ignore_keys)\n","    logits = torch.stack(logits).mean(axis=1)\n","    labels = torch.zeros(logits.shape[0])\n","    return (loss.detach(), logits, labels)"],"metadata":{"id":"sovwY2sTctXd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"148a211d7b8f4bca82e0957abc644b38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7c074ee37014218b6a1215209b18217","IPY_MODEL_d25bb5eb7736444790123ff524650475","IPY_MODEL_8be1470212574c00a90fd0cf6aef699b"],"layout":"IPY_MODEL_8dcfc16355364909a75b16f9d905497a"}},"3d4acfadf90f43cb970c2ea7689003bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c1e8d38be484abe9f92c9d352b6217a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f4dcde0c82c4e479df28f8d2ad42062":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8be1470212574c00a90fd0cf6aef699b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbdcfb038ff841099701835462187656","placeholder":"​","style":"IPY_MODEL_8d4fe5f884db42759f79f410770e6b87","value":" 2/2 [01:22&lt;00:00, 38.31s/it]"}},"8d4fe5f884db42759f79f410770e6b87":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8dcfc16355364909a75b16f9d905497a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0dda0c414a949cd9b1c0a3148eb3f59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7c074ee37014218b6a1215209b18217":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c1e8d38be484abe9f92c9d352b6217a","placeholder":"​","style":"IPY_MODEL_6f4dcde0c82c4e479df28f8d2ad42062","value":"Loading checkpoint shards: 100%"}},"d25bb5eb7736444790123ff524650475":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0dda0c414a949cd9b1c0a3148eb3f59","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d4acfadf90f43cb970c2ea7689003bb","value":2}},"dbdcfb038ff841099701835462187656":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47a502787d3f456b95e4445d864c1862":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0a92bddf8804081bfea830813828dc9","IPY_MODEL_d213db9be2024ecabef164e2f0f18968","IPY_MODEL_929c5c5317af4c568232a802871211a0"],"layout":"IPY_MODEL_b95aba2576ce49b581b74125777f83a3"}},"a0a92bddf8804081bfea830813828dc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_723d8b870f414481990df6c03d34b663","placeholder":"​","style":"IPY_MODEL_1b92fcaf4fc942bf915d1a26468e0328","value":"tokenizer_config.json: 100%"}},"d213db9be2024ecabef164e2f0f18968":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2ab3b908feb49f4ac2c606605ca6a35","max":1337,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e410a2482fa4267939107492151b3e5","value":1337}},"929c5c5317af4c568232a802871211a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82bde1f5a9c94e508414a5a349a1c424","placeholder":"​","style":"IPY_MODEL_464a55e073ec4015939742a4aad6cdf8","value":" 1.34k/1.34k [00:00&lt;00:00, 89.2kB/s]"}},"b95aba2576ce49b581b74125777f83a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"723d8b870f414481990df6c03d34b663":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b92fcaf4fc942bf915d1a26468e0328":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2ab3b908feb49f4ac2c606605ca6a35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e410a2482fa4267939107492151b3e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82bde1f5a9c94e508414a5a349a1c424":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"464a55e073ec4015939742a4aad6cdf8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24d55d4699d44cff845d3c0c0039d80d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3253f46e9df492a9646f8af3db31d58","IPY_MODEL_fd13ee50b13d4f3a8af17706177840fc","IPY_MODEL_9907a7c151db4c3d8902b39f5cd899f1"],"layout":"IPY_MODEL_c37eece4f8964b929165a97ac1418369"}},"a3253f46e9df492a9646f8af3db31d58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e2448b87717418e97a27014a7cd674d","placeholder":"​","style":"IPY_MODEL_0239d0db47a3427db48070e9ba805b1c","value":"tokenizer.json: 100%"}},"fd13ee50b13d4f3a8af17706177840fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92533c253cf04b05ae944b013cdb8bce","max":1795303,"min":0,"orientation":"horizontal","style":"IPY_MODEL_993aee3fc7294d398161d09008c3ff52","value":1795303}},"9907a7c151db4c3d8902b39f5cd899f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c00d5ba7d2fe4cb594c4faf3b22f12db","placeholder":"​","style":"IPY_MODEL_9c4e70ff43b24e258f2c6363fb36bd22","value":" 1.80M/1.80M [00:00&lt;00:00, 5.35MB/s]"}},"c37eece4f8964b929165a97ac1418369":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e2448b87717418e97a27014a7cd674d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0239d0db47a3427db48070e9ba805b1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92533c253cf04b05ae944b013cdb8bce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"993aee3fc7294d398161d09008c3ff52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c00d5ba7d2fe4cb594c4faf3b22f12db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c4e70ff43b24e258f2c6363fb36bd22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f79a85f6709145598ddf0d06139e127c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6bbfc165f9d1422fa510fe173d7b59bd","IPY_MODEL_8b24b7cf314540cea4d4bd60de5be593","IPY_MODEL_65d2631db1724e0bbce1418a068e81de"],"layout":"IPY_MODEL_cf956ea8f3194a79b66ec247ceab0f44"}},"6bbfc165f9d1422fa510fe173d7b59bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e612084f4914e0c8b7fdf5c47af11f5","placeholder":"​","style":"IPY_MODEL_7dafea4eac90404285d74472bbb8da4c","value":"special_tokens_map.json: 100%"}},"8b24b7cf314540cea4d4bd60de5be593":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9981df032f6c4983a35460b11c242594","max":437,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb98a7bbe5ed45b18bf1820ddbd9401f","value":437}},"65d2631db1724e0bbce1418a068e81de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5790b423464a4206ba8660c225ba82ec","placeholder":"​","style":"IPY_MODEL_5e0877463fc9485daf44bae1ca0ba71d","value":" 437/437 [00:00&lt;00:00, 36.4kB/s]"}},"cf956ea8f3194a79b66ec247ceab0f44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e612084f4914e0c8b7fdf5c47af11f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dafea4eac90404285d74472bbb8da4c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9981df032f6c4983a35460b11c242594":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb98a7bbe5ed45b18bf1820ddbd9401f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5790b423464a4206ba8660c225ba82ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e0877463fc9485daf44bae1ca0ba71d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"429e4dd630f14df993222ea0aa4dd0ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b778983b02a4677b95d1f148cba50ed","IPY_MODEL_efd774e7d27d49b381c01402d9d3b1b7","IPY_MODEL_68fe05cd1a774bee8c0ef5b4ec4ccc6c"],"layout":"IPY_MODEL_8f24a784fed14197a4c804a398238988"}},"5b778983b02a4677b95d1f148cba50ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5fff05c886b64cec97cde11a8aebd97a","placeholder":"​","style":"IPY_MODEL_e159caf615ef406186ac09ecb9443f69","value":"Formatting comparisons with prompt template (num_proc=12): 100%"}},"efd774e7d27d49b381c01402d9d3b1b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d2f3826fbf4787b206ddb4f3b50197","max":1239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77d22a3041854781bcd3435b62665f91","value":1239}},"68fe05cd1a774bee8c0ef5b4ec4ccc6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2436c53be9d4480c871bbf5951343acb","placeholder":"​","style":"IPY_MODEL_70675aed911b4821b7189608c0222ed8","value":" 1239/1239 [00:02&lt;00:00, 945.09 examples/s]"}},"8f24a784fed14197a4c804a398238988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fff05c886b64cec97cde11a8aebd97a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e159caf615ef406186ac09ecb9443f69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86d2f3826fbf4787b206ddb4f3b50197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77d22a3041854781bcd3435b62665f91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2436c53be9d4480c871bbf5951343acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70675aed911b4821b7189608c0222ed8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"18e25cc41a104110b96735c4d4fab6c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_912781c5b95640b49ce84e649d723e97","IPY_MODEL_fbfb3ff87fb54b4cbe0a663b0ca907c1","IPY_MODEL_ee2dd25cdc514d50aeefc929bdcedbe0"],"layout":"IPY_MODEL_c1e5ca1338214b528668c5c7663ef001"}},"912781c5b95640b49ce84e649d723e97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71281efb42cf4f7ea54d140fb34dc81c","placeholder":"​","style":"IPY_MODEL_d41528a54e304a8d83dc121736705bcf","value":"Formatting comparisons with prompt template (num_proc=12): 100%"}},"fbfb3ff87fb54b4cbe0a663b0ca907c1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff91c218e3f44802880031e159d078d7","max":2000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38f44329792d4dc788c4bd4e5d71da2c","value":2000}},"ee2dd25cdc514d50aeefc929bdcedbe0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1085098b8d84f639b300533783095cd","placeholder":"​","style":"IPY_MODEL_3be615fb0cb2472c85db347593baa57a","value":" 2000/2000 [00:03&lt;00:00, 1440.69 examples/s]"}},"c1e5ca1338214b528668c5c7663ef001":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71281efb42cf4f7ea54d140fb34dc81c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d41528a54e304a8d83dc121736705bcf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff91c218e3f44802880031e159d078d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38f44329792d4dc788c4bd4e5d71da2c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1085098b8d84f639b300533783095cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3be615fb0cb2472c85db347593baa57a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b19ce09f106443a49f5b97adfa642848":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75ac6169981142f0916e5811e3b97d25","IPY_MODEL_fb31a8cc9eeb44ce9d913a58de7e98b4","IPY_MODEL_28ace6c5e7404d49ab79e477c552e89f"],"layout":"IPY_MODEL_92af225991814ee38713d9ad1ed913dd"}},"75ac6169981142f0916e5811e3b97d25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c7c2c912c0f4187a1c9f8618bda9408","placeholder":"​","style":"IPY_MODEL_36c87f576a8343f39cf30ae4d2d3d476","value":"Loading checkpoint shards: 100%"}},"fb31a8cc9eeb44ce9d913a58de7e98b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09e7b8c9bb5e4adab9c376653569940a","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e29123e753be434f8c95e4fe415b073b","value":2}},"28ace6c5e7404d49ab79e477c552e89f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3eec2372e998477baaacf5bf6d659216","placeholder":"​","style":"IPY_MODEL_d592da67e76941e0a5e0c8f9803b80da","value":" 2/2 [00:02&lt;00:00,  1.28s/it]"}},"92af225991814ee38713d9ad1ed913dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c7c2c912c0f4187a1c9f8618bda9408":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36c87f576a8343f39cf30ae4d2d3d476":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09e7b8c9bb5e4adab9c376653569940a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e29123e753be434f8c95e4fe415b073b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3eec2372e998477baaacf5bf6d659216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d592da67e76941e0a5e0c8f9803b80da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}