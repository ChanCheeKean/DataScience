{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[]},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install datasets torch transformers\n","!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n","!unzip -qq cornell_movie_dialogs_corpus.zip\n","!rm cornell_movie_dialogs_corpus.zip\n","!mkdir datasets\n","!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n","!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets"],"metadata":{"id":"pXnZMry3we8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","import json\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.utils.data\n","import math\n","import torch.nn.functional as F"],"metadata":{"id":"e3yy1ZGr4dbN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1) Data Processing\n","\n","This tutorial trains a <a href=\"https://arxiv.org/abs/1706.03762\" class=\"external\">Transformer model</a> to be a chatbot. This is an advanced example that assumes knowledge of [text generation](https://tensorflow.org/alpha/tutorials/text/text_generation), [attention](https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention) and [transformer](https://www.tensorflow.org/alpha/tutorials/text/transformer).\n","\n","\n","We will use the conversations in movies and TV shows provided by [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which contains more than 220 thousands conversational exchanges between more than 10k pairs of movie characters, as our dataset.\n","\n","`movie_conversations.txt` contains list of the conversation IDs and `movie_lines.text` contains the text of assoicated with each conversation ID. For further  information regarding the dataset, please check the README file in the zip file.\n"],"metadata":{"id":"jGHsiOnj4fii"}},{"cell_type":"code","source":["# data processing\n","max_len = 25\n","\n","def remove_punc(string):\n","    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n","    no_punct = \"\"\n","    for char in string:\n","        if char not in punctuations:\n","            no_punct = no_punct + char  # space is also a character\n","    return no_punct.lower()\n","\n","corpus_movie_conv = './datasets/movie_conversations.txt'\n","corpus_movie_lines = './datasets/movie_lines.txt'\n","with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n","    conv = c.readlines()\n","with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n","    lines = l.readlines()\n","\n","# extract text\n","lines_dic = {}\n","for line in lines:\n","    objects = line.split(\" +++$+++ \")\n","    lines_dic[objects[0]] = objects[-1]\n","\n","# generate question answer pairs\n","pairs = []\n","for con in conv:\n","    ids = eval(con.split(\" +++$+++ \")[-1])\n","    for i in range(len(ids)):\n","        qa_pairs = []\n","\n","        if i == len(ids) - 1:\n","            break\n","\n","        first = remove_punc(lines_dic[ids[i]].strip())\n","        second = remove_punc(lines_dic[ids[i+1]].strip())\n","        qa_pairs.append(first.split()[:max_len])\n","        qa_pairs.append(second.split()[:max_len])\n","        pairs.append(qa_pairs)\n","\n","# sample\n","print(pairs[20])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RxZmpdunyI6B","executionInfo":{"status":"ok","timestamp":1661446523411,"user_tz":-480,"elapsed":5611,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"c9e92f3d-e6bc-4508-be2d-47e39e7519a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['i', 'really', 'really', 'really', 'wanna', 'go', 'but', 'i', 'cant', 'not', 'unless', 'my', 'sister', 'goes'], ['im', 'workin', 'on', 'it', 'but', 'she', 'doesnt', 'seem', 'to', 'be', 'goin', 'for', 'him']]\n"]}]},{"cell_type":"code","source":["# word map\n","min_word_freq = 5\n","\n","word_freq = Counter()\n","for pair in pairs:\n","    word_freq.update(pair[0])\n","    word_freq.update(pair[1])\n","\n","words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n","word_map = {k: v + 1 for v, k in enumerate(words)}\n","word_map['<unk>'] = len(word_map) + 1\n","word_map['<start>'] = len(word_map) + 1\n","word_map['<end>'] = len(word_map) + 1\n","word_map['<pad>'] = 0\n","\n","print(\"Total words are: {}\".format(len(word_map)))\n","\n","# encode sentences based on word map\n","def encode_question(words, word_map):\n","    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n","        [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","pairs_encoded = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    ans = encode_reply(pair[1], word_map)\n","    pairs_encoded.append([qus, ans])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_q0Ek2foz39k","executionInfo":{"status":"ok","timestamp":1661446527661,"user_tz":-480,"elapsed":4262,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"85f7e928-cf99-47d2-e05e-5be7bece5ed8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total words are: 18243\n"]}]},{"cell_type":"code","source":["# dataset and dataloader\n","class Dataset(Dataset):\n","\n","    def __init__(self, pairs):\n","\n","        self.pairs = pairs\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size\n","\n","train_loader = DataLoader(Dataset(pairs_encoded), batch_size=32, shuffle=True, pin_memory=True)\n","question, reply = next(iter(train_loader))\n","print(\"Question: \", question.size())\n","print(\"Answer: \", reply.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34PY0Zjd2JNw","executionInfo":{"status":"ok","timestamp":1661446540894,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"59ac1e4d-3888-47b4-9e7b-8a1367edb00b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question:  torch.Size([32, 25])\n","Answer:  torch.Size([32, 27])\n"]}]},{"cell_type":"markdown","source":["# 2) Masking\n","\n","1. Mask all the pad tokens (value `0`) in the batch to ensure the model does not treat padding as input.\n","\n","2. **Look-ahead Mask** to mask the future tokens in a sequence.\n","We also mask out pad tokens. i.e. To predict the third word, only the first and second word will be used"],"metadata":{"id":"HuoQgsta4mBZ"}},{"cell_type":"code","source":["# create mask\n","def create_masks(question, reply_input, reply_target, device='cpu'):\n","\n","    def subsequent_mask(size):\n","        # (max_words, max_words)\n","        # binary triangle\n","        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        # (1, max_words, max_words)\n","        return mask.unsqueeze(0)\n","\n","    # boolean(m, max_words)\n","    question_mask = question != 0\n","\n","    # (m, 1, 1, max_words)\n","    question_mask = question_mask.to(device)\n","    question_mask = question_mask.unsqueeze(1).unsqueeze(1)\n","\n","    # boolean(m, max_words)\n","    reply_input_mask = reply_input != 0\n","    # (m, 1, max_words)\n","    reply_input_mask = reply_input_mask.unsqueeze(1)\n","\n","    # only include triangle and non-pad token\n","    # (m, max_words, max_words)\n","    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data)\n","\n","    # (batch_size, 1, max_words, max_words)\n","    reply_input_mask = reply_input_mask.unsqueeze(1)\n","\n","    # (batch_size, max_words)\n","    reply_target_mask = reply_target != 0\n","\n","    return question_mask, reply_input_mask, reply_target_mask\n","\n","reply_input = reply[:, :-1]\n","reply_target = reply[:, 1:]\n","print('Reply Target Size: ', reply_target.size())\n","\n","# Create mask and add dimensions\n","question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","print('question_mask Size: ', question_mask.size())\n","print('reply_input_mask Size: ', reply_input_mask.size())\n","print('reply_target_mask Size: ', reply_target_mask.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQQckg0E6jer","executionInfo":{"status":"ok","timestamp":1661446985540,"user_tz":-480,"elapsed":286,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"5386d6bd-95fe-41b8-9b2b-3fa727f07500"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reply Target Size:  torch.Size([32, 26])\n","question_mask Size:  torch.Size([32, 1, 1, 25])\n","reply_input_mask Size:  torch.Size([32, 1, 26, 26])\n","reply_target_mask Size:  torch.Size([32, 26])\n"]}]},{"cell_type":"markdown","source":["# 3) Embedding"],"metadata":{"id":"44kxspAw4qyk"}},{"cell_type":"markdown","source":["## 3.1 Positional Embedding\n","Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.\n","\n","The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n","\n","See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n","\n","$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n","$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"],"metadata":{"id":"EL5TUmSFMXEj"}},{"cell_type":"code","source":["# Positional Embedding\n","class Embeddings(nn.Module):\n","    \"\"\"\n","    Implements embeddings of the words and adds their positional encodings.\n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len=50, num_layers=6):\n","        super(Embeddings, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        # (1, max_len, d_model)\n","        self.pe = self.create_positional_encoding(max_len, self.d_model)\n","        # (1, num_layers, d_model)\n","        self.te = self.create_positional_encoding(num_layers, self.d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def create_positional_encoding(self, max_len, d_model, device='cpu'):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        # for each position of the word\n","        for pos in range(max_len):\n","            # for each dimension of the each position\n","            for i in range(0, d_model, 2):\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        # (1, max_len, d_model)\n","        pe = pe.unsqueeze(0)\n","        return pe\n","\n","    def forward(self, embedding, layer_idx):\n","        # create embed weight during first layer\n","        # (m, max_len) --> (m, max_len, d_model)\n","        if layer_idx == 0:\n","            # scaling helps in stabilizing and improving the convergence properties\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","\n","        ### Positional Embedding\n","        # pe will automatically be expanded with the same batch size as encoded_words\n","        # (m, max_len, d_model)\n","        embedding += self.pe[:, :embedding.size(1)]\n","\n","        # te: (1, num_layers, d_model) --> (1, 1, d_model) --> (1, max_len, d_model)\n","        # (m, max_len, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        return embedding\n","\n","embed_model = Embeddings(len(word_map), 512)\n","result = embed_model.forward(question, 0)\n","print(result.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUuBWtyN9nB6","executionInfo":{"status":"ok","timestamp":1661448121991,"user_tz":-480,"elapsed":301,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"e1acf6b4-f60b-4d6d-d955-4e4efca12545"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 25, 512])\n"]}]},{"cell_type":"markdown","source":["# 4) Transformers"],"metadata":{"id":"AtvZKs_k4vnQ"}},{"cell_type":"markdown","source":["## 4.1 Scaled dot product Attention\n","\n","The scaled dot-product attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n","\n","$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}} + M) V} $$\n","\n","As the softmax normalization is done on the `key`, its values decide the amount of importance given to the `query`.\n","\n","The output represents the multiplication of the attention weights and the `value` vector. This ensures that the words we want to focus on are kept as is and the irrelevant words are flushed out.\n","\n","The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.\n","\n","For example, consider that `query` and `key` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `query` and `key` should have a mean of 0 and variance of 1, so that we get a gentler softmax.\n","\n","Masking is needed to prevent the attention mechanism of a transformer from “cheating” in the decoder or peeking to the future. The mask is multiplied with *-1e9 (close to negative infinity).* This is done because the mask is summed with the scaled matrix multiplication of `query` and `key` and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."],"metadata":{"id":"2bSbXD1ALil1"}},{"cell_type":"markdown","source":["## 4.2 Multi-head attention\n","\n","<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n","\n","Multi-head attention consists of four parts:\n","* Linear layers and split into heads.\n","* Scaled dot-product attention.\n","* Concatenation of heads.\n","* Final linear layer.\n","\n","Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads.\n","\n","The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n","\n","Instead of one single attention head, `query`, `key`, and `value` are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."],"metadata":{"id":"X3JilbLyMOHq"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","\n","    def __init__(self, heads, d_model, dropout=0.1):\n","\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % heads == 0\n","        self.d_k = d_model // heads\n","        self.heads = heads\n","        self.dropout = nn.Dropout(dropout)\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.output_linear = nn.Linear(d_model, d_model)\n","\n","    def forward(self, query, key, value, mask):\n","        \"\"\"\n","        query, key, value of shape: (batch_size, max_len, d_model)\n","        mask of shape: (batch_size, 1, 1, max_words)\n","        \"\"\"\n","        # (batch_size, max_len, d_model)\n","        query = self.query(query)\n","        key = self.key(key)\n","        value = self.value(value)\n","\n","        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n","        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n","        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n","        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)\n","\n","        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n","        # query could be different length --> (batch_size, h, query_len, max_len)\n","        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n","\n","        # fill 0 mask with super small number so it wont affect the softmax weight\n","        # (batch_size, h, max_len, max_len)\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","\n","        # (batch_size, h, max_len, max_len)\n","        # softmax to put attention weight for all non-pad tokens\n","        # max_len X max_len matrix of attention\n","        weights = F.softmax(scores, dim=-1)\n","        weights = self.dropout(weights)\n","\n","        # (batch_size, h, query_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, query_len, d_k)\n","        context = torch.matmul(weights, value)\n","\n","        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n","        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n","\n","        # (batch_size, max_len, d_model)\n","        return self.output_linear(context)"],"metadata":{"id":"iVmDCiw7MI1J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.3 FeedForward Layer\n","\n","Network with one hidden layer that applies a non-linear activation function (GELU) to the output of the first linear layer"],"metadata":{"id":"1L1DDYh_Iyuy"}},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, middle_dim=2048):\n","        super(FeedForward, self).__init__()\n","\n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        self.activation = torch.nn.GELU()\n","\n","    def forward(self, x):\n","        out = self.activation(self.fc1(x))\n","        out = self.fc2(self.dropout(out))\n","        return out"],"metadata":{"id":"Yu5iXHLXI1ei"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.4 Encoder Layer\n","\n","Each encoder layer consists of sublayers:\n","\n","1. Multi-head attention (with padding mask)\n","2. 2 dense layers followed by dropout\n","\n","Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n","\n","The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis."],"metadata":{"id":"DSWSkPXuMttj"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, embeddings, mask):\n","        # embeddings: (batch_size, max_len, d_model)\n","        # encoder mask: (batch_size, 1, 1, max_len)\n","        # result: (batch_size, max_len, d_model)\n","        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n","        # residual layer\n","        interacted = self.layernorm(interacted + embeddings)\n","        # bottleneck\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded"],"metadata":{"id":"fHTuAm1nMu_W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.5 Decoder Layer\n","\n","Each decoder layer consists of sublayers:\n","\n","1.   Masked multi-head attention (with look ahead mask and padding mask)\n","2.   Multi-head attention (with padding mask). `value` and `key` receive the *encoder output* as inputs. `query` receives the *output from the masked multi-head attention sublayer.*\n","3.   2 dense layers followed by dropout\n","\n","Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n","\n","As `query` receives the output from decoder's first attention block, and `key` receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."],"metadata":{"id":"PmGtDC_4MyVi"}},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, heads):\n","        super(DecoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","\n","        # embeddings: (batch_size, max_len, d_model)\n","        # decoder mask: (m, 1, max_len, max_len)\n","        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n","        query = self.layernorm(query + embeddings)\n","\n","        # (batch_size, max_len, d_model)\n","        # encoder-decoder mask: (m, 1, 1, max_len)\n","        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded"],"metadata":{"id":"Cb9tdtgBBnbg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.6 Transformer\n","\n","Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned.\n","\n","<img src='https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/transformer.png'>\n","\n","<img src='https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png'>"],"metadata":{"id":"0az3bHuFJNF9"}},{"cell_type":"code","source":["class Transformer(nn.Module):\n","\n","    def __init__(self, d_model, heads, num_layers, word_map):\n","        super(Transformer, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers=num_layers)\n","        self.encoder = EncoderLayer(d_model, heads)\n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","\n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","\n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","\n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        # (m, max_len) --> (m, max_len, d_model)\n","        encoded = self.encode(src_words, src_mask)\n","        # (m, max_len, d_model) --> (m, max_len, d_model)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        # (m, max_len, vocab_size)\n","        out = F.log_softmax(self.logit(decoded), dim=2)\n","        return out"],"metadata":{"id":"6v8zZ_AkLxqG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5) Optimizer\n","\n","<img src='https://miro.medium.com/max/886/1*ZhGLUwaaqlJ9C0WK0nbAEA.png'>\n","\n","\n","**Custom learning rate**\n","$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n","\n","[Relationship between Entropy, Cross-Entropy, and KL-Divergence](https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a)"],"metadata":{"id":"WKhVM3wENWRY"}},{"cell_type":"code","source":["# optimizer & loss\n","class AdamWarmup:\n","\n","    def __init__(self, model_size, warmup_steps, optimizer):\n","        self.model_size = model_size\n","        self.warmup_steps = warmup_steps\n","        self.optimizer = optimizer\n","        self.current_step = 0\n","        self.lr = 0\n","\n","    def get_lr(self):\n","        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n","\n","    def step(self):\n","        # Increment the number of steps each time we call the step function\n","        self.current_step += 1\n","        lr = self.get_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","        # update the learning rate\n","        self.lr = lr\n","        self.optimizer.step()\n","\n","class LossWithLS(nn.Module):\n","\n","    def __init__(self, size, smooth):\n","        super(LossWithLS, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n","        self.confidence = 1.0 - smooth\n","        self.smooth = smooth\n","        self.size = size\n","\n","    def forward(self, prediction, target, mask):\n","        \"\"\"\n","        prediction of shape: (batch_size, max_words, vocab_size)\n","        target and mask of shape: (batch_size, max_words)\n","        \"\"\"\n","\n","        # (batch_size * max_words, vocab_size)\n","        prediction = prediction.view(-1, prediction.size(-1))\n","\n","        # (batch_size * max_words)\n","        # contiguous() makes a copy of the tensor\n","        target = target.contiguous().view(-1)\n","        mask = mask.float()\n","        mask = mask.view(-1)\n","\n","        # (batch_size * max_words)\n","        labels = prediction.data.clone()\n","        # fills the labels tensor with a uniform distribution\n","        # fills labels with a uniform distribution(0.025)\n","        labels.fill_(self.smooth / (self.size - 1))\n","\n","        # but assigns a 1.0 to the true target\n","        # target = torch.tensor([1, 0, 2, 3, 0, 2])\n","        # labels = [\n","        #     [0.025, 1.0, 0.025, 0.025],\n","        #     [1.0, 0.025, 0.025, 0.025],\n","        #     [0.025, 0.025, 1.0, 0.025],\n","        #     [0.025, 0.025, 0.025, 1.0],\n","        #     [1.0, 0.025, 0.025, 0.025],\n","        #     [0.025, 0.025, 1.0, 0.025]\n","        # ]\n","        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","\n","        # (batch_size * max_words, vocab_size)\n","        loss = self.criterion(prediction, labels)\n","        loss = (loss.sum(1) * mask).sum() / mask.sum()\n","        return loss\n","\n","### testing\n","transformer = Transformer(d_model=512, heads=8, num_layers=2, word_map=word_map)\n","transformer = transformer.to('cpu')\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size=512, warmup_steps=4000, optimizer=adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","out = transformer(question, question_mask, reply_input, reply_input_mask)\n","print(out.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dby1mizMOSo","executionInfo":{"status":"ok","timestamp":1661452063848,"user_tz":-480,"elapsed":1436,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9972dd0e-e7ad-49db-c58f-8fdbb26946f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([32, 26, 18243])\n"]}]},{"cell_type":"code","source":["### training step\n","def train(train_loader, transformer, criterion, epoch, device='cpu'):\n","\n","    transformer.train()\n","    sum_loss = 0\n","    count = 0\n","\n","    for i, (question, reply) in enumerate(train_loader):\n","\n","        samples = question.shape[0]\n","\n","        # Move to device\n","        question = question.to(device)\n","        reply = reply.to(device)\n","\n","        # Prepare Target Data\n","        reply_input = reply[:, :-1]\n","        reply_target = reply[:, 1:]\n","\n","        # Create mask and add dimensions\n","        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","\n","        # Get the transformer outputs\n","        out = transformer(question, question_mask, reply_input, reply_input_mask)\n","\n","        # Compute the loss\n","        loss = criterion(out, reply_target, reply_target_mask)\n","\n","        # Backprop\n","        transformer_optimizer.optimizer.zero_grad()\n","        loss.backward()\n","        transformer_optimizer.step()\n","\n","        sum_loss += loss.item() * samples\n","        count += samples\n","\n","        if i % 100 == 0:\n","            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))"],"metadata":{"id":"phb58a7lQZ8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluation\n","def evaluate(transformer, question, question_mask, max_len, word_map, device=\"cpu\"):\n","    \"\"\"\n","    Performs Greedy Decoding with a batch size of 1\n","    \"\"\"\n","    rev_word_map = {v: k for k, v in word_map.items()}\n","    transformer.eval()\n","    start_token = word_map['<start>']\n","    encoded = transformer.encode(question, question_mask)\n","    words = torch.LongTensor([[start_token]]).to(device)\n","\n","    for step in range(max_len - 1):\n","        size = words.shape[1]\n","        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n","        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n","        # only need the last tokens\n","        predictions = transformer.logit(decoded[:, -1])\n","        _, next_word = torch.max(predictions, dim=1)\n","        next_word = next_word.item()\n","        if next_word == word_map['<end>']:\n","            break\n","        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim=1)\n","\n","    # Construct Sentence\n","    if words.dim() == 2:\n","        words = words.squeeze(0)\n","        words = words.tolist()\n","\n","    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n","    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n","\n","    return sentence\n","\n","device = 'cpu'\n","while(1):\n","    question = input(\"Question: \")\n","    if question == 'quit':\n","        break\n","    max_len = input(\"Maximum Reply Length: \")\n","    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)\n","    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","    print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5AIhIjK0RIM7","executionInfo":{"status":"ok","timestamp":1661452348596,"user_tz":-480,"elapsed":20046,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"c187398a-bd35-4332-c607-60fc49baea97"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Question: what is your name\n","Maximum Reply Length: 25\n","begged funky begged funky begged funky begged funky begged funky begged funky begged funky begged funky begged funky begged funky begged funky porters fiction\n","Question: quit\n"]}]},{"cell_type":"markdown","source":["# 6) Pretrained Transformer"],"metadata":{"id":"XwY3zeGX42Dp"}},{"cell_type":"code","source":["# dataset and dataloader\n","from transformers import BartTokenizer, BartModel\n","tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n","max_len = 25\n","class Dataset(Dataset):\n","\n","    def __init__(self, pairs):\n","\n","        self.pairs = pairs\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","\n","        question = tokenizer(\" \".join(self.pairs[i][0]), max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","        reply = tokenizer(\" \".join(self.pairs[i][1]), max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","\n","        return question['input_ids'][0], question['attention_mask'][0], reply['input_ids'][0], reply['attention_mask'][0]\n","\n","    def __len__(self):\n","        return self.dataset_size\n","\n","train_loader = DataLoader(Dataset(pairs), batch_size=32, shuffle=True, pin_memory=True)\n","question, q_mask, reply, t_mask = next(iter(train_loader))\n","print(\"Question: \", question.size())\n","print(\"Answer: \", reply.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rc17QHJ9TUVf","executionInfo":{"status":"ok","timestamp":1661454926774,"user_tz":-480,"elapsed":1428,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"df92c9f7-89db-4d10-fdd6-65202e7ef47d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question:  torch.Size([32, 25])\n","Answer:  torch.Size([32, 25])\n"]}]},{"cell_type":"code","source":["class Transformers(nn.Module):\n","\n","    def __init__(self, num_classes, hidden_dim=512, nheads=8,\n","                 num_encoder_layers=6, num_decoder_layers=6):\n","        super().__init__()\n","\n","        # create a default PyTorch transformer\n","        self.backbone = BartModel.from_pretrained(\"facebook/bart-base\")\n","        self.transformer = nn.Transformer(\n","            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n","        self.linear_class = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","\n","        # take bart as embedding layers\n","        src_embed = self.backbone(src, src_mask)['encoder_last_hidden_state'].permute(1, 0, 2)\n","        tgt_embed = self.backbone(tgt, tgt_mask)['encoder_last_hidden_state'].permute(1, 0, 2)\n","\n","        # default transformer input: (seq_len, m, hidden_dim)\n","        out = self.transformer(\n","            src=src_embed,\n","            tgt=tgt_embed,\n","            src_key_padding_mask=src_mask,\n","            tgt_key_padding_mask=tgt_mask)\n","\n","        # (max_len, m, num_classes)\n","        result = self.linear_class(out)\n","        result = F.log_softmax(result, dim=2)\n","        return result\n","\n","model = Transformers(len(word_map), hidden_dim=768)\n","result = model(question, q_mask, reply, t_mask)\n","print(result.size())"],"metadata":{"id":"4rL0ySaY43mj"},"execution_count":null,"outputs":[]}]}