{"nbformat":4,"nbformat_minor":0,"metadata":{"jupytext":{"encoding":"# -*- coding: utf-8 -*-","formats":"ipynb,py:percent"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"FPsZuw9XJcF7"},"source":["pip install sacrebleu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"-qbaaYMlJRUZ","executionInfo":{"status":"ok","timestamp":1618668314468,"user_tz":-480,"elapsed":1488,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"338202c6-21f4-48a0-c731-08560a979748"},"source":["import numpy as np\n","import nltk\n","from nltk.util import ngrams\n","from collections import Counter\n","import sacrebleu\n","import matplotlib.pyplot as plt\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"iRn_YauCJRT5"},"source":["# 1) BLEU Score\n","\n","A popular metric for evaluating the quality of machine-translated text: the BLEU score proposed by Kishore Papineni, et al. In their 2002 paper [\"BLEU: a Method for Automatic Evaluation of Machine Translation\"](https://www.aclweb.org/anthology/P02-1040.pdf), the BLEU score works by comparing \"candidate\" text to one or more \"reference\" translations. The result is better the closer the score is to 1. Let's see how to get this value in the following sections.\n","\n","[Explanation](https://towardsdatascience.com/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b)"]},{"cell_type":"markdown","metadata":{"id":"x5FHoDaXJRUa"},"source":["## 1.1 Defining the BLEU Score\n","\n","You have seen the formula for calculating the BLEU score in this week's lectures. More formally, we can express the BLEU score as:\n","\n","$$BLEU = BP\\Bigl(\\prod_{i=1}^{4}precision_i\\Bigr)^{(1/4)}$$\n","\n","with the Brevity Penalty and precision defined as:\n","\n","$$BP = min\\Bigl(1, e^{(1-({ref}/{cand}))}\\Bigr)$$\n","\n","$$precision_i = \\frac {\\sum_{snt \\in{cand}}\\sum_{i\\in{snt}}min\\Bigl(m^{i}_{cand}, m^{i}_{ref}\\Bigr)}{w^{i}_{t}}$$\n","\n","where:\n","\n","* $m^{i}_{cand}$, is the count of i-gram in candidate matching the reference translation.\n","* $m^{i}_{ref}$, is the count of i-gram in the reference translation.\n","* $w^{i}_{t}$, is the total number of i-grams in candidate translation."]},{"cell_type":"markdown","metadata":{"id":"K0uJ6NXJJRUb"},"source":["The n-gram precision counts how many unigrams, bigrams, trigrams, and four-grams (i=1,...,4) match their n-gram counterpart in the reference translations. This term acts as a precision metric. Unigrams account for adequacy while longer n-grams account for fluency of the translation. To avoid overcounting, the n-gram counts are clipped to the maximal n-gram count occurring in the reference ($m_{n}^{ref}$). Typically precision shows exponential decay with the with the degree of the n-gram."]},{"cell_type":"markdown","metadata":{"id":"Em5m9iZlJRUb"},"source":["## 1.2 Calculations of the BLEU score"]},{"cell_type":"code","metadata":{"tags":[],"id":"lC8q0YWZJRUc"},"source":["reference = \"The NASA Opportunity rover is battling a massive dust storm on planet Mars.\"\n","candidate_1 = \"The Opportunity rover is combating a big sandstorm on planet Mars.\"\n","candidate_2 = \"A NASA rover is fighting a massive storm on planet Mars.\"\n","\n","tokenized_ref = nltk.word_tokenize(reference.lower())\n","tokenized_cand_1 = nltk.word_tokenize(candidate_1.lower())\n","tokenized_cand_2 = nltk.word_tokenize(candidate_2.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r4aqjTKoJRUd"},"source":["def brevity_penalty(reference, candidate):\n","    '''To penalize short sentence, from 0 to 1'''\n","    ref_length = len(reference)\n","    can_length = len(candidate)\n","\n","    # Brevity Penalty by length\n","    if ref_length > can_length:\n","        BP = 1\n","    else:\n","        penalty = 1 - (ref_length / can_length)\n","        BP = np.exp(penalty)\n","    return BP\n","\n","def clipped_precision(reference, candidate):\n","    \"\"\"Bleu score function given a original and a machine translated sentences\"\"\"\n","\n","    clipped_precision_score = []\n","\n","    # n-gram matching\n","    for i in range(1, 5):\n","        candidate_n_gram = Counter(ngrams(candidate, i))\n","        reference_n_gram = Counter(ngrams(reference, i))\n","        # sentence length\n","        c = sum(reference_n_gram.values())\n","\n","        # for every pair\n","        for j in reference_n_gram:\n","            if j in candidate_n_gram:\n","                # clip candidate's freq to reference's freq\n","                # clipped number of correct predicted words / Number of total predicted words\n","                # if more than reference word length, clip to one\n","                if (reference_n_gram[j] > candidate_n_gram[j]):\n","                    reference_n_gram[j] = candidate_n_gram[j]\n","            else:\n","                # default is 1, if not in reference then become 0\n","                reference_n_gram[j] = 0\n","\n","        clipped_precision_score.append(sum(reference_n_gram.values()) / c)\n","\n","    weights = [0.25] * 4\n","\n","    # combine all 4 precisions\n","    s = (w_i * np.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))\n","    s = np.exp(np.sum(s))\n","    return s\n","\n","def bleu_score(reference, candidate):\n","    BP = brevity_penalty(reference, candidate)\n","    precision = clipped_precision(reference, candidate)\n","    return BP * precision"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"gRwNYbtGJRUe","executionInfo":{"status":"ok","timestamp":1618668315779,"user_tz":-480,"elapsed":1346,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"e3b1644a-35d5-4e5a-ca9c-fff7e727598f"},"source":["print(\"Results reference versus candidate 1 our own code BLEU: \",\n","      round(bleu_score(tokenized_ref, tokenized_cand_1) * 100, 1))\n","\n","print(\"Results reference versus candidate 2 our own code BLEU: \",\n","      round(bleu_score(tokenized_ref, tokenized_cand_2) * 100, 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Results reference versus candidate 1 our own code BLEU:  27.4\n","Results reference versus candidate 2 our own code BLEU:  35.0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"oN3yrVygJRUp"},"source":["**BLEU Score Interpretation on a Corpus**\n","\n","|Score      | Interpretation                                                |\n","|:---------:|:-------------------------------------------------------------:|\n","| < 10      | Almost useless                                                |\n","| 10 - 19   | Hard to get the gist                                          |\n","| 20 - 29   | The gist is clear, but has significant grammatical errors     |\n","| 30 - 40   | Understandable to good translations                           |\n","| 40 - 50   | High quality translations                                     |\n","| 50 - 60   | Very high quality, adequate, and fluent translations          |\n","| > 60      | Quality often better than human                               |"]},{"cell_type":"markdown","source":["# 2) Beam Search\n","\n","With Greedy Search, we took just the single best word at each position. In contrast, Beam Search expands this and takes the best ’N’ words. We considered each position in isolation. Once we had identified the best word for that position, we did not examine what came before it (ie. in the previous position), or after it.\n","\n","In contrast, Beam Search picks the ’N’ best sequences so far and considers the probabilities of the combination of all of the preceding words along with the word in the current position.\n","\n","[Explanation](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24)"],"metadata":{"id":"w710a2QBdyFD"}}]}