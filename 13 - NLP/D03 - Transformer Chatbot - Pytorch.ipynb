{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"colab":{"provenance":[],"collapsed_sections":[]},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install datasets torch transformers\n","!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n","!unzip -qq cornell_movie_dialogs_corpus.zip\n","!rm cornell_movie_dialogs_corpus.zip\n","!mkdir datasets\n","!mv cornell\\ movie-dialogs\\ corpus/movie_conversations.txt ./datasets\n","!mv cornell\\ movie-dialogs\\ corpus/movie_lines.txt ./datasets"],"metadata":{"id":"pXnZMry3we8P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import Counter\n","import json\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.utils.data\n","import math\n","import torch.nn.functional as F"],"metadata":{"id":"e3yy1ZGr4dbN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1) Data Processing"],"metadata":{"id":"jGHsiOnj4fii"}},{"cell_type":"code","source":["# data processing\n","max_len = 25\n","\n","def remove_punc(string):\n","    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n","    no_punct = \"\"\n","    for char in string:\n","        if char not in punctuations:\n","            no_punct = no_punct + char  # space is also a character\n","    return no_punct.lower()\n","\n","corpus_movie_conv = './datasets/movie_conversations.txt'\n","corpus_movie_lines = './datasets/movie_lines.txt'\n","with open(corpus_movie_conv, 'r', encoding='iso-8859-1') as c:\n","    conv = c.readlines()\n","with open(corpus_movie_lines, 'r', encoding='iso-8859-1') as l:\n","    lines = l.readlines()\n","\n","# extract text\n","lines_dic = {}\n","for line in lines:\n","    objects = line.split(\" +++$+++ \")\n","    lines_dic[objects[0]] = objects[-1]\n","\n","# generate question answer pairs\n","pairs = []\n","for con in conv:\n","    ids = eval(con.split(\" +++$+++ \")[-1])\n","    for i in range(len(ids)):\n","        qa_pairs = []\n","        \n","        if i == len(ids) - 1:\n","            break\n","        \n","        first = remove_punc(lines_dic[ids[i]].strip())      \n","        second = remove_punc(lines_dic[ids[i+1]].strip())\n","        qa_pairs.append(first.split()[:max_len])\n","        qa_pairs.append(second.split()[:max_len])\n","        pairs.append(qa_pairs)\n","\n","# sample\n","print(pairs[20])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RxZmpdunyI6B","executionInfo":{"status":"ok","timestamp":1661446523411,"user_tz":-480,"elapsed":5611,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"c9e92f3d-e6bc-4508-be2d-47e39e7519a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[['i', 'really', 'really', 'really', 'wanna', 'go', 'but', 'i', 'cant', 'not', 'unless', 'my', 'sister', 'goes'], ['im', 'workin', 'on', 'it', 'but', 'she', 'doesnt', 'seem', 'to', 'be', 'goin', 'for', 'him']]\n"]}]},{"cell_type":"code","source":["# word map\n","min_word_freq = 5\n","\n","word_freq = Counter()\n","for pair in pairs:\n","    word_freq.update(pair[0])\n","    word_freq.update(pair[1])\n","\n","words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n","word_map = {k: v + 1 for v, k in enumerate(words)}\n","word_map['<unk>'] = len(word_map) + 1\n","word_map['<start>'] = len(word_map) + 1\n","word_map['<end>'] = len(word_map) + 1\n","word_map['<pad>'] = 0\n","\n","print(\"Total words are: {}\".format(len(word_map)))\n","\n","# encode sentences based on word map\n","def encode_question(words, word_map):\n","    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n","    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","pairs_encoded = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    ans = encode_reply(pair[1], word_map)\n","    pairs_encoded.append([qus, ans])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_q0Ek2foz39k","executionInfo":{"status":"ok","timestamp":1661446527661,"user_tz":-480,"elapsed":4262,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"85f7e928-cf99-47d2-e05e-5be7bece5ed8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total words are: 18243\n"]}]},{"cell_type":"code","source":["# dataset and dataloader\n","class Dataset(Dataset):\n","\n","    def __init__(self, pairs):\n","\n","        self.pairs = pairs\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size\n","\n","train_loader = DataLoader(Dataset(pairs_encoded), batch_size=32, shuffle=True, pin_memory=True)\n","question, reply = next(iter(train_loader))\n","print(\"Question: \", question.size())\n","print(\"Answer: \", reply.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34PY0Zjd2JNw","executionInfo":{"status":"ok","timestamp":1661446540894,"user_tz":-480,"elapsed":6,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"59ac1e4d-3888-47b4-9e7b-8a1367edb00b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question:  torch.Size([32, 25])\n","Answer:  torch.Size([32, 27])\n"]}]},{"cell_type":"markdown","source":["# 2) Masking"],"metadata":{"id":"HuoQgsta4mBZ"}},{"cell_type":"code","source":["# create mask\n","def create_masks(question, reply_input, reply_target, device='cpu'):\n","    \n","    def subsequent_mask(size):\n","        # (max_words, max_words)\n","        # binary triangle\n","        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        # (1. max_words, max_words)\n","        return mask.unsqueeze(0)\n","    \n","    # boolean(m, max_words)\n","    question_mask = question != 0\n","\n","    # (m, 1, 1, max_words)\n","    question_mask = question_mask.to(device)\n","    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         \n","    \n","    # boolean(m, max_words)\n","    reply_input_mask = reply_input != 0\n","    # (m, 1, max_words)\n","    reply_input_mask = reply_input_mask.unsqueeze(1)  \n","\n","    # only include triangle and non-pad token\n","    # (m, max_words, max_words)\n","    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n","\n","    # (batch_size, 1, max_words, max_words)\n","    reply_input_mask = reply_input_mask.unsqueeze(1) \n","\n","    # (batch_size, max_words)\n","    reply_target_mask = reply_target != 0              \n","    \n","    return question_mask, reply_input_mask, reply_target_mask\n","\n","reply_input = reply[:, :-1]\n","reply_target = reply[:, 1:]\n","print('Reply Target Size: ', reply_target.size())\n","\n","# Create mask and add dimensions\n","question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","print('question_mask Size: ', question_mask.size())\n","print('reply_input_mask Size: ', reply_input_mask.size())\n","print('reply_target_mask Size: ', reply_target_mask.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VQQckg0E6jer","executionInfo":{"status":"ok","timestamp":1661446985540,"user_tz":-480,"elapsed":286,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"5386d6bd-95fe-41b8-9b2b-3fa727f07500"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reply Target Size:  torch.Size([32, 26])\n","question_mask Size:  torch.Size([32, 1, 1, 25])\n","reply_input_mask Size:  torch.Size([32, 1, 26, 26])\n","reply_target_mask Size:  torch.Size([32, 26])\n"]}]},{"cell_type":"markdown","source":["# 3) Embedding"],"metadata":{"id":"44kxspAw4qyk"}},{"cell_type":"code","source":["# Positional Embedding\n","class Embeddings(nn.Module):\n","    \"\"\"\n","    Implements embeddings of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len=50, num_layers=6):\n","        super(Embeddings, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        # (1, max_len, d_model)\n","        self.pe = self.create_positional_encoding(max_len, self.d_model)     \n","        # (1, num_layers, d_model)\n","        self.te = self.create_positional_encoding(num_layers, self.d_model)  \n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positional_encoding(self, max_len, d_model, device='cpu'):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        # for each position of the word\n","        for pos in range(max_len):   \n","            # for each dimension of the each position\n","            for i in range(0, d_model, 2):   \n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        # include the batch size\n","        pe = pe.unsqueeze(0)   \n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        # create embed weight during first layer\n","        # (m, max_len) --> (m, max_len, d_model)\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","\n","        ### Positional Embedding\n","\n","        # pe will automatically be expanded with the same batch size as encoded_words\n","        # (m, max_len, d_model)\n","        embedding += self.pe[:, :embedding.size(1)]\n","\n","        # te: (1, num_layers, d_model) --> (1, 1, d_model) --> (1, max_len, d_model)\n","        # (m, max_len, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        return embedding\n","\n","embed_model = Embeddings(len(word_map), 512)\n","result = embed_model.forward(question, 0)\n","print(result.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BUuBWtyN9nB6","executionInfo":{"status":"ok","timestamp":1661448121991,"user_tz":-480,"elapsed":301,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"e1acf6b4-f60b-4d6d-d955-4e4efca12545"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 25, 512])\n"]}]},{"cell_type":"markdown","source":["<img src='https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/transformer.png'>\n","\n","<img src='https://1.bp.blogspot.com/-AVGK0ApREtk/WaiAuzddKVI/AAAAAAAAB_A/WPV5ropBU-cxrcMpqJBFHg73K9NX4vywwCLcBGAs/s1600/image2.png'>"],"metadata":{"id":"0az3bHuFJNF9"}},{"cell_type":"markdown","source":["# 4) Transformers"],"metadata":{"id":"AtvZKs_k4vnQ"}},{"cell_type":"code","source":["### transformers\n","class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self, heads, d_model, dropout=0.1):\n","        \n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % heads == 0\n","        self.d_k = d_model // heads\n","        self.heads = heads\n","        self.dropout = nn.Dropout(dropout)\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.output_linear = nn.Linear(d_model, d_model)\n","        \n","    def forward(self, query, key, value, mask):\n","        \"\"\"\n","        query, key, value of shape: (batch_size, max_len, d_model)\n","        mask of shape: (batch_size, 1, 1, max_words)\n","        \"\"\"\n","        # (batch_size, max_len, d_model)\n","        query = self.query(query)\n","        key = self.key(key)        \n","        value = self.value(value)   \n","        \n","        # (batch_size, max_len, d_model) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n","        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n","        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        \n","        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n","        scores = torch.matmul(query, key.permute(0, 1, 3, 2)) / math.sqrt(query.size(-1))\n","\n","        # fill 0 mask with super small number so it wont affect the softmax weight\n","        # (batch_size, h, max_len, max_len)\n","        scores = scores.masked_fill(mask == 0, -1e9)    \n","\n","        # (batch_size, h, max_len, max_len)\n","        # softmax to put attention weight for all non-pad tokens\n","        # max_len X max_len matrix of attention\n","        weights = F.softmax(scores, dim=-1)           \n","        weights = self.dropout(weights)\n","\n","        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n","        context = torch.matmul(weights, value)\n","\n","        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, d_model)\n","        context = context.permute(0, 2, 1, 3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n","\n","        # (batch_size, max_len, d_model)\n","        return self.output_linear(context)\n","\n","class FeedForward(nn.Module):\n","\n","    def __init__(self, d_model, middle_dim=2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        self.activation = torch.nn.GELU()\n","\n","    def forward(self, x):\n","        out = self.activation(self.fc1(x))\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class EncoderLayer(nn.Module):\n","\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, embeddings, mask):\n","        # embeddings: (batch_size, max_len, d_model)\n","        # encoder mask: (batch_size, 1, 1, max_len)\n","        # result: (batch_size, max_len, d_model)\n","        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n","        # residual layer\n","        interacted = self.layernorm(interacted + embeddings)\n","        # bottleneck\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","\n","class DecoderLayer(nn.Module):\n","    \n","    def __init__(self, d_model, heads):\n","        super(DecoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        \n","        # embeddings: (batch_size, max_len, d_model)\n","        # decoder mask: (m, 1, max_len, max_len)\n","        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n","        query = self.layernorm(query + embeddings)\n","\n","        # (batch_size, max_len, d_model)\n","        # encoder-decoder mask: (m, 1, 1, max_len)\n","        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded\n","\n","class Transformer(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map):\n","        super(Transformer, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers=num_layers)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        # (m, max_len) --> (m, max_len, d_model)\n","        encoded = self.encode(src_words, src_mask)\n","        # (m, max_len, d_model) --> (m, max_len, d_model)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        # (m, max_len, vocab_size)\n","        out = F.log_softmax(self.logit(decoded), dim=2)\n","        return out"],"metadata":{"id":"Cb9tdtgBBnbg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5) Optimizer\n","\n","<img src='https://miro.medium.com/max/886/1*ZhGLUwaaqlJ9C0WK0nbAEA.png'>\n","\n","\n","\n","## Relationship between Entropy, Cross-Entropy, and KL-Divergence\n","\n","https://towardsdatascience.com/entropy-cross-entropy-and-kl-divergence-explained-b09cdae917a"],"metadata":{"id":"WKhVM3wENWRY"}},{"cell_type":"code","source":["# optimizer & loss\n","class AdamWarmup:\n","    \n","    def __init__(self, model_size, warmup_steps, optimizer):\n","        self.model_size = model_size\n","        self.warmup_steps = warmup_steps\n","        self.optimizer = optimizer\n","        self.current_step = 0\n","        self.lr = 0\n","        \n","    def get_lr(self):\n","        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n","        \n","    def step(self):\n","        # Increment the number of steps each time we call the step function\n","        self.current_step += 1\n","        lr = self.get_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","        # update the learning rate\n","        self.lr = lr\n","        self.optimizer.step()\n","\n","class LossWithLS(nn.Module):\n","\n","    def __init__(self, size, smooth):\n","        super(LossWithLS, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n","        self.confidence = 1.0 - smooth\n","        self.smooth = smooth\n","        self.size = size\n","        \n","    def forward(self, prediction, target, mask):\n","        \"\"\"\n","        prediction of shape: (batch_size, max_words, vocab_size)\n","        target and mask of shape: (batch_size, max_words)\n","        \"\"\"\n","\n","        # (batch_size * max_words, vocab_size)\n","        prediction = prediction.view(-1, prediction.size(-1))   \n","\n","        # (batch_size * max_words)\n","        target = target.contiguous().view(-1)   \n","        mask = mask.float()\n","        mask = mask.view(-1)       \n","\n","        # (batch_size * max_words)\n","        labels = prediction.data.clone()\n","        labels.fill_(self.smooth / (self.size - 1))\n","        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","\n","        # (batch_size * max_words, vocab_size)\n","        loss = self.criterion(prediction, labels)    \n","        loss = (loss.sum(1) * mask).sum() / mask.sum()\n","        return loss\n","\n","### testing\n","transformer = Transformer(d_model=512, heads=8, num_layers=2, word_map=word_map)\n","transformer = transformer.to('cpu')\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size=512, warmup_steps=4000, optimizer=adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","out = transformer(question, question_mask, reply_input, reply_input_mask)\n","print(out.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dby1mizMOSo","executionInfo":{"status":"ok","timestamp":1661452063848,"user_tz":-480,"elapsed":1436,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"9972dd0e-e7ad-49db-c58f-8fdbb26946f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([32, 26, 18243])\n"]}]},{"cell_type":"code","source":["### training step\n","def train(train_loader, transformer, criterion, epoch, device='cpu'):\n","    \n","    transformer.train()\n","    sum_loss = 0\n","    count = 0\n","\n","    for i, (question, reply) in enumerate(train_loader):\n","        \n","        samples = question.shape[0]\n","\n","        # Move to device\n","        question = question.to(device)\n","        reply = reply.to(device)\n","\n","        # Prepare Target Data\n","        reply_input = reply[:, :-1]\n","        reply_target = reply[:, 1:]\n","\n","        # Create mask and add dimensions\n","        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","\n","        # Get the transformer outputs\n","        out = transformer(question, question_mask, reply_input, reply_input_mask)\n","\n","        # Compute the loss\n","        loss = criterion(out, reply_target, reply_target_mask)\n","        \n","        # Backprop\n","        transformer_optimizer.optimizer.zero_grad()\n","        loss.backward()\n","        transformer_optimizer.step()\n","        \n","        sum_loss += loss.item() * samples\n","        count += samples\n","        \n","        if i % 100 == 0:\n","            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))"],"metadata":{"id":"phb58a7lQZ8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# evaluation\n","def evaluate(transformer, question, question_mask, max_len, word_map, device=\"cpu\"):\n","    \"\"\"\n","    Performs Greedy Decoding with a batch size of 1\n","    \"\"\"\n","    rev_word_map = {v: k for k, v in word_map.items()}\n","    transformer.eval()\n","    start_token = word_map['<start>']\n","    encoded = transformer.encode(question, question_mask)\n","    words = torch.LongTensor([[start_token]]).to(device)\n","    \n","    for step in range(max_len - 1):\n","        size = words.shape[1]\n","        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n","        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n","        predictions = transformer.logit(decoded[:, -1])\n","        _, next_word = torch.max(predictions, dim=1)\n","        next_word = next_word.item()\n","        if next_word == word_map['<end>']:\n","            break\n","        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim=1)\n","        \n","    # Construct Sentence\n","    if words.dim() == 2:\n","        words = words.squeeze(0)\n","        words = words.tolist()\n","        \n","    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n","    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n","    \n","    return sentence\n","\n","device = 'cpu'\n","while(1):\n","    question = input(\"Question: \") \n","    if question == 'quit':\n","        break\n","    max_len = input(\"Maximum Reply Length: \")\n","    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","    print(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5AIhIjK0RIM7","executionInfo":{"status":"ok","timestamp":1661452348596,"user_tz":-480,"elapsed":20046,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"c187398a-bd35-4332-c607-60fc49baea97"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Question: what is your name\n","Maximum Reply Length: 25\n","begged funky begged funky begged funky begged funky begged funky begged funky begged funky begged funky begged funky begged funky begged funky porters fiction\n","Question: quit\n"]}]},{"cell_type":"markdown","source":["# 6) Pretrained Transformer"],"metadata":{"id":"XwY3zeGX42Dp"}},{"cell_type":"code","source":["# dataset and dataloader\n","from transformers import BartTokenizer, BartModel\n","tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n","max_len = 25\n","class Dataset(Dataset):\n","\n","    def __init__(self, pairs):\n","\n","        self.pairs = pairs\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","\n","        question = tokenizer(\" \".join(self.pairs[i][0]), max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","        reply = tokenizer(\" \".join(self.pairs[i][1]), max_length=max_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","        \n","        return question['input_ids'][0], question['attention_mask'][0], reply['input_ids'][0], reply['attention_mask'][0]\n","\n","    def __len__(self):\n","        return self.dataset_size\n","\n","train_loader = DataLoader(Dataset(pairs), batch_size=32, shuffle=True, pin_memory=True)\n","question, q_mask, reply, t_mask = next(iter(train_loader))\n","print(\"Question: \", question.size())\n","print(\"Answer: \", reply.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rc17QHJ9TUVf","executionInfo":{"status":"ok","timestamp":1661454926774,"user_tz":-480,"elapsed":1428,"user":{"displayName":"Kean Chan","userId":"05792587367281359063"}},"outputId":"df92c9f7-89db-4d10-fdd6-65202e7ef47d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Question:  torch.Size([32, 25])\n","Answer:  torch.Size([32, 25])\n"]}]},{"cell_type":"code","source":["class Transformers(nn.Module):\n","\n","    def __init__(self, num_classes, hidden_dim=512, nheads=8,\n","                 num_encoder_layers=6, num_decoder_layers=6):\n","        super().__init__()\n","\n","        # create a default PyTorch transformer\n","        self.backbone = BartModel.from_pretrained(\"facebook/bart-base\")\n","        self.transformer = nn.Transformer(\n","            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n","        self.linear_class = nn.Linear(hidden_dim, num_classes)\n","\n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","\n","        src_embed = self.backbone(src, src_mask)['encoder_last_hidden_state'].permute(1, 0, 2)\n","        tgt_embed = self.backbone(tgt, tgt_mask)['encoder_last_hidden_state'].permute(1, 0, 2)\n","\n","        out = self.transformer(\n","            src=src_embed, \n","            tgt=tgt_embed, \n","            src_key_padding_mask=src_mask, \n","            tgt_key_padding_mask=tgt_mask)\n","        \n","        result = self.linear_class(out)\n","        result = F.log_softmax(result, dim=2)\n","        return result\n","\n","model = Transformers(len(word_map), hidden_dim=768)\n","result = model(question, q_mask, reply, t_mask)\n","print(result.size())"],"metadata":{"id":"4rL0ySaY43mj"},"execution_count":null,"outputs":[]}]}