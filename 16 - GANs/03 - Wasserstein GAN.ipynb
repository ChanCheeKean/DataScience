{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03 - Wasserstein GAN.ipynb","provenance":[],"collapsed_sections":[]},"coursera":{"schema_names":["GANSC1-3A"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1czVdIlqnImH"},"source":["WGAN-GP solves some of the stability issues with the GANs. Specifically, you'll use a special kind of loss function known as the W-loss, where W stands for Wasserstein, and gradient penalties to prevent mode collapse."]},{"cell_type":"code","metadata":{"id":"JfkorNJrnmNO"},"source":["import torch\n","from torch import nn\n","from tqdm.auto import tqdm\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from torchvision.utils import make_grid\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yh2dIhyNbnf4"},"source":["def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n","    '''\n","    Function for visualizing images: Given a tensor of images, number of images, and\n","    size per image, plots and prints the images in an uniform grid.\n","    '''\n","    image_tensor = (image_tensor + 1) / 2\n","    image_unflat = image_tensor.detach().cpu()\n","    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n","    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n","    plt.show()\n","\n","def make_grad_hook():\n","    '''\n","    Function to keep track of gradients for visualization purposes, \n","    which fills the grads list when using model.apply(grad_hook).\n","    '''\n","    grads = []\n","    def grad_hook(m):\n","        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","            grads.append(m.weight.grad)\n","    return grads, grad_hook"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P1A1M6kpnfxw"},"source":["# Generator and Noise"]},{"cell_type":"code","metadata":{"id":"tFwajQ3tGgI2"},"source":["class Generator(nn.Module):\n","    '''\n","    Generator Class\n","    Values:\n","        z_dim: the dimension of the noise vector, a scalar\n","        im_chan: the number of channels of the output image, a scalar (MNIST is black-and-white, so 1 channel is your default)\n","        hidden_dim: the inner dimension, a scalar\n","    '''\n","    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):\n","        super(Generator, self).__init__()\n","        self.z_dim = z_dim\n","        self.gen = nn.Sequential(\n","            self.make_gen_block(z_dim, hidden_dim * 4),\n","            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n","            self.make_gen_block(hidden_dim * 2, hidden_dim),\n","            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n","        )\n","\n","    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n","        '''\n","        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n","        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n","        Parameters:\n","            input_channels: how many channels the input feature representation has\n","            output_channels: how many channels the output feature representation should have\n","            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n","            stride: the stride of the convolution\n","            final_layer: a boolean, true if it is the final layer and false otherwise  (affects activation and batchnorm)\n","        '''\n","        if not final_layer:\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n","                nn.BatchNorm2d(output_channels),\n","                nn.ReLU(inplace=True),\n","            )\n","        else:\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n","                nn.Tanh(),\n","            )\n","\n","    def forward(self, noise):\n","        '''\n","        Function for completing a forward pass of the generator: Given a noise tensor (n_samples, z_dim), returns generated images.\n","        '''\n","        x = noise.view(len(noise), self.z_dim, 1, 1)\n","        return self.gen(x)\n","\n","def get_noise(n_samples, z_dim, device='cpu'):\n","    '''\n","    Function for creating noise vectors: Given the dimensions (n_samples, z_dim)\n","    creates a tensor of that shape filled with random numbers from the normal distribution.\n","    '''\n","    return torch.randn(n_samples, z_dim, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_2WxZFpDKxul","executionInfo":{"status":"ok","timestamp":1620467823620,"user_tz":-480,"elapsed":2728,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"ad294108-12fc-4c8f-88a2-560ba096e27a"},"source":["gen = Generator()\n","num_test = 100\n","hidden_dim = 64\n","test_hidden_noise = get_noise(num_test, gen.z_dim)\n","x = test_hidden_noise.view(len(test_hidden_noise), gen.z_dim, 1, 1)\n","print(x.shape)\n","\n","test_hidden_block = gen.make_gen_block(10, hidden_dim * 4, kernel_size=3, stride=2)\n","hidden_output = test_hidden_block(x)\n","print(hidden_output.shape)\n","\n","test_hidden_block = gen.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1)\n","hidden_output = test_hidden_block(hidden_output)\n","print(hidden_output.shape)\n","\n","test_hidden_block = gen.make_gen_block(hidden_dim * 2, hidden_dim, kernel_size=3, stride=2)\n","hidden_output = test_hidden_block(hidden_output)\n","print(hidden_output.shape)\n","\n","test_hidden_block = gen.make_gen_block(hidden_dim, 1, kernel_size=4, stride=2, final_layer=True)\n","hidden_output = test_hidden_block(hidden_output)\n","print(hidden_output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([100, 10, 1, 1])\n","torch.Size([100, 256, 3, 3])\n","torch.Size([100, 128, 6, 6])\n","torch.Size([100, 64, 13, 13])\n","torch.Size([100, 1, 28, 28])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r9fScH98nkYH"},"source":["# Critic"]},{"cell_type":"code","metadata":{"id":"aA4AxGnmpuPq"},"source":["class Critic(nn.Module):\n","    '''\n","    Critic Class\n","    Values:\n","        im_chan: the number of channels of the output image, a scalar (MNIST is black-and-white, so 1 channel is your default)\n","        hidden_dim: the inner dimension, a scalar\n","    '''\n","    def __init__(self, im_chan=1, hidden_dim=64):\n","        super(Critic, self).__init__()\n","        self.crit = nn.Sequential(\n","            self.make_crit_block(im_chan, hidden_dim),\n","            self.make_crit_block(hidden_dim, hidden_dim * 2),\n","            self.make_crit_block(hidden_dim * 2, 1, final_layer=True),\n","        )\n","\n","    def make_crit_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n","        '''\n","        Function to return a sequence of operations corresponding to a critic block of DCGAN;\n","        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n","        Parameters:\n","            input_channels: how many channels the input feature representation has\n","            output_channels: how many channels the output feature representation should have\n","            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n","            stride: the stride of the convolution\n","            final_layer: a boolean, true if it is the final layer and false otherwise  (affects activation and batchnorm)\n","        '''\n","        if not final_layer:\n","            return nn.Sequential(\n","                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n","                nn.BatchNorm2d(output_channels),\n","                nn.LeakyReLU(0.2, inplace=True),\n","            )\n","        else:\n","            return nn.Sequential(\n","                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n","            )\n","\n","    def forward(self, image):\n","        '''\n","        Function for completing a forward pass of the critic: Given an image tensor(im_chan), returns a 1-dimension tensor representing fake/real.\n","        '''\n","        crit_pred = self.crit(image)\n","        return crit_pred.view(len(crit_pred), -1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4IYFgeKKO9j-","executionInfo":{"status":"ok","timestamp":1620467826119,"user_tz":-480,"elapsed":1292,"user":{"displayName":"Chan Chee Kean","photoUrl":"","userId":"05792587367281359063"}},"outputId":"5e8b67aa-ebb0-45ac-fa3d-8ffdcdf853a6"},"source":["disc = Critic()\n","hidden_dim=64\n","\n","block = disc.make_crit_block(1, hidden_dim, kernel_size=4, stride=2)\n","output = block(hidden_output)\n","print(output.shape)\n","\n","block = disc.make_crit_block(hidden_dim, hidden_dim * 2, kernel_size=4, stride=2)\n","output = block(output)\n","print(output.shape)\n","\n","block = disc.make_crit_block(hidden_dim * 2, 1, kernel_size=4, stride=2, final_layer=True)\n","output = block(output)\n","print(output.shape)\n","\n","print(output.view(len(output), -1).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([100, 64, 13, 13])\n","torch.Size([100, 128, 5, 5])\n","torch.Size([100, 1, 1, 1])\n","torch.Size([100, 1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qRk_8azSq3tF"},"source":["# Model Training\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"IFLQ039u-qdu"},"source":["n_epochs = 100\n","z_dim = 64\n","display_step = 50\n","batch_size = 128\n","lr = 0.0002\n","beta_1 = 0.5\n","beta_2 = 0.999\n","c_lambda = 10 # weight of gradient penalty\n","crit_repeats = 5 # number of times to update the critic per generator update\n","device = 'cuda'\n","\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,)),\n","])\n","\n","dataloader = DataLoader(\n","    MNIST('.', download=True, transform=transform),\n","    batch_size=batch_size,\n","    shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sDFRZ8tg_Y57"},"source":["gen = Generator(z_dim).to(device)\n","gen_opt = torch.optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n","crit = Critic().to(device) \n","crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","        torch.nn.init.constant_(m.bias, 0)\n","gen = gen.apply(weights_init)\n","crit = crit.apply(weights_init)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFEi5BhVX5-P"},"source":["# Gradient Penalty"]},{"cell_type":"code","metadata":{"id":"Tn4dkXnNtcv6"},"source":["def get_gradient(crit, real, fake, epsilon):\n","    '''\n","    Return the gradient of the critic's scores with respect to mixes of real and fake images.\n","    Parameters:\n","        crit: the critic model\n","        real: a batch of real images\n","        fake: a batch of fake images\n","        epsilon: a vector of the uniformly random proportions of real/fake per mixed image\n","    Returns:\n","        gradient: the gradient of the critic's scores, with respect to the mixed image\n","    '''\n","    # Mix the images together\n","    mixed_images = real * epsilon + fake * (1 - epsilon)\n","\n","    # Calculate the critic's scores on the mixed images\n","    mixed_scores = crit(mixed_images)\n","    \n","    # Take the gradient of the scores with respect to the images\n","    gradient = torch.autograd.grad(\n","        inputs=mixed_images,\n","        outputs=mixed_scores,\n","        grad_outputs=torch.ones_like(mixed_scores), \n","        create_graph=True,\n","        retain_graph=True,\n","    )[0]\n","    return gradient"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VPwBH83IzCpS"},"source":["def gradient_penalty(gradient):\n","    '''\n","    Return the gradient penalty, given a gradient.\n","    Given a batch of image gradients, you calculate the magnitude of each image's gradient\n","    and penalize the mean quadratic distance of each magnitude to 1.\n","    Parameters:\n","        gradient: the gradient of the critic's scores, with respect to the mixed image\n","    Returns:\n","        penalty: the gradient penalty\n","    '''\n","    # Flatten the gradients so that each row captures one image\n","    gradient = gradient.view(len(gradient), -1)\n","\n","    # Calculate the magnitude of every row\n","    gradient_norm = gradient.norm(2, dim=1)\n","    \n","    # Penalize the mean squared distance of the gradient norms from 1\n","    penalty = torch.mean((gradient_norm - 1)**2)\n","    return penalty"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sob-u9Z1X9sb"},"source":["# Losses"]},{"cell_type":"code","metadata":{"id":"YnJFs-qkMCA-"},"source":["def get_gen_loss(crit_fake_pred):\n","    '''\n","    Return the loss of a generator given the critic's scores of the generator's fake images.\n","    Parameters:\n","        crit_fake_pred: the critic's scores of the fake images\n","    Returns:\n","        gen_loss: a scalar loss value for the current batch of the generator\n","    '''\n","    gen_loss = -1. * torch.mean(crit_fake_pred)\n","    return gen_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-jvbz1zDMTdu"},"source":["def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n","    '''\n","    Return the loss of a critic given the critic's scores for fake and real images,\n","    the gradient penalty, and gradient penalty weight.\n","    Parameters:\n","        crit_fake_pred: the critic's scores of the fake images\n","        crit_real_pred: the critic's scores of the real images\n","        gp: the unweighted gradient penalty\n","        c_lambda: the current weight of the gradient penalty \n","    Returns:\n","        crit_loss: a scalar for the critic's loss, accounting for the relevant factors\n","    '''\n","    crit_loss = torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp\n","    return crit_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_x5wu7rUMlnZ"},"source":["## Putting It All Together\n","Before you put everything together, there are a few things to note.\n","1.   Even on GPU, the **training will run more slowly** than previous labs because the gradient penalty requires you to compute the gradient of a gradient -- this means potentially a few minutes per epoch! For best results, run this for as long as you can while on GPU.\n","2.   One important difference from earlier versions is that you will **update the critic multiple times** every time you update the generator This helps prevent the generator from overpowering the critic. Sometimes, you might see the reverse, with the generator updated more times than the critic. This depends on architectural (e.g. the depth and width of the network) and algorithmic choices (e.g. which loss you're using). \n","3.   WGAN-GP isn't necessarily meant to improve overall performance of a GAN, but just **increases stability** and avoids mode collapse. In general, a WGAN will be able to train in a much more stable way than the vanilla DCGAN from last assignment, though it will generally run a bit slower. You should also be able to train your model for more epochs without it collapsing.\n","\n","\n","<!-- Once again, be warned that this runs very slowly on a CPU. One way to run this more quickly is to download the .ipynb and upload it to Google Drive, then open it with Google Colab and make the runtime type GPU and replace\n","`device = \"cpu\"`\n","with\n","`device = \"cuda\"`\n","and make sure that your `get_noise` function uses the right device.  -->\n","\n","Here is a snapshot of what your WGAN-GP outputs should resemble:\n","![MNIST Digits Progression](https://raw.githubusercontent.com/amanchadha/coursera-gan-specialization/main/C1%20-%20Build%20Basic%20Generative%20Adversarial%20Networks/Week%203/MNIST_WGAN_Progression.png)"]},{"cell_type":"code","metadata":{"id":"UXptQZcwrBrq"},"source":["cur_step = 0\n","generator_losses = []\n","critic_losses = []\n","for epoch in range(n_epochs):\n","    # Dataloader returns the batches\n","    for real, _ in tqdm(dataloader):\n","        cur_batch_size = len(real)\n","        real = real.to(device)\n","\n","        mean_iteration_critic_loss = 0\n","        for _ in range(crit_repeats):\n","            ### Update critic ###\n","            crit_opt.zero_grad()\n","            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n","            fake = gen(fake_noise)\n","            crit_fake_pred = crit(fake.detach())\n","            crit_real_pred = crit(real)\n","\n","            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n","            gradient = get_gradient(crit, real, fake.detach(), epsilon)\n","            gp = gradient_penalty(gradient)\n","            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n","\n","            # Keep track of the average critic loss in this batch\n","            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n","            # Update gradients\n","            crit_loss.backward(retain_graph=True)\n","            # Update optimizer\n","            crit_opt.step()\n","        critic_losses += [mean_iteration_critic_loss]\n","\n","        ### Update generator ###\n","        gen_opt.zero_grad()\n","        fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n","        fake_2 = gen(fake_noise_2)\n","        crit_fake_pred = crit(fake_2)\n","        \n","        gen_loss = get_gen_loss(crit_fake_pred)\n","        gen_loss.backward()\n","\n","        # Update the weights\n","        gen_opt.step()\n","\n","        # Keep track of the average generator loss\n","        generator_losses += [gen_loss.item()]\n","\n","        ### Visualization code ###\n","        if cur_step % display_step == 0 and cur_step > 0:\n","            gen_mean = sum(generator_losses[-display_step:]) / display_step\n","            crit_mean = sum(critic_losses[-display_step:]) / display_step\n","            print(f\"Step {cur_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n","            show_tensor_images(fake)\n","            show_tensor_images(real)\n","            step_bins = 20\n","            num_examples = (len(generator_losses) // step_bins) * step_bins\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"Generator Loss\"\n","            )\n","            plt.plot(\n","                range(num_examples // step_bins), \n","                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n","                label=\"Critic Loss\"\n","            )\n","            plt.legend()\n","            plt.show()\n","\n","        cur_step += 1"],"execution_count":null,"outputs":[]}]}